{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from six.moves import xrange \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "log = logging.getLogger('log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "lhnd = logging.StreamHandler()\n",
    "lhnd.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "lhnd.setFormatter(formatter)\n",
    "\n",
    "log.addHandler(lhnd)\n",
    "\n",
    "%autonotify -a 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_dumps = False\n",
    "\n",
    "def lmap(f, arr):\n",
    "    return list(map(f, arr))\n",
    "\n",
    "def lfilter(f, arr):\n",
    "    return list(filter(f, arr))\n",
    "\n",
    "def foreach(it, f):\n",
    "    for e in it:\n",
    "        f(e)\n",
    "        \n",
    "def dump(data, name):\n",
    "    with open('data/' + name, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load(name):\n",
    "    with open('data/' + name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def load_or_dump(path, func):\n",
    "    if not Path('data/' + path).exists() or ignore_dumps:\n",
    "        res = func()\n",
    "    \n",
    "        dump(res, path)\n",
    "    else:\n",
    "        res = load(path)\n",
    "        \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "with open('auth/token') as f:\n",
    "    token = f.readline().strip()\n",
    "\n",
    "def get_info(ids):\n",
    "    sleep(0.2)\n",
    "    mc = 'members_count'\n",
    "    payload = {'v': '5.92', 'access_token': token, 'fields':mc}\n",
    "    \n",
    "    str_ids = functools.reduce(\n",
    "        lambda x, y: x + y,\n",
    "        lmap(lambda x: str(x) + ',', ids)\n",
    "    )\n",
    "    \n",
    "    payload['group_ids'] = str_ids[0:- 1]\n",
    "    \n",
    "    r = requests.get('https://api.vk.com/method/groups.getById', \n",
    "                     params=payload)\n",
    "    \n",
    "    if (not 'response' in r.json()):\n",
    "        print(r.json())\n",
    "        \n",
    "    res = lmap(lambda x: (x['name'], x['screen_name'], \"{:,}\".format(x[mc]) if mc in x else -1),r.json()['response'])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 1015925\n",
    "\n",
    "def raw_data_filter(file):\n",
    "    # Mapping to events\n",
    "    res = list()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for line in file:\n",
    "        cur = line.rstrip().split(',')\n",
    "        cur = lmap(lambda p: (re.sub(';.*', '', p), re.sub('.*;', '', p)), cur)\n",
    "\n",
    "        session = list()\n",
    "        \n",
    "        for j in range(0, len(cur)):\n",
    "            try:\n",
    "                session.append(int(cur[j][1]))\n",
    "            except ValueError:\n",
    "                None\n",
    "                \n",
    "        res.append(session)\n",
    "\n",
    "        i = i + 1\n",
    "                \n",
    "        if (i % 100000 == 0):\n",
    "            gc.collect()\n",
    "\n",
    "            log.debug(\"%d %% of mapping is done.\", i / total * 100)\n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode == 0):\n",
    "    raw_data = load_or_dump('raw', lambda: raw_data_filter(open(\"data/public_sessions_2.txt\",\"r\")))\n",
    "\n",
    "    log.info(\"Data loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_count(data):\n",
    "    total = dict()\n",
    "\n",
    "    for i in data:\n",
    "        for j in i[0]:\n",
    "            if (j in total.keys()):\n",
    "                total[j] = total[j] + 1\n",
    "            else:\n",
    "                total[j] = 1\n",
    "                \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words_data(file):\n",
    "    words = []\n",
    "    \n",
    "    for line in file:\n",
    "        for word in line.split():\n",
    "            words.append(word)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode == 1):\n",
    "    words_size = 50000\n",
    "    \n",
    "    ignore_dumps = True\n",
    "    data = load_or_dump('raw_txt', lambda: load_words_data(open(\"data/text8.txt\",\"r\")))\n",
    "    groups = group_count([[data]])\n",
    "    \n",
    "    dictlist = list(groups.items())\n",
    "    dictlist.sort(key = lambda x: x[1])\n",
    "    allowed = set(lmap(lambda x: x[0], dictlist[-words_size:]))\n",
    "\n",
    "    for i in xrange(len(data)):\n",
    "        if not data[i] in allowed:\n",
    "            data[i] = '-1'\n",
    "            \n",
    "    groups = group_count([[data]])\n",
    "    \n",
    "    data = [[data, []]]\n",
    "    \n",
    "    print(len(groups))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_session_size = 2\n",
    "max_session_size = 20\n",
    "\n",
    "def initiail_mapping(lst, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set()\n",
    "        malformed = set()\n",
    "        \n",
    "        for event in session:\n",
    "            if (event < 0):\n",
    "                sub_event = -event\n",
    "                \n",
    "                if (sub_event in sub or sub_event in malformed):\n",
    "                    sub.discard(sub_event)\n",
    "                    unsub.discard(sub_event)\n",
    "                    malformed.add(sub_event)\n",
    "                else:\n",
    "                    unsub.add(sub_event)\n",
    "            else:\n",
    "                if (event in unsub or event in malformed):\n",
    "                    unsub.discard(event)\n",
    "                    sub.discard(event)\n",
    "                    malformed.add(event)\n",
    "                else:\n",
    "                    sub.add(event)\n",
    "        \n",
    "        if (len(sub) >= min_session_size and len(sub) <= max_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "    \n",
    "\n",
    "def set_map(lst, cnt, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set() \n",
    "        \n",
    "        for event in session[0]:\n",
    "            if (cnt[event] > min_allowed):\n",
    "                sub.add(event)\n",
    "                \n",
    "        for event in session[1]:\n",
    "            if (cnt.get(event, -1) > min_allowed):\n",
    "                unsub.add(event)    \n",
    "        \n",
    "        if (len(sub) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def drop_uncommon(raw_data, min_allowed = 10):\n",
    "    cnt = None\n",
    "    sorted_cnt = None\n",
    "    \n",
    "    data, groups = initiail_mapping(raw_data, min_allowed)\n",
    "    cnt = group_count(data) \n",
    "    sorted_cnt = sorted(list(cnt.values()))\n",
    "    \n",
    "    while (cnt == None or sorted_cnt[0] < min_allowed):\n",
    "        data, groups = set_map(data, cnt, min_allowed)\n",
    "                \n",
    "        cnt = group_count(data) \n",
    "        sorted_cnt = sorted(list(cnt.values()))\n",
    "        \n",
    "        log.info(\"Length of data:   %d\", len(data))\n",
    "        log.info(\"Total length:     %d\", \n",
    "                functools.reduce((lambda x, y: x + y), lmap(lambda a: len(a), data))\n",
    "                )\n",
    "        log.info(\"Number of groups: %d\", len(groups))\n",
    "        log.info(\"Minimum count:    %d\\n\", sorted_cnt[0])\n",
    "        \n",
    "    return data, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (mode == 0):\n",
    "    ignore_dumps = True\n",
    "    data, groups = load_or_dump('final_data', lambda: drop_uncommon(raw_data, 50))\n",
    "\n",
    "    most_common = sorted(group_count(data).items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = {w: i for i, w in enumerate(groups)}\n",
    "i2w = {i: w for i, w in enumerate(groups)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i2w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = None\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dex = 0\n",
    "event_dex = 0\n",
    "\n",
    "def generate_batch(batch_size, negative_size, window_size = 1):\n",
    "    assert min_session_size > 1\n",
    "    \n",
    "    global session_dex\n",
    "    global event_dex\n",
    "    \n",
    "    labels = np.ndarray(shape=(batch_size, window_size), dtype=np.int32)\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "#     negative = np.ndarray(shape=(batch_size, negative_size), dtype=np.int32)\n",
    "     \n",
    "    current = 0\n",
    "    session = list(data[session_dex][0])\n",
    "    \n",
    "    for i in range(0, batch_size):     \n",
    "        batch[i] = w2i[session[event_dex]]\n",
    "        \n",
    "        for j in range(1, window_size + 1):\n",
    "            labels[i][j - 1] = w2i[session[(event_dex + j) % len(session)]]\n",
    "            \n",
    "            if (labels[i][j - 1] == batch[i]):\n",
    "                labels[i][j - 1] = labels[i][j - 2]\n",
    "            \n",
    "#         neg = 0\n",
    "        \n",
    "#         for j in data[session_dex][1]:\n",
    "#             negative[i][neg] = w2i[j]\n",
    "#             neg += 1\n",
    "#             if (neg == negative_size):\n",
    "#                 break\n",
    "                \n",
    "#         rand_neg = np.random.randint(len(groups), size=negative_size - neg)\n",
    "        \n",
    "#         for j in range(0, negative_size - neg):\n",
    "#             negative[i][neg + j] = rand_neg[j]\n",
    "            \n",
    "        event_dex += 1\n",
    "\n",
    "        if (event_dex == len(session)):\n",
    "            event_dex = 0\n",
    "            session_dex = session_dex + 1\n",
    "            if (session_dex >= len(data)):\n",
    "                session_dex = 0\n",
    "            session = list(data[session_dex][0])        \n",
    "     \n",
    "    return batch, labels, []\n",
    "#     return batch, labels, negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session_dex = 0\n",
    "\n",
    "#print(data[session_dex])\n",
    "#print(data[session_dex + 1])\n",
    "\n",
    "batch, labels, negative = generate_batch(16, 1, 1)\n",
    "\n",
    "for i in range(10):\n",
    "    print(i2w[batch[i]], '->', lmap(lambda x: i2w[x], labels[i]), )\n",
    "#      print(i2w[batch[i]], '->', lmap(lambda x: i2w[x], labels[i]), '-> (negative)', lmap(lambda x: i2w[x], negative[i]))\n",
    "    \n",
    "#print(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data = None\n",
    "test_ids=lfilter(lambda x: x in w2i, [129440544, 28261334, 92876084, 51016572, 91933860])\n",
    "\n",
    "if (mode == 1):\n",
    "    test_ids = ['term', 'first', 'used', 'early', 'against', 'working']\n",
    "\n",
    "learning_rate = 0.1\n",
    "vocabulary_size = len(groups)\n",
    "\n",
    "window_size = 4\n",
    "embedding_size = 48\n",
    "num_sampled = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(emb, index, f = None):\n",
    "    p = emb[index]\n",
    "    cnst = tf.constant(p, shape=[1, embedding_size])\n",
    "    d = tf.matmul(cnst, emb, transpose_b=True).eval()[0]\n",
    "\n",
    "    dxs = np.argsort(np.array(d))\n",
    "    \n",
    "    ids = []\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(dxs) - 10, len(dxs)):\n",
    "        ids.append(i2w[dxs[i]])\n",
    "        res.append(d[dxs[i]])\n",
    "    \n",
    "    if (mode == 0):\n",
    "        info = get_info(ids)\n",
    "    else:\n",
    "        info = ids\n",
    "    \n",
    "    for i in xrange(len(res)):\n",
    "        print(ids[i], ' ', res[i], ' ', info[i])\n",
    "        \n",
    "        if (f != None):\n",
    "            f.write(str(ids[i]) + ' ' + str(res[i]) + ' ' + str(info[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 200000\n",
    "\n",
    "def loss_fn(batch_size, batch_inputs, batch_labels, batch_negative, embeddings): \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "        \n",
    "    log.info(\"loss_fn init\")\n",
    "    res_lst = []\n",
    "\n",
    "    for i in xrange(batch_size):\n",
    "        inp = batch_inputs[i]\n",
    "        lbl = batch_labels[i]\n",
    "        ng = batch_negative[i]\n",
    "\n",
    "        m = tf.map_fn(lambda k: tf.matmul(tf.gather(normalized_embeddings, [inp]), \n",
    "                                            tf.gather(normalized_embeddings, [k]), \n",
    "                                            transpose_b=True), \n",
    "                        lbl,\n",
    "                        dtype=tf.float32)\n",
    "        nm = tf.map_fn(lambda n: tf.matmul(tf.gather(normalized_embeddings, [inp]), \n",
    "                                                tf.gather(normalized_embeddings, [n]), \n",
    "                                                transpose_b=True),\n",
    "                            ng,\n",
    "                            dtype=tf.float32)\n",
    "        \n",
    "\n",
    "        s = tf.map_fn(lambda x: tf.log(tf.math.sigmoid(x)), m) \n",
    "\n",
    "        ns = tf.map_fn(lambda x: tf.log(tf.math.sigmoid(-x)), nm) \n",
    "\n",
    "        res = -(tf.math.reduce_sum(ns) + tf.math.reduce_sum(s))\n",
    "\n",
    "        res_lst.append(res)\n",
    "\n",
    "    return tf.stack(res_lst)\n",
    "\n",
    "def tf_train(window_size, embedding_size, num_sampled, batch_size):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        # Input data.\n",
    "        with tf.name_scope('inputs'):\n",
    "            train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "            negative_samples = tf.placeholder(tf.int64, shape=[batch_size, num_sampled])\n",
    "            train_labels = tf.placeholder(tf.int64, shape=[batch_size, window_size])\n",
    "\n",
    "        # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "        with tf.device('/cpu:0'):\n",
    "            # Look up embeddings for inputs.\n",
    "            with tf.name_scope('embeddings'):\n",
    "                embeddings = tf.Variable(\n",
    "                    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "                )\n",
    "                \n",
    "#                 norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "#                 normalized_embeddings = embeddings / norm\n",
    "                \n",
    "                embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "            \n",
    "        with tf.name_scope('loss'):\n",
    "             loss = tf.reduce_mean(\n",
    "#                  loss_fn(\n",
    "#                      batch_size=batch_size, \n",
    "#                      batch_inputs=train_inputs,\n",
    "#                      batch_labels=train_labels,\n",
    "#                      batch_negative=negative_samples,\n",
    "#                      embeddings=embeddings))\n",
    "                tf.nn.nce_loss(\n",
    "                      weights=nce_weights,\n",
    "                      biases=nce_biases,\n",
    "                      labels=train_labels,\n",
    "                      inputs=embed,\n",
    "                      num_sampled=num_sampled,\n",
    "                      num_classes=vocabulary_size,\n",
    "                      num_true=window_size))\n",
    "\n",
    "        # Add the loss value as a scalar to summary.\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        with tf.name_scope('optimizer'):\n",
    "              optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Merge all summaries.\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Create a saver.\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    with tf.Session(graph=graph) as session:     \n",
    "        \n",
    "        # Open a writer to write summaries.\n",
    "        writer = tf.summary.FileWriter(\"tmp\", session.graph)\n",
    "\n",
    "        # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        log.info('Initialized. Embedding size: %s; Num sampled: %s; Window size: %s; Batch size: %s', embedding_size, num_sampled, window_size, batch_size)\n",
    "        average_loss = 0\n",
    "        \n",
    "        for step in xrange(num_steps):\n",
    "            batch_inputs, batch_labels, batch_negative = generate_batch(batch_size, num_sampled, window_size)\n",
    "#             feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels, negative_samples:batch_negative}\n",
    "            feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "            \n",
    "            # Define metadata variable.\n",
    "            run_metadata = tf.RunMetadata()\n",
    "\n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "            # Also, evaluate the merged op to get all summaries from the returned\n",
    "            # \"summary\" variable. Feed metadata variable to session for visualizing\n",
    "            # the graph in TensorBoard.\n",
    "            _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                             feed_dict=feed_dict,\n",
    "                                             run_metadata=run_metadata)     \n",
    "                   \n",
    "            average_loss += loss_val\n",
    "       \n",
    "#             tg = tf.gradients(loss, embeddings)\n",
    "#             print(session.run(tg, feed_dict))\n",
    "    \n",
    "#             print(tf.train.AdagradOptimizer(learning_rate).compute_gradients(loss))\n",
    "#             print(session.run(tf.train.AdagradOptimizer(learning_rate).compute_gradients(loss), feed_dict=feed_dict))\n",
    "#             print(\"====\")\n",
    "#             print(session.run(tf.train.AdagradOptimizer(learning_rate).compute_gradients(loss, var_list=embeddings), feed_dict=feed_dict))\n",
    "          \n",
    "            # Add returned summaries to writer in each step.\n",
    "            writer.add_summary(summary, step)\n",
    "            # Add metadata to visualize the graph for the last run.\n",
    "            if step == (num_steps - 1):\n",
    "                writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                      average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000\n",
    "                # batches.\n",
    "                log.debug('Average loss at step %d: %.4f', step, average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            if step % 10000 == 0:\n",
    "                for id in test_ids:\n",
    "                    norm_ = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "                    normalized_embeddings_ = embeddings / norm_\n",
    "                    final_embeddings = normalized_embeddings_.eval()\n",
    "                    get_closest(final_embeddings, w2i[id])\n",
    "                    print(\"\\n\")\n",
    "\n",
    "        norm_ = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "        normalized_embeddings_ = embeddings / norm_\n",
    "        final_embeddings = normalized_embeddings_.eval()\n",
    "\n",
    "    return graph, final_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, final_embeddings = tf_train(window_size, embedding_size, num_sampled, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def print_random(graph, final_embeddings, window_size, embedding_size, num_sampled):\n",
    "    try:\n",
    "        p = 'data/reports/' + 'es_' + str(embedding_size) + '_ns_' + str(num_sampled) + '_ws_' + str(window_size)\n",
    "        with open(p, 'w') as f:\n",
    "            for i in range(0, 10):\n",
    "                with tf.Session(graph=graph) as session:\n",
    "                    index = len(groups) * i // 10 + randint(0, 100)\n",
    "\n",
    "                    f.write(str(index) + '\\n')\n",
    "                    print(index)\n",
    "\n",
    "                    get_closest(final_embeddings, w2i[most_common[index][0]], f)\n",
    "\n",
    "                f.write('\\n\\n')\n",
    "                print('\\n')\n",
    "    except err:\n",
    "        print(err)\n",
    "        None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_random(graph, final_embeddings, window_size, embedding_size, num_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    for id in test_ids:\n",
    "        get_closest(final_embeddings, w2i[id])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    embedding_sizes = [32, 48, 64]\n",
    "    num_sampled_arr = [32, 48, 64]\n",
    "    window_sizes = [1, 2, 3, 4, 5]\n",
    "    batch_sizes = [128, 128, 132, 128, 130]\n",
    "    for i in xrange(len(embedding_sizes)):\n",
    "        for j in xrange(len(num_sampled_arr)):\n",
    "            for k in xrange(len(window_sizes)):\n",
    "                embedding_size = embedding_sizes[i]\n",
    "                num_sampled = num_sampled_arr[j]\n",
    "                window_size = window_sizes[k]\n",
    "                batch_size = batch_sizes[k]\n",
    "                \n",
    "                graph, final_embeddings = tf_train(window_size, embedding_size, num_sampled, batch_size)\n",
    "                \n",
    "                print_random(window_size, embedding_size, num_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session(graph=graph) as session:\n",
    "#     tf.train.AdagradOptimizer(learning_rate).compute_gradients(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "test_ids=[129440544, 28261334, 92876084, 51016572, 91933860]\n",
    "\n",
    "for i in data:\n",
    "    for j in test_ids:\n",
    "        if j in i[0]:\n",
    "            f = list(i[0])\n",
    "            f.remove(j)\n",
    "            f.append(j)\n",
    "            lst.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lst))\n",
    "print(len(lst[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in xrange(100):\n",
    "    print(get_info(lst[i]))\n",
    "    print()\n",
    "    print(\"==================================================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "\n",
    "# with open('auth/token') as f:\n",
    "#     token = f.readline().strip()\n",
    "\n",
    "# def get_info(ids):\n",
    "#     sleep(0.2)\n",
    "#     mc = 'members_count'\n",
    "#     payload = {'v': '5.92', 'access_token': token, 'fields':mc}\n",
    "    \n",
    "#     str_ids = functools.reduce(\n",
    "#         lambda x, y: x + y,\n",
    "#         lmap(lambda x: str(x) + ',', ids)\n",
    "#     )\n",
    "    \n",
    "#     payload['group_ids'] = str_ids[0:- 1]\n",
    "    \n",
    "#     r = requests.get('https://api.vk.com/method/groups.getById', \n",
    "#                      params=payload)\n",
    "    \n",
    "#     print(r)\n",
    "#     if (not 'response' in r.json()):\n",
    "#         print(r.json())\n",
    "        \n",
    "#     res = lmap(lambda x: (x['name'], x['screen_name'], \"{:,}\".format(x[mc]) if mc in x else -1),r.json()['response'])\n",
    "#     return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
