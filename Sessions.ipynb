{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from six.moves import xrange \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "log = logging.getLogger('log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "lhnd = logging.StreamHandler()\n",
    "lhnd.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "lhnd.setFormatter(formatter)\n",
    "\n",
    "log.addHandler(lhnd)\n",
    "\n",
    "%autonotify -a 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_dumps = False\n",
    "\n",
    "def lmap(f, arr):\n",
    "    return list(map(f, arr))\n",
    "\n",
    "def lfilter(f, arr):\n",
    "    return list(filter(f, arr))\n",
    "\n",
    "def foreach(it, f):\n",
    "    for e in it:\n",
    "        f(e)\n",
    "        \n",
    "def dump(data, name):\n",
    "    with open('data/' + name, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load(name):\n",
    "    with open('data/' + name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def load_or_dump(path, func):\n",
    "    if not Path('data/' + path).exists() or ignore_dumps:\n",
    "        res = func()\n",
    "    \n",
    "        dump(res, path)\n",
    "    else:\n",
    "        res = load(path)\n",
    "        \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "with open('auth/token') as f:\n",
    "    token = f.readline().strip()\n",
    "\n",
    "def get_info(ids):\n",
    "    sleep(0.2)\n",
    "    mc = 'members_count'\n",
    "    payload = {'v': '5.92', 'access_token': token, 'fields':mc}\n",
    "    \n",
    "    str_ids = functools.reduce(\n",
    "        lambda x, y: x + y,\n",
    "        lmap(lambda x: str(x) + ',', ids)\n",
    "    )\n",
    "    \n",
    "    payload['group_ids'] = str_ids[0:- 1]\n",
    "    \n",
    "    r = requests.get('https://api.vk.com/method/groups.getById', \n",
    "                     params=payload)\n",
    "    \n",
    "    if (not 'response' in r.json()):\n",
    "        print(r.json())\n",
    "        \n",
    "    res = lmap(lambda x: (x['name'], x['screen_name'], \"{:,}\".format(x[mc]) if mc in x else -1),r.json()['response'])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 947528\n",
    "\n",
    "def raw_data_filter(file):\n",
    "    # Mapping to events\n",
    "    res = list()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for line in file:\n",
    "        cur = line.rstrip().split(',')\n",
    "        cur = lmap(lambda p: (re.sub(';.*', '', p), re.sub('.*;', '', p)), cur)\n",
    "\n",
    "        session = list()\n",
    "        \n",
    "        for j in range(0, len(cur)):\n",
    "            try:\n",
    "                session.append(int(cur[j][1]))\n",
    "            except ValueError:\n",
    "                None\n",
    "                \n",
    "        res.append(session)\n",
    "\n",
    "        i = i + 1\n",
    "                \n",
    "        if (i % 100000 == 0):\n",
    "            gc.collect()\n",
    "\n",
    "            log.debug(\"%d %% of mapping is done.\", i / total * 100)\n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-17 16:13:14,563 DEBUG 10 % of mapping is done.\n",
      "2019-03-17 16:13:26,355 DEBUG 21 % of mapping is done.\n",
      "2019-03-17 16:13:44,403 DEBUG 31 % of mapping is done.\n",
      "2019-03-17 16:14:00,651 DEBUG 42 % of mapping is done.\n",
      "2019-03-17 16:14:16,835 DEBUG 52 % of mapping is done.\n",
      "2019-03-17 16:14:35,332 DEBUG 63 % of mapping is done.\n",
      "2019-03-17 16:14:53,979 DEBUG 73 % of mapping is done.\n",
      "2019-03-17 16:16:11,982 DEBUG 84 % of mapping is done.\n",
      "2019-03-17 16:17:05,157 DEBUG 94 % of mapping is done.\n",
      "2019-03-17 16:17:09,733 INFO Data loaded\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"6686db44-b61d-4da7-a434-5cc312520f42\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"6686db44-b61d-4da7-a434-5cc312520f42\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": false};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data = load_or_dump('raw', lambda: raw_data_filter(open(\"data/sessions_public.txt\",\"r\")))\n",
    "\n",
    "log.info(\"Data loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_count(data):\n",
    "    total = dict()\n",
    "\n",
    "    for i in data:\n",
    "        for j in i[0]:\n",
    "            if (j in total.keys()):\n",
    "                total[j] = total[j] + 1\n",
    "            else:\n",
    "                total[j] = 1\n",
    "                \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_session_size = 5\n",
    "\n",
    "def initiail_mapping(lst, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set()\n",
    "        malformed = set()\n",
    "        \n",
    "        for event in session:\n",
    "            if (event < 0):\n",
    "                sub_event = -event\n",
    "                \n",
    "                if (sub_event in sub or sub_event in malformed):\n",
    "                    sub.discard(sub_event)\n",
    "                    unsub.discard(sub_event)\n",
    "                    malformed.add(sub_event)\n",
    "                else:\n",
    "                    unsub.add(sub_event)\n",
    "            else:\n",
    "                if (event in unsub or event in malformed):\n",
    "                    unsub.discard(event)\n",
    "                    sub.discard(event)\n",
    "                    malformed.add(event)\n",
    "                else:\n",
    "                    sub.add(event)\n",
    "        \n",
    "        if (len(sub) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "    \n",
    "\n",
    "def set_map(lst, cnt, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set() \n",
    "        \n",
    "        for event in session[0]:\n",
    "            if (cnt[event] > min_allowed):\n",
    "                sub.add(event)\n",
    "                \n",
    "        for event in session[1]:\n",
    "            if (cnt.get(event, -1) > min_allowed):\n",
    "                unsub.add(event)    \n",
    "        \n",
    "        if (len(sub) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def drop_uncommon(raw_data, min_allowed = 10):\n",
    "    cnt = None\n",
    "    sorted_cnt = None\n",
    "    \n",
    "    data, groups = initiail_mapping(raw_data, min_allowed)\n",
    "    cnt = group_count(data) \n",
    "    sorted_cnt = sorted(list(cnt.values()))\n",
    "    \n",
    "    while (cnt == None or sorted_cnt[0] < min_allowed):\n",
    "        data, groups = set_map(data, cnt, min_allowed)\n",
    "                \n",
    "        cnt = group_count(data) \n",
    "        sorted_cnt = sorted(list(cnt.values()))\n",
    "        \n",
    "        log.info(\"Length of data:   %d\", len(data))\n",
    "        log.info(\"Total length:     %d\", \n",
    "                functools.reduce((lambda x, y: x + y), lmap(lambda a: len(a), data))\n",
    "                )\n",
    "        log.info(\"Number of groups: %d\", len(groups))\n",
    "        log.info(\"Minimum count:    %d\\n\", sorted_cnt[0])\n",
    "        \n",
    "    return data, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-17 16:55:32,370 INFO Length of data:   318524\n",
      "2019-03-17 16:55:32,450 INFO Total length:     637048\n",
      "2019-03-17 16:55:32,450 INFO Number of groups: 50062\n",
      "2019-03-17 16:55:32,451 INFO Minimum count:    16\n",
      "\n",
      "2019-03-17 16:55:45,852 INFO Length of data:   316823\n",
      "2019-03-17 16:55:45,925 INFO Total length:     633646\n",
      "2019-03-17 16:55:45,926 INFO Number of groups: 48561\n",
      "2019-03-17 16:55:45,926 INFO Minimum count:    42\n",
      "\n",
      "2019-03-17 16:56:03,204 INFO Length of data:   316710\n",
      "2019-03-17 16:56:03,278 INFO Total length:     633420\n",
      "2019-03-17 16:56:03,279 INFO Number of groups: 48490\n",
      "2019-03-17 16:56:03,279 INFO Minimum count:    44\n",
      "\n",
      "2019-03-17 16:56:20,517 INFO Length of data:   316686\n",
      "2019-03-17 16:56:20,591 INFO Total length:     633372\n",
      "2019-03-17 16:56:20,592 INFO Number of groups: 48482\n",
      "2019-03-17 16:56:20,592 INFO Minimum count:    44\n",
      "\n",
      "2019-03-17 16:56:38,089 INFO Length of data:   316666\n",
      "2019-03-17 16:56:38,167 INFO Total length:     633332\n",
      "2019-03-17 16:56:38,167 INFO Number of groups: 48479\n",
      "2019-03-17 16:56:38,168 INFO Minimum count:    45\n",
      "\n",
      "2019-03-17 16:56:55,847 INFO Length of data:   316632\n",
      "2019-03-17 16:56:55,918 INFO Total length:     633264\n",
      "2019-03-17 16:56:55,918 INFO Number of groups: 48474\n",
      "2019-03-17 16:56:55,919 INFO Minimum count:    41\n",
      "\n",
      "2019-03-17 16:57:09,311 INFO Length of data:   316619\n",
      "2019-03-17 16:57:09,383 INFO Total length:     633238\n",
      "2019-03-17 16:57:09,384 INFO Number of groups: 48470\n",
      "2019-03-17 16:57:09,384 INFO Minimum count:    50\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"61eae677-f2d8-4316-ade9-b183f4620bbf\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"61eae677-f2d8-4316-ade9-b183f4620bbf\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": false};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ignore_dumps = True\n",
    "data, groups = load_or_dump('final_data', lambda: drop_uncommon(raw_data, 50))\n",
    "\n",
    "most_common = sorted(group_count(data).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "w2i = {w: i for i, w in enumerate(groups)}\n",
    "i2w = {i: w for i, w in enumerate(groups)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = None\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dex = 0\n",
    "event_dex = 0\n",
    "\n",
    "def generate_batch(batch_size, window_size = 1):\n",
    "    assert min_session_size >= window_size * 2 + 1 \n",
    "    assert batch_size % (window_size * 2) == 0\n",
    "    \n",
    "    global session_dex\n",
    "    global event_dex\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    if (event_dex == 0):\n",
    "        event_dex = window_size\n",
    "    \n",
    "    current = 0\n",
    "    session = list(data[session_dex][0])\n",
    "    \n",
    "    while (current < batch_size):\n",
    "        i = 0\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if (j != 0):\n",
    "                batch[current + i] = w2i[session[event_dex + j]]\n",
    "                labels[current + i][0] = w2i[session[event_dex]]\n",
    "                i += 1\n",
    "\n",
    "        event_dex += 1\n",
    "        current += window_size * 2\n",
    "\n",
    "        if (event_dex + window_size >= len(session)):\n",
    "            event_dex = window_size\n",
    "            session_dex = session_dex + 1\n",
    "            if (session_dex >= len(data)):\n",
    "                session_dex = 0\n",
    "            session = list(data[session_dex][0])\n",
    "\n",
    "     \n",
    "    return batch, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({23372133, 1959, 20650061, 70034991, 22741624, 35540891, 75909948}, set())\n",
      "({25794755, 91683885, 34812270, 39325103, 104237982, 34523318, 42533142, 46755517, 49128190}, set())\n",
      "15333 23372133 -> 26475 20650061\n",
      "745 1959 -> 26475 20650061\n",
      "15795 70034991 -> 26475 20650061\n",
      "24463 22741624 -> 26475 20650061\n",
      "745 1959 -> 15795 70034991\n",
      "26475 20650061 -> 15795 70034991\n",
      "24463 22741624 -> 15795 70034991\n",
      "7531 35540891 -> 15795 70034991\n",
      "26475 20650061 -> 24463 22741624\n",
      "15795 70034991 -> 24463 22741624\n"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(16, 2)\n",
    "\n",
    "print(data[0])\n",
    "print(data[1])\n",
    "\n",
    "for i in range(10):\n",
    "    print(batch[i], i2w[batch[i]], '->', labels[i, 0],\n",
    "          i2w[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data = None\n",
    "\n",
    "learning_rate = 1\n",
    "vocabulary_size = len(groups)\n",
    "\n",
    "window_size = 3\n",
    "embedding_size = 48\n",
    "num_sampled = 64\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(emb, index, f = None):\n",
    "    p = emb[index]\n",
    "    cnst = tf.constant(p, shape=[1, embedding_size])\n",
    "    d = tf.matmul(cnst, emb, transpose_b=True).eval()[0]\n",
    "\n",
    "    dxs = np.argsort(np.array(d))\n",
    "    \n",
    "    ids = []\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(dxs) - 10, len(dxs)):\n",
    "        ids.append(i2w[dxs[i]])\n",
    "        res.append(d[dxs[i]])\n",
    "        \n",
    "    info = get_info(ids)\n",
    "    \n",
    "    for i in xrange(len(res)):\n",
    "        print(ids[i], ' ', res[i], ' ', info[i])\n",
    "        \n",
    "        if (f != None):\n",
    "            f.write(str(ids[i]) + ' ' + str(res[i]) + ' ' + str(info[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 200000\n",
    "\n",
    "def tf_train(window_size, embedding_size, num_sampled, batch_size):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        # Input data.\n",
    "        with tf.name_scope('inputs'):\n",
    "            train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "            train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "        # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "        with tf.device('/cpu:0'):\n",
    "            # Look up embeddings for inputs.\n",
    "            with tf.name_scope('embeddings'):\n",
    "                embeddings = tf.Variable(\n",
    "                    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "                )\n",
    "                embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "          # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        # Explanation of the meaning of NCE loss:\n",
    "        #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(\n",
    "                      weights=nce_weights,\n",
    "                      biases=nce_biases,\n",
    "                      labels=train_labels,\n",
    "                      inputs=embed,\n",
    "                      num_sampled=num_sampled,\n",
    "                      num_classes=vocabulary_size))\n",
    "\n",
    "            # Add the loss value as a scalar to summary.\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        with tf.name_scope('optimizer'):\n",
    "              optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Compute the cosine similarity between minibatch examples and all\n",
    "        # embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        #valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "        #                                          valid_dataset)\n",
    "        #similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        # Merge all summaries.\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Create a saver.\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    with tf.Session(graph=graph) as session:     \n",
    "        \n",
    "        # Open a writer to write summaries.\n",
    "        writer = tf.summary.FileWriter(\"tmp\", session.graph)\n",
    "\n",
    "        # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        log.info('Initialized. Embedding size: %s; Num sampled: %s; Window size: %s; Batch size: %s', embedding_size, num_sampled, window_size, batch_size)\n",
    "        average_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "            batch_inputs, batch_labels = generate_batch(batch_size)\n",
    "            feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "            # Define metadata variable.\n",
    "            run_metadata = tf.RunMetadata()\n",
    "\n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "            # Also, evaluate the merged op to get all summaries from the returned\n",
    "            # \"summary\" variable. Feed metadata variable to session for visualizing\n",
    "            # the graph in TensorBoard.\n",
    "            _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                             feed_dict=feed_dict,\n",
    "                                             run_metadata=run_metadata)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            # Add returned summaries to writer in each step.\n",
    "            writer.add_summary(summary, step)\n",
    "            # Add metadata to visualize the graph for the last run.\n",
    "            if step == (num_steps - 1):\n",
    "                writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                      average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000\n",
    "                # batches.\n",
    "                log.debug('Average loss at step %d: %.4f', step, average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            #if step % 20000 == 0 and step != 0:\n",
    "                #print('Most closest to ', most_common[0][0])\n",
    "                #get_closest(normalized_embeddings.eval(), w2i[most_common[0][0]])\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    return graph, final_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-17 17:33:02,152 INFO Initialized. Embedding size: 48; Num sampled: 64; Window size: 3; Batch size: 128\n",
      "2019-03-17 17:33:02,237 DEBUG Average loss at step 0: 291.5664\n",
      "2019-03-17 17:33:04,284 DEBUG Average loss at step 2000: 138.7917\n",
      "2019-03-17 17:33:06,306 DEBUG Average loss at step 4000: 60.2892\n",
      "2019-03-17 17:33:08,273 DEBUG Average loss at step 6000: 34.3937\n",
      "2019-03-17 17:33:10,218 DEBUG Average loss at step 8000: 21.7610\n",
      "2019-03-17 17:33:12,160 DEBUG Average loss at step 10000: 16.1726\n",
      "2019-03-17 17:33:14,119 DEBUG Average loss at step 12000: 11.7242\n",
      "2019-03-17 17:33:16,064 DEBUG Average loss at step 14000: 9.6428\n",
      "2019-03-17 17:33:18,012 DEBUG Average loss at step 16000: 8.0565\n",
      "2019-03-17 17:33:19,967 DEBUG Average loss at step 18000: 6.6830\n",
      "2019-03-17 17:33:21,915 DEBUG Average loss at step 20000: 6.3439\n",
      "2019-03-17 17:33:23,859 DEBUG Average loss at step 22000: 5.6677\n",
      "2019-03-17 17:33:25,818 DEBUG Average loss at step 24000: 5.5724\n",
      "2019-03-17 17:33:27,768 DEBUG Average loss at step 26000: 4.7339\n",
      "2019-03-17 17:33:29,717 DEBUG Average loss at step 28000: 4.8709\n",
      "2019-03-17 17:33:31,672 DEBUG Average loss at step 30000: 4.6538\n",
      "2019-03-17 17:33:33,620 DEBUG Average loss at step 32000: 4.1191\n",
      "2019-03-17 17:33:35,602 DEBUG Average loss at step 34000: 4.0771\n",
      "2019-03-17 17:33:37,554 DEBUG Average loss at step 36000: 4.2215\n",
      "2019-03-17 17:33:39,511 DEBUG Average loss at step 38000: 4.1722\n",
      "2019-03-17 17:33:41,461 DEBUG Average loss at step 40000: 3.8594\n",
      "2019-03-17 17:33:43,412 DEBUG Average loss at step 42000: 4.1693\n",
      "2019-03-17 17:33:45,361 DEBUG Average loss at step 44000: 3.9134\n",
      "2019-03-17 17:33:47,333 DEBUG Average loss at step 46000: 3.8515\n",
      "2019-03-17 17:33:49,287 DEBUG Average loss at step 48000: 3.9657\n",
      "2019-03-17 17:33:51,250 DEBUG Average loss at step 50000: 3.7647\n",
      "2019-03-17 17:33:53,200 DEBUG Average loss at step 52000: 3.9665\n",
      "2019-03-17 17:33:55,155 DEBUG Average loss at step 54000: 4.0310\n",
      "2019-03-17 17:33:57,115 DEBUG Average loss at step 56000: 3.6548\n",
      "2019-03-17 17:33:59,084 DEBUG Average loss at step 58000: 3.8732\n",
      "2019-03-17 17:34:01,044 DEBUG Average loss at step 60000: 3.8048\n",
      "2019-03-17 17:34:03,003 DEBUG Average loss at step 62000: 3.5287\n",
      "2019-03-17 17:34:04,969 DEBUG Average loss at step 64000: 3.7206\n",
      "2019-03-17 17:34:06,929 DEBUG Average loss at step 66000: 3.7426\n",
      "2019-03-17 17:34:08,899 DEBUG Average loss at step 68000: 3.4469\n",
      "2019-03-17 17:34:10,868 DEBUG Average loss at step 70000: 3.4923\n",
      "2019-03-17 17:34:12,841 DEBUG Average loss at step 72000: 3.6577\n",
      "2019-03-17 17:34:14,806 DEBUG Average loss at step 74000: 3.5791\n",
      "2019-03-17 17:34:16,768 DEBUG Average loss at step 76000: 3.2411\n",
      "2019-03-17 17:34:18,742 DEBUG Average loss at step 78000: 3.3315\n",
      "2019-03-17 17:34:20,734 DEBUG Average loss at step 80000: 3.4068\n",
      "2019-03-17 17:34:22,768 DEBUG Average loss at step 82000: 3.4257\n",
      "2019-03-17 17:34:25,196 DEBUG Average loss at step 84000: 3.2619\n",
      "2019-03-17 17:34:27,422 DEBUG Average loss at step 86000: 3.6116\n",
      "2019-03-17 17:34:29,479 DEBUG Average loss at step 88000: 3.6451\n",
      "2019-03-17 17:34:31,483 DEBUG Average loss at step 90000: 3.3906\n",
      "2019-03-17 17:34:33,478 DEBUG Average loss at step 92000: 3.2935\n",
      "2019-03-17 17:34:35,648 DEBUG Average loss at step 94000: 3.3106\n",
      "2019-03-17 17:34:37,725 DEBUG Average loss at step 96000: 3.2402\n",
      "2019-03-17 17:34:39,921 DEBUG Average loss at step 98000: 3.1753\n",
      "2019-03-17 17:34:42,050 DEBUG Average loss at step 100000: 3.0757\n",
      "2019-03-17 17:34:44,141 DEBUG Average loss at step 102000: 3.0445\n",
      "2019-03-17 17:34:46,207 DEBUG Average loss at step 104000: 3.0044\n",
      "2019-03-17 17:34:48,300 DEBUG Average loss at step 106000: 3.0174\n",
      "2019-03-17 17:34:50,427 DEBUG Average loss at step 108000: 2.9757\n",
      "2019-03-17 17:34:52,543 DEBUG Average loss at step 110000: 2.9472\n",
      "2019-03-17 17:34:54,633 DEBUG Average loss at step 112000: 2.8875\n",
      "2019-03-17 17:34:56,793 DEBUG Average loss at step 114000: 2.8538\n",
      "2019-03-17 17:34:58,882 DEBUG Average loss at step 116000: 2.8155\n",
      "2019-03-17 17:35:01,001 DEBUG Average loss at step 118000: 2.8102\n",
      "2019-03-17 17:35:03,095 DEBUG Average loss at step 120000: 2.7629\n",
      "2019-03-17 17:35:05,265 DEBUG Average loss at step 122000: 2.7366\n",
      "2019-03-17 17:35:07,493 DEBUG Average loss at step 124000: 2.7104\n",
      "2019-03-17 17:35:09,992 DEBUG Average loss at step 126000: 2.6980\n",
      "2019-03-17 17:35:12,325 DEBUG Average loss at step 128000: 2.6259\n",
      "2019-03-17 17:35:14,484 DEBUG Average loss at step 130000: 2.6207\n",
      "2019-03-17 17:35:16,668 DEBUG Average loss at step 132000: 2.6090\n",
      "2019-03-17 17:35:18,807 DEBUG Average loss at step 134000: 2.5523\n",
      "2019-03-17 17:35:20,983 DEBUG Average loss at step 136000: 2.5901\n",
      "2019-03-17 17:35:23,154 DEBUG Average loss at step 138000: 2.6010\n",
      "2019-03-17 17:35:25,266 DEBUG Average loss at step 140000: 2.6142\n",
      "2019-03-17 17:35:27,447 DEBUG Average loss at step 142000: 2.5278\n",
      "2019-03-17 17:35:29,616 DEBUG Average loss at step 144000: 2.4584\n",
      "2019-03-17 17:35:31,744 DEBUG Average loss at step 146000: 2.4354\n",
      "2019-03-17 17:35:33,877 DEBUG Average loss at step 148000: 2.4113\n",
      "2019-03-17 17:35:36,100 DEBUG Average loss at step 150000: 2.4043\n",
      "2019-03-17 17:35:38,220 DEBUG Average loss at step 152000: 2.4222\n",
      "2019-03-17 17:35:40,307 DEBUG Average loss at step 154000: 2.3277\n",
      "2019-03-17 17:35:42,538 DEBUG Average loss at step 156000: 2.3607\n",
      "2019-03-17 17:35:44,631 DEBUG Average loss at step 158000: 2.3666\n",
      "2019-03-17 17:35:46,734 DEBUG Average loss at step 160000: 2.3990\n",
      "2019-03-17 17:35:48,889 DEBUG Average loss at step 162000: 2.3989\n",
      "2019-03-17 17:35:50,982 DEBUG Average loss at step 164000: 2.4070\n",
      "2019-03-17 17:35:53,117 DEBUG Average loss at step 166000: 2.3981\n",
      "2019-03-17 17:35:55,218 DEBUG Average loss at step 168000: 2.3090\n",
      "2019-03-17 17:35:57,442 DEBUG Average loss at step 170000: 2.3041\n",
      "2019-03-17 17:35:59,644 DEBUG Average loss at step 172000: 2.2842\n",
      "2019-03-17 17:36:01,826 DEBUG Average loss at step 174000: 2.2925\n",
      "2019-03-17 17:36:03,986 DEBUG Average loss at step 176000: 2.3579\n",
      "2019-03-17 17:36:06,152 DEBUG Average loss at step 178000: 2.4214\n",
      "2019-03-17 17:36:08,314 DEBUG Average loss at step 180000: 2.3908\n",
      "2019-03-17 17:36:10,559 DEBUG Average loss at step 182000: 2.5324\n",
      "2019-03-17 17:36:12,790 DEBUG Average loss at step 184000: 2.6055\n",
      "2019-03-17 17:36:14,977 DEBUG Average loss at step 186000: 2.6555\n",
      "2019-03-17 17:36:17,095 DEBUG Average loss at step 188000: 2.7557\n",
      "2019-03-17 17:36:19,317 DEBUG Average loss at step 190000: 3.0258\n",
      "2019-03-17 17:36:21,459 DEBUG Average loss at step 192000: 3.1621\n",
      "2019-03-17 17:36:23,541 DEBUG Average loss at step 194000: 2.9362\n",
      "2019-03-17 17:36:25,632 DEBUG Average loss at step 196000: 2.8006\n",
      "2019-03-17 17:36:27,855 DEBUG Average loss at step 198000: 2.7170\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"ca95a2f0-2ed7-45ff-8fa2-4294ec7c80f8\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"ca95a2f0-2ed7-45ff-8fa2-4294ec7c80f8\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": false};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph, final_embeddings = tf_train(window_size, embedding_size, num_sampled, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def print_random(graph, final_embeddings, window_size, embedding_size, num_sampled):\n",
    "    try:\n",
    "        p = 'data/reports/' + 'es_' + str(embedding_size) + '_ns_' + str(num_sampled) + '_ws_' + str(window_size)\n",
    "        with open(p, 'w') as f:\n",
    "            for i in range(0, 10):\n",
    "                with tf.Session(graph=graph) as session:\n",
    "                    index = len(groups) * i // 10 + randint(0, 100)\n",
    "\n",
    "                    f.write(str(index) + '\\n')\n",
    "                    print(index)\n",
    "\n",
    "                    get_closest(final_embeddings, w2i[most_common[index][0]], f)\n",
    "\n",
    "                f.write('\\n\\n')\n",
    "                print('\\n')\n",
    "    except err:\n",
    "        print(err)\n",
    "        None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "77093415   0.76223373   ('–õ–∞–π—Ñ—Ö–∞–∫—É–º | –°–æ–≤–µ—Ç—ã, —Ö–∏—Ç—Ä–æ—Å—Ç–∏, –∏–¥–µ–∏', 'vk_lifehack_club', '1,266,203')\n",
      "42440233   0.77038264   ('–ú—É–∑—ã–∫–∞', 'exp.music', '1,789,340')\n",
      "124999723   0.7732413   ('–°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –∏ —Ä–µ–º–æ–Ω—Ç', 'stroy_ok', '567,107')\n",
      "54391852   0.77620864   ('–ü–∞–¥–∏–∫', 'club54391852', '1,641,563')\n",
      "85087785   0.7827162   ('AuRuM TV Àñ Clash Royale Àñ Brawl Stars', 'aurum_tv', '735,320')\n",
      "158473256   0.79303634   ('–∫–∞–∫ —Ç–∞–∫?', 'hzkaktak', '875,764')\n",
      "26419239   0.79396236   ('–°–º–µ–π—Å—è –¥–æ —Å–ª—ë–∑ :D', 'ifun', '11,186,708')\n",
      "35061290   0.8380492   ('–≠–≥–æ–∏—Å—Ç', 'e_goist', '4,756,878')\n",
      "94255146   0.880185   ('–†–µ–∞–ª—å–Ω–æ —Å–º–µ—à–Ω–æ', 'onovoe', '1,984,204')\n",
      "133668394   0.99999994   ('–ó–∞–±—Ä–æ—à–µ–Ω–Ω–æ–µ', 'zabroshenoevk', '3,096,028')\n",
      "\n",
      "\n",
      "4867\n",
      "94943864   0.69538283   ('12 —Å—Ç—É–ª—å–µ–≤', 'twelve_h', '456,835')\n",
      "106277494   0.7013999   ('–ê–∑–±—É–∫–∞ —Ä–µ–º–æ–Ω—Ç–∞ | –°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', 'azbuka.remonta', '1,173,632')\n",
      "159450742   0.7036063   ('–†–∏—Ñ–º–∞—á', 'vizhu_rifmi', '877,194')\n",
      "156222072   0.7179694   ('–ü–æ–∑–æ—Ä–Ω–æ', 'pozornnoo', '923,786')\n",
      "151414392   0.72471106   ('–ê–Ω–æ–º–∞–ª—å–Ω–æ–µ', 'anomalii_anomalnoe', '572,011')\n",
      "39236729   0.7331387   ('–ü–æ–ª–Ω—ã–π –ë–∞–∫', 'bak', '1,371,098')\n",
      "76520569   0.73658323   ('69 –ü–û–•', 'sex.erotika.porno', '1,516,716')\n",
      "22741624   0.7420393   ('–£–ª–µ—Ç–Ω—ã–µ –ø—Ä–∏–∫–æ–ª—ã', 'humour.page', '5,214,742')\n",
      "141149815   0.7694683   ('–ö–ê–ô–§', 'mykaiff', '527,373')\n",
      "160588921   1.0   ('Raketa', 'findoriginal', '312,949')\n",
      "\n",
      "\n",
      "9781\n",
      "40928748   0.6490109   ('–ú“±“õ–∞“ì–∞–ª–∏ –ú–∞“õ–∞—Ç–∞–µ–≤', 'mucagali', '222,431')\n",
      "166568656   0.65195054   ('–ü–æ—à–ª—ã–µ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞ | —é–Ω—ã–µ —Å—Ç—É–¥–µ–Ω—Ç–∫–∏ —à–∫—É—Ä—ã', 'club166568656', -1)\n",
      "68030706   0.6530079   ('–¢–∏–ø–∏—á–Ω—ã–π –°–æ—á–∏', 'this_is_sochi', '169,245')\n",
      "132582362   0.6543877   ('H I P I S H. üí™üèª', 'hipish_tut', '222,685')\n",
      "134385259   0.65533495   ('–¢–û–ü –§–ò–õ–¨–ú–´', 'topfilm2019', '47,357')\n",
      "67463766   0.66852856   ('–†–∞–±–æ—Ç–∞ –≤ –ê—Å—Ç—Ä–∞—Ö–∞–Ω–∏', 'rg.astrahan', '29,484')\n",
      "82478081   0.6692716   ('—Ç–≤–æ—è –ø–∏–∫—á–∞ „É†', 'your_piccha', '49,782')\n",
      "61918436   0.6796011   ('–ü–æ—á–µ–º—É?', 'pochemui', '151,093')\n",
      "35814059   0.7099612   ('Mary Kay¬Æ –†–æ—Å—Å–∏—è', 'mkrussia', '164,916')\n",
      "129393663   0.99999994   ('Crossover Art | –ê–Ω–∏–º–µ', 'crossover_arts', '103,098')\n",
      "\n",
      "\n",
      "14626\n",
      "110402963   0.64257854   ('–º—ã —Ä–∞–∑–±–∏–≤–∞–µ–º—Å—è', 'wearebreaking', '46,705')\n",
      "147207430   0.6452425   (\"–ü—É–ø'—Å —Å–µ–º—å–∏\", 'pups_family', '464,406')\n",
      "64171147   0.6473814   ('–ù–µ —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã', 'ms_ink', '326,230')\n",
      "62433315   0.6529431   ('–ó–õ–û–ô –ü–ê–†–ò–ö–ú–ê–•–ï–†', 'zloyparikmaher', '257,922')\n",
      "29823745   0.6544262   ('–í–ì–ò–ö', 'komment', '139,016')\n",
      "141739215   0.6551107   ('–ü–æ—Ä–Ω–æ', 'autowasted', -1)\n",
      "39782666   0.6567144   ('MADMEN', 'madmen', '536,657')\n",
      "39329944   0.6616489   ('Eburg ‚ôî', 'onw2w2g8vwu', -1)\n",
      "68307720   0.6782447   ('–ö–∏–Ω–æ –¶–∏—Ç–∞—Ç—ã', 'kin_cit', '496,998')\n",
      "21245447   0.99999976   ('–§–æ–±–æ—Å. –ö–∞—Ç–∞–∫–ª–∏–∑–º—ã –∏ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ã –ø—Ä–∏—Ä–æ–¥—ã.', 'fobosplanet', '126,628')\n",
      "\n",
      "\n",
      "19477\n",
      "134077574   0.6353128   ('–ú–µ–¥–≤–µ–∂–∏–π —Ñ–∞—Ä—à +18 (R17)', 'medfarshon', '35,142')\n",
      "108945544   0.6356422   ('–¢.–ú. Tushe (–ú–∞–≥–∞–∑–∏–Ω , —Ä–µ—Ü–µ–ø—Ç—ã)', 'tushe_od', '39,419')\n",
      "105552905   0.63671076   ('VIDEO Like', 'tmbrphoto', '87,073')\n",
      "88668229   0.64370716   ('–ú–µ–±–µ–ª—å –ù–∞–±–µ—Ä–µ–∂–Ω—ã–µ –ß–µ–ª–Ω—ã', 'mebel.optom116', '29,390')\n",
      "157775369   0.64420503   ('–£–º–∞—Ç–Ω–∞—è –ú–∞—Ç–∏–ª—å–¥–∞', 'umatnayam', '1,020,811')\n",
      "99923339   0.65311486   ('–ë–µ—Å–ø–ª–∞—Ç–Ω–æ –¥–µ—Ç—è–º', 'ymommy', -1)\n",
      "31998388   0.65532076   ('–ö–£–ö–õ–ê-–®–ê–†–ñ.–†–§‚Ñ¢ | –°—Ç–∞—Ç—É—ç—Ç–∫–∏ –ø–æ —Ñ–æ—Ç–æ –Ω–∞ –∑–∞–∫–∞–∑', 'kukla_sharzh_ru', '34,772')\n",
      "96973296   0.65843666   ('–î–ò–ö–ò–ï 18+', 'club96973296', '31,923')\n",
      "12673560   0.6594181   ('–î—Ä—É–≥–í–æ–∫—Ä—É–≥ - —á–∞—Ç, –∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞ –∏ –æ–±—â–µ–Ω–∏–µ', 'drugvokrug_ru', '242,699')\n",
      "25612449   1.0   ('–ú—ã ‚Äî –†–£–°–°–ö–ò–ï', 'are_we_russian', '153,177')\n",
      "\n",
      "\n",
      "24334\n",
      "72271018   0.65659165   ('Nike –∏–∑ –°–®–ê', 'nikesneakersstore', -1)\n",
      "76539541   0.6574575   ('HERBALIFE | –ì–ï–†–ë–ê–õ–ê–ô–§', 'herbalifeofficial', '89,127')\n",
      "157142840   0.6597764   ('–§–∞–∫—Ç—ã –î–µ–≥–µ–Ω–µ—Ä–∞—Ç–∞', 'club157142840', '1,282,593')\n",
      "54993073   0.66494966   ('–ú—ã –ø–µ—Ä–µ–µ—Ö–∞–ª–∏ —Å—é–¥–∞: vk.com/pac_brat', 'cult_poc', '327,070')\n",
      "33926577   0.6655837   ('–°–û–í–ú–ï–°–¢–ù–´–ï –ü–û–ö–£–ü–ö–ò', 'sovmestnye_pokupki_5', '62,152')\n",
      "159687985   0.6741018   ('–°–∞–º—ã–π –≥—Ä—É—Å—Ç–Ω—ã–π', 'grustnenkoaga', '127,929')\n",
      "33788217   0.6741156   ('\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c', 'emptiness', '1,053,357')\n",
      "54756276   0.7031802   ('LIBERTY | –ú–û–î–ï–õ–ò | –ì–û–õ–´–ï –î–ï–í–£–®–ö–ò', 'liberty18', '85,585')\n",
      "26062647   0.7311721   ('Kate Mobile', 'kate_mobile', '6,000,455')\n",
      "99009078   1.0   ('Lena Golovach', 'golovachtv', '46,255')\n",
      "\n",
      "\n",
      "29179\n",
      "70137822   0.628206   ('–±–µ–ª–∞—è–Ω–µ–∂–Ω–æ—Å—Ç—å', 'belnj', '124,351')\n",
      "34441005   0.6292951   ('ƒígor Kreed ‚Ä¢ –ï–≥–æ—Ä –ö—Ä–∏–¥', 'kreedegorblack', '34,214')\n",
      "121178293   0.6344591   ('–ú–∞—Ç–µ—Ä–∏–Ω—Å—Ç–≤–æ | –º–∞–º—ã –∏ –¥–µ—Ç–∏', 'motherho', '30,106')\n",
      "16317935   0.6377406   ('LoveYouDress –ú–∞–≥–∞–∑–∏–Ω –∫—Ä–∞—Å–∏–≤–æ–π –∂–µ–Ω—Å–∫–æ–π –æ–¥–µ–∂–¥—ã', 'russian_fashion_shop', '83,369')\n",
      "34468726   0.64409876   ('–ü—Ä–∏—á–µ—Å–∫–∏', 'escanlee', '508,937')\n",
      "16556086   0.65172696   ('*–ó–î–û–†–û–í–ê–Ø –ö–û–ñ–ê* –ú–ê–°–ö–ò –î–õ–Ø –õ–ò–¶–ê*–£–•–û–î –ó–ê –ö–û–ñ–ï–ô*–°–ü–±', 'cleanbody', '92,415')\n",
      "67423687   0.6543195   ('BEST–ö–ê–î–† –ü—É—Ç–µ—à–µ—Å—Ç–≤–∏—è | –ì–æ—Ä—è—â–∏–µ –¢—É—Ä—ã | –ü—É—Ç–µ–≤–∫–∏', 'bestcadr', '176,129')\n",
      "51945689   0.6681634   ('–ü–∏—Ü—Ü–∞ –°–∏–Ω–∏—Ü–∞\\xa0| –î–æ—Å—Ç–∞–≤–∫–∞ –ø–∏—Ü—Ü—ã –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫', 'pizzasinizza', '27,632')\n",
      "60074553   0.7189444   ('Tiger Bets | –°—Ç–∞–≤–∫–∏ –Ω–∞ —Å–ø–æ—Ä—Ç | LIVE –ø—Ä–æ–≥–Ω–æ–∑—ã', 'tytdobro', -1)\n",
      "7734336   1.0000001   ('STIGMA SHOW —Å–≤–µ—Ç–æ–≤–æ–µ, –æ–≥–Ω–µ–Ω–Ω–æ–µ, –∑–µ—Ä–∫–∞–ª—å–Ω–æ–µ —à–æ—É.', 'wowstigmashow', '49,917')\n",
      "\n",
      "\n",
      "34003\n",
      "166163423   0.6226273   ('‚ûõ–°–∏–º–ø–∞ –ø–∞ –ø–∞ –ø–æ–ª—é–±–∏–ª–∞ |‚ôö|', 'detkadetka0', '83,931')\n",
      "101720721   0.63280165   ('–ó–Ω–∞–∫–æ–º—Å—Ç–≤–∞ –¥–ª—è –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤ –∏ —à–∫–æ–ª—å–Ω–∏–∫–æ–≤', 'club101720721', '19,522')\n",
      "77471290   0.6333336   ('–ü—Ä–∏–∑–Ω–∞–≤–∞—à–∫–∏ ‚ô•  –ê–∫—Ç–æ–±–µ', 'live_aktobe', '31,586')\n",
      "59165594   0.63597775   ('Calzedonia', 'calzedoniaofficial', '44,547')\n",
      "168540915   0.6363076   ('POPASS', 'club168540915', '115,145')\n",
      "108549341   0.6370534   (\"–î–æ–º–∏–Ω–æ'—Å –ü–∏—Ü—Ü–∞ –†–æ—Å—Å–∏—è\", 'dominospizzarussia', '30,439')\n",
      "146854291   0.6456125   ('FaweMc ¬ª –ú–∞–π–Ω–∫—Ä–∞—Ñ—Ç —Å–µ—Ä–≤–µ—Ä–∞', 'fawe_mc', '38,018')\n",
      "27960267   0.64895475   ('–¢–æ–º –•–∏–¥–¥–ª—Å—Ç–æ–Ω ‚ùñ Hiddlestown ‚ùñ Tom Hiddleston', 'hiddlestown', '46,699')\n",
      "80127821   0.6559217   ('–†–∞–±–æ—Ç–∞ –∏ –ü—Ä–∞–∫—Ç–∏–∫–∞ –∑–∞ –ì—Ä–∞–Ω–∏—Ü–µ–π AWI Agency', 'rabota_awi', '36,551')\n",
      "83733599   1.0000001   ('–ú—É–∂—Å–∫–∏–µ –º—ã—Å–ª–∏', 'menym', '80,405')\n",
      "\n",
      "\n",
      "38800\n",
      "37247645   0.6426229   ('—Ç–≤–æ—è –º—É–∑—ã–∫–∞ - —Ç–≤–æ—ë –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ', 'best_musical1', '1,054,958')\n",
      "26170520   0.6458631   ('–ö–∞—Ä–ª–æ—Å –ö–∞—Å—Ç–∞–Ω–µ–¥–∞ | –°–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏–µ', 'carlos_salvador_aranha_castaneda', '275,203')\n",
      "51318460   0.6495832   ('–õ–∞–π–∫ –¢–∞–π–º | –õ–¢', 'twi75', '101,384')\n",
      "20682901   0.64971554   ('–¢—é—Ä—è–≥–∞ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –≥—Ä—É–ø–ø–∞ –∏–≥—Ä—ã)', 'club20682901', '538,964')\n",
      "149127842   0.6558092   ('–ß–ê–°–¢–ù–û–ï', 'hakintim', '67,905')\n",
      "158036631   0.6657994   ('–ë–æ–ª—å—à–∞—è –∫–Ω–∏–≥–∞ –∑–Ω–∞—Ö–∞—Ä—è', 'znaharj', '458,778')\n",
      "33816639   0.66714394   ('–ë–µ—Å–µ–¥—ã –í–ö', 'besedvk', '68,549')\n",
      "115545298   0.6752353   ('Clash Royale | –ü—Ä–æ–¥–∞–∂–∞ –ê–∫–∫–∞—É–Ω—Ç–æ–≤', 'clash.royale.accounts', '47,354')\n",
      "151683389   0.6888839   ('–ì–∞—Ä–∞–∂', 'club151683389', '78,079')\n",
      "132812953   1.0   ('–ú–∞—Å—Ç–µ—Ä-–ú–æ–¥–µ–ª—å –ú–æ—Å–∫–≤–∞', 'mastmode', '24,011')\n",
      "\n",
      "\n",
      "43692\n",
      "70228347   0.6389533   ('–ú–æ—Å–∫–æ–≤—Å–∫–æ–µ –º–µ—Ç—Ä–æ', 'officialmosmetro', '95,392')\n",
      "157805483   0.6396885   ('–¶–∏—Ç–∞—Ç—ã –∏–∑ –∏–≥—Ä', 'citations_game', '106,161')\n",
      "46496631   0.6402185   ('#–ü–ê–ì–û–†–ê–î–£', 'pagoradu_rf', '39,132')\n",
      "73432571   0.64120317   ('–ñ”ô–Ω–Ω”ô—Ç“õ–∞ –±—ñ—Ä–≥–µ ‚ôï', 'zhannatkabirge', '163,423')\n",
      "121886608   0.6464621   ('euphoria', 'x.euphoria', '39,574')\n",
      "49099240   0.66141695   ('–ê–Ω–æ–Ω–∏–º–Ω—ã–µ –ø–æ—à–ª–æ—Å—Ç–∏', 'anon_posh123', '104,370')\n",
      "73225212   0.6616862   ('–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–æ–Ω–¥ \"–ü–û–ú–û–ì–ê–¢–¨ –õ–ï–ì–ö–û\"', 'pomogat_legko', '123,143')\n",
      "30306157   0.6649293   ('No end. No beginning.', 'cnenb', '84,201')\n",
      "87985111   0.6727572   ('–ê–Ω–∏–º–µ-–º–∞–≥–∞–∑–∏–Ω OtakuStore', 'otakustore', '62,534')\n",
      "50477479   0.9999999   ('–ü–æ–¥—Å–ª—É—à–∞–Ω–æ –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥ 18+', 'poshlye52', '46,247')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_random(graph, final_embeddings, window_size, embedding_size, num_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    embedding_sizes = [32, 48, 64]\n",
    "    num_sampled_arr = [32, 48, 64]\n",
    "    window_sizes = [1, 2, 3, 4, 5]\n",
    "    batch_sizes = [128, 128, 132, 128, 130]\n",
    "    for i in xrange(len(embedding_sizes)):\n",
    "        for j in xrange(len(num_sampled_arr)):\n",
    "            for k in xrange(len(window_sizes)):\n",
    "                embedding_size = embedding_sizes[i]\n",
    "                num_sampled = num_sampled_arr[j]\n",
    "                window_size = window_sizes[k]\n",
    "                batch_size = batch_sizes[k]\n",
    "                \n",
    "                graph, final_embeddings = tf_train(window_size, embedding_size, num_sampled, batch_size)\n",
    "                \n",
    "                print_random(window_size, embedding_size, num_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
