{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fnc\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "log = logging.getLogger('log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "lhnd = logging.StreamHandler()\n",
    "lhnd.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "lhnd.setFormatter(formatter)\n",
    "\n",
    "log.addHandler(lhnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmap(f, arr):\n",
    "    return list(map(f, arr))\n",
    "\n",
    "def lfilter(f, arr):\n",
    "    return list(filter(f, arr))\n",
    "\n",
    "def foreach(it, f):\n",
    "    for e in it:\n",
    "        f(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 947528\n",
    "\n",
    "def raw_data_filter(file):\n",
    "    # Mapping to events\n",
    "    res = list()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for line in file:\n",
    "        cur = line.rstrip().split(',')\n",
    "        cur = lmap(lambda p: (re.sub(';.*', '', p), re.sub('.*;', '', p)), cur)\n",
    "\n",
    "        session = list()\n",
    "        \n",
    "        for j in range(0, len(cur)):\n",
    "            try:\n",
    "                session.append(int(cur[j][1]))\n",
    "            except ValueError:\n",
    "                None\n",
    "                \n",
    "        res.append(session)\n",
    "\n",
    "        i = i + 1\n",
    "                \n",
    "        if (i % 100000 == 0):\n",
    "            gc.collect()\n",
    "\n",
    "            log.debug(\"%d %% of mapping is done.\", i / total * 100)\n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data_filter(open(\"data/sessions_public.txt\",\"r\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_count(data):\n",
    "    total = dict()\n",
    "\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            if (j in total.keys()):\n",
    "                total[j] = total[j] + 1\n",
    "            else:\n",
    "                total[j] = 1\n",
    "                \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_map(lst, cnt, min_allowed, min_session_size):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set()\n",
    "        \n",
    "        for event in session:\n",
    "            if (event < 0):\n",
    "                sub_event = -event\n",
    "                \n",
    "                if (sub_event in sub):\n",
    "                    sub.remove(sub_event)\n",
    "                    \n",
    "                unsub.add(sub_event)\n",
    "            else:\n",
    "                if (not event in unsub):\n",
    "                    if (cnt == None or cnt[event] > min_allowed):\n",
    "                        sub.add(event)\n",
    "        \n",
    "        if (len(sub) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append(sub)\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def drop_uncommon(raw_data, min_allowed = 10, min_session_size = 4):\n",
    "    cnt = None\n",
    "    sorted_cnt = None\n",
    "    \n",
    "    data = raw_data\n",
    "    groups = None\n",
    "    \n",
    "    while (cnt == None or sorted_cnt[0] < min_allowed):\n",
    "        data, groups = set_map(data, cnt, min_allowed, min_session_size)\n",
    "        \n",
    "        cnt = group_count(data) \n",
    "        sorted_cnt = sorted(list(cnt.values()))\n",
    "        \n",
    "        log.info(\"Length of data:   %d\", len(data))\n",
    "        log.info(\"Number of groups: %d\", len(groups))\n",
    "        log.info(\"Minimum count:    %d\", sorted_cnt[0])\n",
    "        \n",
    "    return data, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = drop_uncommon(data)\n",
    "data, groups = drop_uncommon(raw_data[0:100000], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.embeddings(context).view((1, -1))\n",
    "        score = torch.mm(embed_focus, torch.t(embed_ctx))\n",
    "        log_probs = fnc.logsigmoid(score)\n",
    "\n",
    "\n",
    "    \n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "embd_size = 50\n",
    "learning_rate = 0.001\n",
    "n_epoch = 1\n",
    "\n",
    "def get_train(session):\n",
    "    train = []\n",
    "    \n",
    "    for i in session:\n",
    "        for j in session:\n",
    "            if (i == j):\n",
    "                continue\n",
    "            train.append((i, j, 1))\n",
    "    \n",
    "    sk = shuffle(list(groups))\n",
    "    \n",
    "    for i in groups:\n",
    "        if (not i in session):\n",
    "            for j in session:\n",
    "                train.append((i, j, 0))\n",
    "                train.append((j, i, 0))\n",
    "            \n",
    "            break\n",
    "        \n",
    "    return train\n",
    "    \n",
    "w2i = {w: i for i, w in enumerate(groups)}\n",
    "\n",
    "def train_skipgram(train):\n",
    "    losses = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = SkipGram(len(groups), embd_size)\n",
    "    print(model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss = .0\n",
    "        total_cnt = 0\n",
    "        for session in train:\n",
    "            \n",
    "            for in_w, out_w, target in session:\n",
    "                in_w_var = Variable(torch.LongTensor([w2i[in_w]]))\n",
    "                out_w_var = Variable(torch.LongTensor([w2i[out_w]]))\n",
    "\n",
    "                model.zero_grad()\n",
    "                log_probs = model(in_w_var, out_w_var)\n",
    "                loss = loss_fn(log_probs[0], Variable(torch.Tensor([target])))\n",
    "\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.data.item()\n",
    "                total_cnt += 1\n",
    "        losses.append(total_loss / total_cnt)\n",
    "        print('Epoch ', epoch, ' passed with avg loss ', total_loss)\n",
    "    return model, losses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [get_train(i) for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_models, sg_losses = train_skipgram(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = list(groups)\n",
    "tg = list(data[0])[0]\n",
    "print(data[0])\n",
    "\n",
    "\n",
    "res = []\n",
    "\n",
    "for i in range(0, len(groups)):\n",
    "    if (lg[i] == tg):\n",
    "        continue\n",
    "        \n",
    "    in_w_var = Variable(torch.LongTensor([w2i[tg]]))\n",
    "    out_w_var = Variable(torch.LongTensor([w2i[lg[i]]]))\n",
    "    \n",
    "    sg_model.zero_grad()\n",
    "    log_probs = sg_model(in_w_var, out_w_var)\n",
    "    \n",
    "    _, predicted = torch.max(log_probs.data, 1)\n",
    "\n",
    "    print(log_probs.data[0], ' ', lg[i])\n",
    "    print(nn.MSELoss()(log_probs.data[0],Variable(torch.Tensor([0]))))\n",
    "    #res.append()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sg_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
