{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from six.moves import xrange \n",
    "\n",
    "\n",
    "log = logging.getLogger('log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "lhnd = logging.StreamHandler()\n",
    "lhnd.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "lhnd.setFormatter(formatter)\n",
    "\n",
    "log.addHandler(lhnd)\n",
    "\n",
    "%autonotify -a 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmap(f, arr):\n",
    "    return list(map(f, arr))\n",
    "\n",
    "def lfilter(f, arr):\n",
    "    return list(filter(f, arr))\n",
    "\n",
    "def foreach(it, f):\n",
    "    for e in it:\n",
    "        f(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('auth/token') as f:\n",
    "    token = f.readline().strip()\n",
    "    \n",
    "\n",
    "\n",
    "def get_info(ids):\n",
    "    mc = 'members_count'\n",
    "    payload = {'v': '5.92', 'access_token': token, 'fields':mc}\n",
    "    \n",
    "    str_ids = functools.reduce(\n",
    "        lambda x, y: x + y,\n",
    "        lmap(lambda x: str(x) + ',', ids)\n",
    "    )\n",
    "    \n",
    "    payload['group_ids'] = str_ids[0:- 1]\n",
    "    \n",
    "    r = requests.get('https://api.vk.com/method/groups.getById', \n",
    "                     params=payload)\n",
    "    res = lmap(lambda x: (x['name'], x['screen_name'], x[mc] if mc in x else -1),r.json()['response'])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 947528\n",
    "\n",
    "def raw_data_filter(file):\n",
    "    # Mapping to events\n",
    "    res = list()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for line in file:\n",
    "        cur = line.rstrip().split(',')\n",
    "        cur = lmap(lambda p: (re.sub(';.*', '', p), re.sub('.*;', '', p)), cur)\n",
    "\n",
    "        session = list()\n",
    "        \n",
    "        for j in range(0, len(cur)):\n",
    "            try:\n",
    "                session.append(int(cur[j][1]))\n",
    "            except ValueError:\n",
    "                None\n",
    "                \n",
    "        res.append(session)\n",
    "\n",
    "        i = i + 1\n",
    "                \n",
    "        if (i % 100000 == 0):\n",
    "            gc.collect()\n",
    "\n",
    "            log.debug(\"%d %% of mapping is done.\", i / total * 100)\n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data_filter(open(\"data/sessions_public.txt\",\"r\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_count(data):\n",
    "    total = dict()\n",
    "\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            if (j in total.keys()):\n",
    "                total[j] = total[j] + 1\n",
    "            else:\n",
    "                total[j] = 1\n",
    "                \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_map(lst, cnt, min_allowed, min_session_size):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set()\n",
    "        \n",
    "        for event in session:\n",
    "            if (event < 0):\n",
    "                sub_event = -event\n",
    "                \n",
    "                if (sub_event in sub):\n",
    "                    sub.remove(sub_event)\n",
    "                    \n",
    "                unsub.add(sub_event)\n",
    "            else:\n",
    "                if (not event in unsub):\n",
    "                    if (cnt == None or cnt[event] > min_allowed):\n",
    "                        sub.add(event)\n",
    "        \n",
    "        if (len(sub) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append(sub)\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def drop_uncommon(raw_data, min_allowed = 10, min_session_size = 4):\n",
    "    cnt = None\n",
    "    sorted_cnt = None\n",
    "    \n",
    "    data = raw_data\n",
    "    groups = None\n",
    "    \n",
    "    while (cnt == None or sorted_cnt[0] < min_allowed):\n",
    "        data, groups = set_map(data, cnt, min_allowed, min_session_size)\n",
    "        \n",
    "        cnt = group_count(data) \n",
    "        sorted_cnt = sorted(list(cnt.values()))\n",
    "        \n",
    "        log.info(\"Length of data:   %d\", len(data))\n",
    "        log.info(\"Total length:     %d\", \n",
    "                functools.reduce((lambda x, y: x + y), lmap(lambda a: len(a), data))\n",
    "                )\n",
    "        log.info(\"Number of groups: %d\", len(groups))\n",
    "        log.info(\"Minimum count:    %d\", sorted_cnt[0])\n",
    "        \n",
    "    return data, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = drop_uncommon(data)\n",
    "data, groups = drop_uncommon(raw_data, 10)\n",
    "\n",
    "most_common = sorted(group_count(data).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "w2i = {w: i for i, w in enumerate(groups)}\n",
    "i2w = {i: w for i, w in enumerate(groups)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dex = 0\n",
    "event_dex = 0\n",
    "\n",
    "def generate_batch(batch_size):\n",
    "    global session_dex\n",
    "    global event_dex\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    current = 0\n",
    "    session = list(data[session_dex])\n",
    "    \n",
    "    while (current * 2 < batch_size):\n",
    "        batch[current * 2] = w2i[session[event_dex + 1]]\n",
    "        labels[current * 2][0] = w2i[session[event_dex]]\n",
    "        batch[current * 2 + 1] = w2i[session[event_dex + 1]]\n",
    "        labels[current * 2 + 1][0] = w2i[session[(event_dex + 2) % len(session)]]\n",
    "        \n",
    "        event_dex += 1\n",
    "        current += 1\n",
    "        \n",
    "        if (event_dex + 2 >= len(session)):\n",
    "            event_dex = 0\n",
    "            session_dex = session_dex + 1\n",
    "            if (session_dex >= len(data)):\n",
    "                session_dex = 0\n",
    "            session = list(data[session_dex])\n",
    "\n",
    "     \n",
    "    return batch, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch, labels = generate_batch(10)\n",
    "\n",
    "print(data[0])\n",
    "print(data[1])\n",
    "\n",
    "for i in range(10):\n",
    "    print(batch[i], i2w[batch[i]], '->', labels[i, 0],\n",
    "          i2w[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "num_sampled = 64\n",
    "vocabulary_size = len(groups)\n",
    "\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "            )\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "      # Construct the variables for the NCE loss\n",
    "    with tf.name_scope('weights'):\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    with tf.name_scope('biases'):\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                  weights=nce_weights,\n",
    "                  biases=nce_biases,\n",
    "                  labels=train_labels,\n",
    "                  inputs=embed,\n",
    "                  num_sampled=num_sampled,\n",
    "                  num_classes=vocabulary_size))\n",
    "        \n",
    "        # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "          optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all\n",
    "    # embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    #valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "    #                                          valid_dataset)\n",
    "    #similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(emb, index):\n",
    "    p = emb[index]\n",
    "    cnst = tf.constant(p, shape=[1, embedding_size])\n",
    "    d = tf.matmul(cnst, emb, transpose_b=True).eval()[0]\n",
    "\n",
    "    dxs = np.argsort(np.array(d))\n",
    "\n",
    "    ids = []\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(dxs) - 10, len(dxs)):\n",
    "        ids.append(i2w[dxs[i]])\n",
    "        res.append(d[dxs[i]])\n",
    "        \n",
    "    info = get_info(ids)\n",
    "    \n",
    "    for i in xrange(len(res)):\n",
    "        print(ids[i], ' ', res[i], ' ', info[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 337960\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(\"tmp\", session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned\n",
    "        # \"summary\" variable. Feed metadata variable to session for visualizing\n",
    "        # the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                         feed_dict=feed_dict,\n",
    "                                         run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                  average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000\n",
    "            # batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "            \n",
    "        if step % 20000 == 0 and step != 0:\n",
    "            print('Most closest to ', most_common[0][0])\n",
    "            get_closest(normalized_embeddings.eval(), w2i[most_common[0][0]])\n",
    "            \n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150388347   0.576734   ('–ú–æ–¥–Ω—ã–µ –ü—Ä–∏—á–µ—Å–∫–∏', 'modnue.pricheski', 1381005)\n",
      "151779453   0.57750046   ('–Ø –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ...', 'never_did', 691581)\n",
      "73776762   0.5840418   ('–í–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—å –ø–ª—é—Å', 'vosplus', 2003817)\n",
      "71474813   0.5844791   ('–†–µ–∞–ª—å–Ω—ã–π –§—É—Ç–±–æ–ª | –ë–∏—Ç–≤–∞ –ø–æ–∫–æ–ª–µ–Ω–∏–π', 'refoot', 2531028)\n",
      "28293246   0.59798276   ('Just love', 'vk.just.love', 3906799)\n",
      "22741624   0.6135372   ('–£–ª–µ—Ç–Ω—ã–µ –ø—Ä–∏–∫–æ–ª—ã', 'humour.page', 5214000)\n",
      "30532220   0.61808896   ('–°–ú–° –ø—Ä–∏–∫–æ–ª—ã :D', 'i4sms', 3272684)\n",
      "29573241   0.6675201   ('NR', 'rapnewrap', 4741209)\n",
      "26147450   0.6954052   ('–®–∫–æ–ª–∞? –ù–µ, –Ω–µ —Å–ª—ã—à–∞–ª–∏', 'onesc', 3863075)\n",
      "27895931   0.9999999   ('–ù–æ–≤–∏–Ω–∫–∏ –ú—É–∑—ã–∫–∏ 2019 | –ù–æ–≤–∞—è –ú—É–∑—ã–∫–∞', 'exclusive_muzic', 16384210)\n",
      "\n",
      "\n",
      "23758942   0.5705687   ('–í–∑—Ä–æ—Å–ª–µ–π', 'pafos_oo', 2999892)\n",
      "105999460   0.57192314   ('–ú–æ–∏ –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–∏', 'my_audios', 2622243)\n",
      "31613023   0.576375   ('–ß—ë—Ä—Ç–æ–≤ —Å—Ç—ã–¥', 'grebaniy_stid', 1598456)\n",
      "149537884   0.59037364   ('–°–º–æ—Ç—Ä–∏ —á—Ç–æ –Ø —Å–¥–µ–ª—è–ª—å!', 'sdelyall', 1219087)\n",
      "116779618   0.5985114   ('–ö–∞–∫—Ç—É—Å –ö–æ–ª—è', 'kaktuskola', 314894)\n",
      "72378974   0.61667365   ('–ú–æ–π –ö–æ–º–ø—å—é—Ç–µ—Ä', 'myironcomp', 1196334)\n",
      "73831523   0.6581749   ('Live In Tattoo | –¢–∞—Ç—É–∏—Ä–æ–≤–∫–∏', 'liveintattoo', 1091424)\n",
      "148059228   0.6589261   ('–°–∏–≥–Ω–∞L', 'signaphoto', 3020408)\n",
      "132239455   0.6664603   ('–°–º–æ—Ç—Ä–∏, —á—Ç–æ —è –∫—É–ø–∏–ª', 'club132239455', 1188218)\n",
      "45441631   0.9999999   ('–ü–†–ò–ö–û–õ–´ | –°–º–µ—è–∫–∞', 'smeyaka', 10902737)\n",
      "\n",
      "\n",
      "50138149   0.5907766   ('–†–µ–∞–ª—å–Ω–æ—Å—Ç—å.', 'is.the.reality', 2078510)\n",
      "88469025   0.5925573   ('–ë–£–î–¨ –í –ö–£–†–°–ï (–ë–í–ö)', 'v_kyrce', 673410)\n",
      "6246566   0.5927892   ('–ú–£–ó-–¢–í', 'muztv', 1829586)\n",
      "35061290   0.5966829   ('–≠–≥–æ–∏—Å—Ç', 'e_goist', 4747523)\n",
      "94255146   0.61293805   ('–†–µ–∞–ª—å–Ω–æ —Å–º–µ—à–Ω–æ', 'onovoe', 1995266)\n",
      "24199209   0.6148579   ('LIFE | –ù–æ–≤–æ—Å—Ç–∏', 'life', 1996380)\n",
      "77093415   0.6411072   ('–õ–∞–π—Ñ—Ö–∞–∫—É–º | –°–æ–≤–µ—Ç—ã, —Ö–∏—Ç—Ä–æ—Å—Ç–∏, –∏–¥–µ–∏', 'vk_lifehack_club', 1272951)\n",
      "133668394   0.67619145   ('–ó–∞–±—Ä–æ—à–µ–Ω–Ω–æ–µ', 'zabroshenoevk', 3104676)\n",
      "39153701   0.6911856   ('–ó–ª–æ–π –ì–µ–Ω–∏–π', 'evill_genius', 3492917)\n",
      "26419239   0.9999999   ('–°–º–µ–π—Å—è –¥–æ —Å–ª—ë–∑ :D', 'ifun', 11159769)\n",
      "\n",
      "\n",
      "42144182   0.5722703   ('skromno', 'skromno_vk', 971461)\n",
      "56918454   0.5839436   ('–ß–∏—Ç–∞—é—â–∏–µ', 'club56918454', 1125041)\n",
      "36166073   0.5988996   ('–¢—ã –Ω–∞ –ø–æ–Ω—Ç–∞—Ö, —è –Ω–∞ –∫–∞–±–ª—É–∫–∞—Ö.¬©', 'on.heels', 3726014)\n",
      "149094324   0.6040858   ('–ö–æ–º–º–µ–Ω—Ç–∞—Ç–æ—Ä –æ—Ç –ë–æ–≥–∞', 'komment.broo', 1698405)\n",
      "133814709   0.61621344   ('–ó–∞–º–µ–¥–ª–µ–Ω–Ω–∞—è —Å—ä–µ–º–∫–∞ | Gif', 'slowmovies', 998154)\n",
      "78622646   0.6266579   ('–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ –∑–Ω–∞—Ç—å | –ù–∞—É–∫–∞ –∏ —Ñ–∞–∫—Ç—ã', 'welsis', 1550580)\n",
      "38683579   0.6438988   ('–õ—É—á—à–∏–µ —Å—Ç–∏—Ö–∏ –≤–µ–ª–∏–∫–∏—Ö –ø–æ—ç—Ç–æ–≤ | –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '1poetry', 6083867)\n",
      "22798006   0.6460554   ('–ö–∏–Ω–æ–º–∞–Ω–∏—è', 'kino_mania', 11431397)\n",
      "32370614   0.70771194   ('MARVEL/DC', 'marvel_dc', 3431920)\n",
      "58170807   0.99999976   ('–ö–∏–Ω–æ–ö–∞–π—Ñ - –õ—É—á—à–∏–µ —Ñ–∏–ª—å–º—ã', 'kino_kaif', 9605785)\n",
      "\n",
      "\n",
      "45703770   0.57191145   ('–ú—É–∑—ã–∫–∞', 'vkmuz', 4983581)\n",
      "46105176   0.57324123   ('–ù–æ–≤–∏–Ω–∫–∏ –∫–∏–Ω–æ ‚òÖ –¢-34 | –ü–æ–ª–∏—Ü–µ–π—Å–∫–∏–π —Å –†—É–±–ª—ë–≤–∫–∏', 'new_kino_o', 1882101)\n",
      "94302419   0.5809232   ('–ì–∏—Ñ–∞—á | –ì–∏—Ñ–∫–∏', 'gifki', 908833)\n",
      "126100310   0.58235055   ('–ú–∞–Ω–∏–∫—é—Ä | –ù–æ–≥—Ç–∏ | –ü–µ–¥–∏–∫—é—Ä | –ò–¥–µ–∏ 2019', 'women.nails', 3285198)\n",
      "67580761   0.58700746   ('–ö–ë', 'countryballs_re', 1257792)\n",
      "26492122   0.604583   ('EA7', 'emporioarmani_7', 1482781)\n",
      "127090395   0.64260066   ('–ò–ª–ª—é–∑–∏–æ–Ω–∏—Å—Ç', 'illusionistvk', 2024171)\n",
      "45608667   0.64982516   ('–®—É—Ç–Ω–∏–∫', 'shutniki_ru', 4357221)\n",
      "43776215   0.787217   ('–Ø —Ç–µ–±—è —Ö–æ—á—É', 'iwantyou', 6195565)\n",
      "43215063   1.0   ('–ö–∏–Ω–æ–º–∞–Ω–∏—è | –ù–æ–≤–∏–Ω–∫–∏ 2019', 'kinomania', 12208659)\n",
      "\n",
      "\n",
      "156882121   0.59878755   ('–°–º–µ–∫–∞–ª–æ—á–∫–∞ | GIF', 'xalife', 669105)\n",
      "119377097   0.6002635   ('A L O N E', 'alonelys', 1225493)\n",
      "22522055   0.6024964   ('–°–±–µ—Ä–±–∞–Ω–∫', 'sberbank', 2569098)\n",
      "22753480   0.61097145   ('Discovery', 'tweas', 1705513)\n",
      "77843142   0.61453784   ('–ù–µ –±–µ—Å–∏', 'ne6esi', 2011261)\n",
      "80044744   0.61858463   ('–¢–≤–æ–π –ª—é–±–∏–º—ã–π —Å–µ—Ä–∏–∞–ª  | SKAM / –†–ò–í–ï–†–î–ï–ô–õ', 'lubimiy.serial', 1012009)\n",
      "145510087   0.6437865   ('–ò–î–ï–ê–õ–¨–ù–û', 'superidealno', 1556578)\n",
      "86218441   0.6555147   ('–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è', 'psy_real', 3250460)\n",
      "26030283   0.670519   ('–°–∞—Ä–∫–∞–∑–º', 'agil_vk', 4535418)\n",
      "91050183   0.99999994   ('–õ–µ–æ–Ω–∞—Ä–¥–æ –î–∞–π –í–∏–Ω—á–∏–∫', 'dayvinchik', 7289940)\n",
      "\n",
      "\n",
      "155590170   0.5505915   ('–û–†–£, –°–≠–†!', 'medieval_or', 2167103)\n",
      "30823579   0.5819628   ('–§–ò–õ–¨–ú–´ –£–ñ–ê–°–û–í', 'nightmares', 1744274)\n",
      "151229592   0.58313334   ('–ù–ï—Ç–∏–ø–∏—á–Ω—ã–µ —Ñ–∞–∫—Ç—ã', 'nfacty', 1395211)\n",
      "144962203   0.58742154   ('–ò —Ç–∞–∫ —Å–æ–π–¥–µ—Ç!', 'itaksodet', 1241885)\n",
      "64977560   0.59500724   ('EA7 | MUSIC |', 'e_a_7_music', 1880191)\n",
      "139740824   0.6002177   ('–ë–æ—Ç –õ–µ–Ω–∞', 'bot_lena', 1395500)\n",
      "23390361   0.62155294   ('–≠—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ!', 'smart_log', 3908742)\n",
      "34215577   0.626065   ('–ü–æ–¥—Å–ª—É—à–∞–Ω–æ', 'overhear', 3946857)\n",
      "120254617   0.6376679   ('$$$ DANK MEMES $$$ AYY LMAO $$$', 'dank_memes_ayylmao', 580395)\n",
      "57846937   0.99999994   ('MDK', 'mudakoff', 10483339)\n",
      "\n",
      "\n",
      "18496184   0.5883862   ('–°–¢–°', 'ctc', 3115223)\n",
      "152435896   0.6066171   ('–¢–†–≠–®', 'club152435896', 1301996)\n",
      "54500021   0.6082889   ('–ù–æ–≤–∏–Ω–∫–∏ –ú—É–∑—ã–∫–∏ 2019', 'exclusive_songs', 1439433)\n",
      "26062647   0.6192814   ('Kate Mobile', 'kate_mobile', 5897323)\n",
      "34378420   0.64601034   ('–ù–µ –º—ã —Ç–∞–∫–∏–µ  - –∂–∏–∑–Ω—å —Ç–∞–∫–∞—è', 'mytk1es', 2564987)\n",
      "58170807   0.6460554   ('–ö–∏–Ω–æ–ö–∞–π—Ñ - –õ—É—á—à–∏–µ —Ñ–∏–ª—å–º—ã', 'kino_kaif', 9605785)\n",
      "32370614   0.6835736   ('MARVEL/DC', 'marvel_dc', 3431920)\n",
      "45745333   0.6838178   ('4ch', '4ch', 4962912)\n",
      "23213239   0.6971351   ('–î–∑–µ–Ω', 'dzenpub', 2818905)\n",
      "22798006   1.0000001   ('–ö–∏–Ω–æ–º–∞–Ω–∏—è', 'kino_mania', 11431397)\n",
      "\n",
      "\n",
      "108477741   0.5709563   ('–≠–ª–∏—Ç–Ω—ã–π –Æ–º–æ—Ä', 'toxic_humor', 1434030)\n",
      "68895020   0.5745063   ('–ö–¢–û —Å–≤–µ—Ä—Ö—É', 'ktosverxy', 4033326)\n",
      "128338223   0.5967833   ('–î–∞–≤–∞–π, —É–¥–∏–≤–ª—è–π', 'davai_udivi', 893288)\n",
      "149925677   0.6037697   ('Say.', 'club149925677', 1099076)\n",
      "122282289   0.6180465   ('—Ç—ã —Å–æ—Ö—Ä–∞–Ω–∏—à—å.', 'forver2016', 437874)\n",
      "12353330   0.64057094   ('–ù–∞ –°–ª—É—á–∞–π –í–∞–∂–Ω—ã—Ö –ü–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤', 'peregovorov', 2786659)\n",
      "35809584   0.65562797   ('Hi-Tech | –ù–∞—É–∫–∞ –∏ —Ç–µ—Ö–Ω–∏–∫–∞', 'tech.science', 1832135)\n",
      "48512305   0.678752   ('–¢–µ–ª–µ–∫–∞–Ω–∞–ª –¢–ù–¢', 'tnt', 4876396)\n",
      "66678575   0.7100995   ('–û–≤—Å—è–Ω–∫–∞, —Å—ç—Ä!', 'ovsyanochan', 4145209)\n",
      "135209264   1.0   ('–ë–æ—Ç –ú–∞–∫—Å–∏–º', 'bot_maxim', 5097534)\n",
      "\n",
      "\n",
      "95128240   0.5976789   ('–°–º–µ—Ä—Ç–µ–ª—å–Ω—ã–π —é–º–æ—Ä', 'smehizhizn', 1740251)\n",
      "151257260   0.5987773   ('–û—Å–º—ã—Å–ª–µ–Ω–Ω–æ–µ', 'osmyslennoe', 1028744)\n",
      "65047210   0.6011939   ('–ì–∏—Ñ–∫–∏ :3', 'fun.gifs', 1849423)\n",
      "144853678   0.6042826   ('–¢–´–ñ–ï–î–ï–í–û–ß–ö–ê', 'tigede', 951561)\n",
      "23308460   0.6069876   ('theend.', 'the1end', 2843694)\n",
      "78388911   0.63748956   ('–ö–Ω–∏–≥–∞ —Ä–µ–∫–æ—Ä–¥–æ–≤', 'book.record', 3874615)\n",
      "149126828   0.6406744   ('–§–∏–ª–∏–∞–ª —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤', 'momsms', 1839144)\n",
      "49603755   0.68117356   ('—Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å', 'usatm', 2499794)\n",
      "46509740   0.7223425   ('–ú–∞–Ω–∏–∫—é—Ä | –ù–æ–≥—Ç–∏', 'modnailru', 3682328)\n",
      "23064236   1.0   ('–ß–µ—Ç–∫–∏–µ –ü—Ä–∏–∫–æ–ª—ã', 'ilikes', 8581328)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        get_closest(final_embeddings, w2i[most_common[i][0]])\n",
    "        \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80701061   0.43979344   ('Apple –ë–∞—Ä–∞—Ö–æ–ª–∫–∞ –°–∞—Ä–∞—Ç–æ–≤', 'applebarahlo64', 7344)\n",
      "46007784   0.44030026   ('YouTube Angelville', 'yt_angelville', 23418)\n",
      "43686317   0.4434111   ('–†–∞–±–æ—Ç–∞ –í–∞–∫–∞–Ω—Å–∏–∏ –†–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–∏ –≤ –ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥–µ', 'jobs39', 90756)\n",
      "63303836   0.44487107   ('kipish', 'kipish088', 244231)\n",
      "152366773   0.44687134   ('–†—ã–±–∞–ª–∫–∞ —Å AliExpress', 'club152366773', 4800)\n",
      "109496640   0.45852435   ('–ë–∞—Ä–∞—Ö–æ–ª–∫–∞ –ü—Å–∫–æ–≤', 'club109496640', 12758)\n",
      "102268559   0.46084452   ('7NN üå¥', '7noname', 39879)\n",
      "135339893   0.4645276   ('CrockidUfa (–ö—Ä–æ–∫–∏–¥–£—Ñ–∞) –î–µ—Ç—Å–∫–∞—è –æ–¥–µ–∂–¥–∞ –≤ –£—Ñ–µ', 'crockidufa', 5438)\n",
      "71458926   0.50234413   ('–ë—Ä–æ–Ω–Ω–∏—Ü—ã 24', 'bron24', 8847)\n",
      "8780658   1.0000001   ('–ö—É–±–∏–∫ –†—É–±–∏–∫–∞. –°–ø–∏–¥–∫—É–±–∏–Ω–≥ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –≥—Ä—É–ø–ø–∞)', 'ruspeedcubing', 45648)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    get_closest(final_embeddings, len(groups) // 4 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
