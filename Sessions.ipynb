{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import requests\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from six.moves import xrange \n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "log = logging.getLogger('log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "lhnd = logging.StreamHandler()\n",
    "lhnd.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "lhnd.setFormatter(formatter)\n",
    "\n",
    "log.addHandler(lhnd)\n",
    "\n",
    "%autonotify -a 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_dumps = False\n",
    "\n",
    "def lmap(f, arr):\n",
    "    return list(map(f, arr))\n",
    "\n",
    "def lfilter(f, arr):\n",
    "    return list(filter(f, arr))\n",
    "\n",
    "def foreach(it, f):\n",
    "    for e in it:\n",
    "        f(e)\n",
    "        \n",
    "def dump(data, name):\n",
    "    with open('data/' + name, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load(name):\n",
    "    with open('data/' + name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def load_or_dump(path, func):\n",
    "    if not Path('data/' + path).exists() or ignore_dumps:\n",
    "        res = func()\n",
    "    \n",
    "        dump(res, path)\n",
    "    else:\n",
    "        res = load(path)\n",
    "        \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "with open('auth/token') as f:\n",
    "    token = f.readline().strip()\n",
    "\n",
    "def get_info(ids):\n",
    "    sleep(0.2)\n",
    "    mc = 'members_count'\n",
    "    payload = {'v': '5.92', 'access_token': token, 'fields':mc}\n",
    "    \n",
    "    str_ids = functools.reduce(\n",
    "        lambda x, y: x + y,\n",
    "        lmap(lambda x: str(x) + ',', ids)\n",
    "    )\n",
    "    \n",
    "    print(str_ids)\n",
    "    \n",
    "    payload['group_ids'] = str_ids[0:- 1]\n",
    "    \n",
    "    r = requests.get('https://api.vk.com/method/groups.getById', \n",
    "                     params=payload)\n",
    "    \n",
    "    if (not 'response' in r.json()):\n",
    "        print(r.json())\n",
    "        \n",
    "    res = lmap(lambda x: (x['name'], x['screen_name'], \"{:,}\".format(x[mc]) if mc in x else -1),r.json()['response'])\n",
    "    \n",
    "    return(res)\n",
    "\n",
    "def info_print(lst):\n",
    "    info = get_info(lst)\n",
    "    \n",
    "    print(lmap(lambda x: x[0], info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 1015925\n",
    "\n",
    "def raw_data_filter(file):\n",
    "    # Mapping to events\n",
    "    res = list()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for line in file:\n",
    "        cur = line.rstrip().split(',')\n",
    "        cur = lmap(lambda p: (re.sub(';.*', '', p), re.sub('.*;', '', p)), cur)\n",
    "\n",
    "        session = list()\n",
    "        \n",
    "        for j in range(0, len(cur)):\n",
    "            try:\n",
    "                session.append(int(cur[j][1]))\n",
    "            except ValueError:\n",
    "                None\n",
    "                \n",
    "        res.append(session)\n",
    "\n",
    "        i = i + 1\n",
    "                \n",
    "        if (i % 100000 == 0):\n",
    "            gc.collect()\n",
    "\n",
    "            log.debug(\"%d %% of mapping is done.\", i / total * 100)\n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode == 0):\n",
    "    raw_data = load_or_dump('raw', lambda: raw_data_filter(open(\"data/public_sessions_2.txt\",\"r\")))\n",
    "\n",
    "    log.info(\"Data loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_count(data):\n",
    "    total = dict()\n",
    "\n",
    "    for i in data:\n",
    "        for j in i[0]:\n",
    "            if (j in total.keys()):\n",
    "                total[j] = total[j] + 1\n",
    "            else:\n",
    "                total[j] = 1\n",
    "                \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words_data(file):\n",
    "    words = []\n",
    "    \n",
    "    for line in file:\n",
    "        for word in line.split():\n",
    "            words.append(word)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode == 1):\n",
    "    words_size = 50000\n",
    "    \n",
    "    ignore_dumps = True\n",
    "    data = load_or_dump('raw_txt', lambda: load_words_data(open(\"data/text8.txt\",\"r\")))\n",
    "    groups = group_count([[data]])\n",
    "    \n",
    "    dictlist = list(groups.items())\n",
    "    dictlist.sort(key = lambda x: x[1])\n",
    "    allowed = set(lmap(lambda x: x[0], dictlist[-words_size:]))\n",
    "\n",
    "    for i in xrange(len(data)):\n",
    "        if not data[i] in allowed:\n",
    "            data[i] = '-1'\n",
    "            \n",
    "    groups = group_count([[data]])\n",
    "    \n",
    "    data = [[data, []]]\n",
    "    \n",
    "    print(len(groups))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_session_size = 2\n",
    "max_session_size = 20\n",
    "\n",
    "def initiail_mapping(lst, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set()\n",
    "        malformed = set()\n",
    "        \n",
    "        for event in session:\n",
    "            if (event < 0):\n",
    "                sub_event = -event\n",
    "                \n",
    "                if (sub_event in sub or sub_event in malformed):\n",
    "                    sub.discard(sub_event)\n",
    "                    unsub.discard(sub_event)\n",
    "                    malformed.add(sub_event)\n",
    "                else:\n",
    "                    unsub.add(sub_event)\n",
    "            else:\n",
    "                if (event in unsub or event in malformed):\n",
    "                    unsub.discard(event)\n",
    "                    sub.discard(event)\n",
    "                    malformed.add(event)\n",
    "                else:\n",
    "                    sub.add(event)\n",
    "        \n",
    "        if (len(sub) >= min_session_size and len(sub) <= max_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "    \n",
    "\n",
    "def set_map(lst, cnt, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set() \n",
    "        \n",
    "        for event in session[0]:\n",
    "            if (cnt[event] > min_allowed):\n",
    "                sub.add(event)\n",
    "                \n",
    "        for event in session[1]:\n",
    "            if (cnt.get(event, -1) > min_allowed):\n",
    "                unsub.add(event)    \n",
    "        \n",
    "        if (len(sub) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def drop_uncommon(raw_data, min_allowed = 10):\n",
    "    cnt = None\n",
    "    sorted_cnt = None\n",
    "    \n",
    "    data, groups = initiail_mapping(raw_data, min_allowed)\n",
    "    cnt = group_count(data) \n",
    "    sorted_cnt = sorted(list(cnt.values()))\n",
    "    \n",
    "    while (cnt == None or sorted_cnt[0] < min_allowed):\n",
    "        data, groups = set_map(data, cnt, min_allowed)\n",
    "                \n",
    "        cnt = group_count(data) \n",
    "        sorted_cnt = sorted(list(cnt.values()))\n",
    "        \n",
    "        log.info(\"Length of data:   %d\", len(data))\n",
    "        log.info(\"Total length:     %d\", \n",
    "                functools.reduce((lambda x, y: x + y), lmap(lambda a: len(a), data))\n",
    "                )\n",
    "        log.info(\"Number of groups: %d\", len(groups))\n",
    "        log.info(\"Minimum count:    %d\\n\", sorted_cnt[0])\n",
    "        \n",
    "    return data, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (mode == 0):\n",
    "    ignore_dumps = False\n",
    "    data, groups = load_or_dump('final_data', lambda: drop_uncommon(raw_data, 50))\n",
    "\n",
    "    most_common = sorted(group_count(data).items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = {w: i for i, w in enumerate(groups)}\n",
    "i2w = {i: w for i, w in enumerate(groups)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_path = \"data/filtered\"\n",
    "\n",
    "if not Path(fil_path).exists():\n",
    "    with open(fil_path, \"w\") as out:  \n",
    "        for id, session in enumerate(data):\n",
    "            for sub in session[0]:\n",
    "                out.write(str(id) + \" \" + str(w2i[sub]) + \"\\n\")\n",
    "                \n",
    "            for unsub in session[1]:\n",
    "                out.write(str(id) + \" \" + str(-w2i[unsub]) + \"\\n\")\n",
    "                \n",
    "    with open(\"data/filtered_all\", \"w\") as out:  \n",
    "        for id, session in enumerate(data):\n",
    "            for sub in session[0]:\n",
    "                out.write(str(w2i[sub]) + \" \")\n",
    "            \n",
    "            out.write(\"\\n\")\n",
    "\n",
    "    dump(w2i, \"w2i\")\n",
    "    dump(i2w, \"i2w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i2w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = None\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dex = 0\n",
    "event_dex = 0\n",
    "\n",
    "def generate_window_batch(batch_size, negative_size, window_size = 1):\n",
    "    assert min_session_size > 1\n",
    "    \n",
    "    global session_dex\n",
    "    global event_dex\n",
    "    \n",
    "    labels = np.ndarray(shape=(batch_size, window_size), dtype=np.int32)\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    negative = np.ndarray(shape=(batch_size, negative_size), dtype=np.int32)\n",
    "     \n",
    "    current = 0\n",
    "    session = list(data[session_dex][0])\n",
    "    \n",
    "    for i in range(0, batch_size):     \n",
    "        batch[i] = w2i[session[event_dex]]\n",
    "        \n",
    "        for j in range(1, window_size + 1):\n",
    "            labels[i][j - 1] = w2i[session[(event_dex + j) % len(session)]]\n",
    "            \n",
    "            if (labels[i][j - 1] == batch[i]):\n",
    "                labels[i][j - 1] = labels[i][j - 2]\n",
    "            \n",
    "        neg = 0\n",
    "        \n",
    "        for j in data[session_dex][1]:\n",
    "            negative[i][neg] = w2i[j]\n",
    "            neg += 1\n",
    "            if (neg == negative_size):\n",
    "                break\n",
    "                \n",
    "        rand_neg = np.random.randint(len(groups), size=negative_size - neg)\n",
    "        \n",
    "        for j in range(0, negative_size - neg):\n",
    "            negative[i][neg + j] = rand_neg[j]\n",
    "            \n",
    "        event_dex += 1\n",
    "\n",
    "        if (event_dex == len(session)):\n",
    "            event_dex = 0\n",
    "            session_dex = session_dex + 1\n",
    "            if (session_dex >= len(data)):\n",
    "                session_dex = 0\n",
    "            session = list(data[session_dex][0])        \n",
    "     \n",
    "    return batch, labels, negative\n",
    "\n",
    "\n",
    "def generate_batch(negative_size):\n",
    "    assert min_session_size > 1\n",
    "    \n",
    "    global session_dex\n",
    "    global event_dex\n",
    "    \n",
    "    session = list(data[session_dex][0])\n",
    "    \n",
    "    labels = lmap(lambda x: w2i[x], session)\n",
    "    batch = w2i[session[event_dex]]\n",
    "    negative = np.ndarray(shape=(negative_size), dtype=np.int32)\n",
    "                \n",
    "    neg = 0\n",
    "\n",
    "    for j in data[session_dex][1]:\n",
    "        negative[neg] = w2i[j]\n",
    "        neg += 1\n",
    "        if (neg == negative_size):\n",
    "            break\n",
    "\n",
    "    rand_neg = np.random.randint(len(groups), size=negative_size - neg)\n",
    "\n",
    "    for j in range(0, negative_size - neg):\n",
    "        negative[neg + j] = rand_neg[j]\n",
    "\n",
    "    event_dex += 1\n",
    "\n",
    "    if (event_dex == len(session)):\n",
    "        event_dex = 0\n",
    "        session_dex = session_dex + 1\n",
    "        if (session_dex >= len(data)):\n",
    "            session_dex = 0 \n",
    "\n",
    "    return batch, labels, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# session_dex = 0\n",
    "\n",
    "print(data[session_dex])\n",
    "print(data[session_dex + 1])\n",
    "\n",
    "batch, labels, negative = generate_batch(10)\n",
    "\n",
    "print(i2w[batch], '->', lmap(lambda x: i2w[x], labels), '-> (negative)', lmap(lambda x: i2w[x], negative))\n",
    "    \n",
    "print(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = lfilter(lambda x: x in w2i, [129440544, 28261334, 92876084, 51016572, 91933860, 22751485])\n",
    "\n",
    "if (mode == 1):\n",
    "    test_ids = ['term', 'first', 'used', 'early', 'against', 'working']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for i in data:\n",
    "    for j in test_ids:\n",
    "        if j in i[0]:\n",
    "            f = list(i[0])\n",
    "            f.remove(j)\n",
    "            f.append(j)\n",
    "            lst.append(f)\n",
    "            \n",
    "random.shuffle(lst)\n",
    "\n",
    "for i in xrange(10):\n",
    "    sleep(1)\n",
    "    print(get_info(lst[i]))\n",
    "    print()\n",
    "    print(\"==================================================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    for i in test_ids:\n",
    "        t = []\n",
    "        \n",
    "        for j in range(vocab_size):\n",
    "            fst = Variable(torch.LongTensor([w2i[i]]))\n",
    "            snd = Variable(torch.LongTensor([j]))\n",
    "            t.append([model.score(fst, snd), i2w[j]])\n",
    "        \n",
    "        t.sort(key = lambda x: -x[0])\n",
    "        \n",
    "        ids = []\n",
    "        res = t[:10]\n",
    "       \n",
    "        for k in res:\n",
    "            ids.append(k[1])\n",
    "            \n",
    "        info = get_info(ids)\n",
    "        \n",
    "        print(i)\n",
    "        for i in range(10):\n",
    "            print(res[i], ' ', info[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "vocab_size = len(groups)\n",
    "\n",
    "window_size = 4\n",
    "embedding_size = 64\n",
    "negative_size = 10\n",
    "batch_size = 1\n",
    "\n",
    "pref = \"/etmp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iterations = 2000001\n",
    "\n",
    "def loss_sampled(scores):\n",
    "    res = scores[0]\n",
    "    \n",
    "    for i in range(1, len(scores)):\n",
    "        res = res + scores[i]\n",
    "        \n",
    "    return res * Variable(torch.Tensor([-1]))\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.in_embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.out_embeddings(context).view((1, -1))\n",
    "\n",
    "        score = torch.mm(embed_focus, torch.t(embed_ctx))\n",
    "      \n",
    "        return score\n",
    "    \n",
    "    def score(self, focus, context):\n",
    "        embed_focus = self.in_embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.in_embeddings(context).view((1, -1))\n",
    "\n",
    "        score = F.cosine_similarity(embed_focus, embed_ctx)\n",
    "    \n",
    "        return score\n",
    "    \n",
    "model = SkipGram(vocab_size, embedding_size)    \n",
    "    \n",
    "def train_skipgram():\n",
    "    losses = []\n",
    "    loss_fn = loss_sampled\n",
    "\n",
    "    print(model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    total_loss = .0\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        target, contexts, negative = generate_batch(negative_size)\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        it_losses = []\n",
    "        \n",
    "        scores = []\n",
    "\n",
    "        in_w_var = Variable(torch.LongTensor([target]))\n",
    "\n",
    "        for ctx in contexts:\n",
    "            out_w_var = Variable(torch.LongTensor([ctx]))\n",
    "\n",
    "            score = torch.sigmoid(model(in_w_var, out_w_var))\n",
    "\n",
    "            if (score != 0):\n",
    "                scores.append(torch.log(score))\n",
    "            else:\n",
    "                scores.append(torch.log(score + torch.Tensor([0.0000001])))\n",
    "\n",
    "        for neg in negative:\n",
    "            out_w_var = Variable(torch.LongTensor([neg]))\n",
    "\n",
    "            score = torch.sigmoid(model(in_w_var, out_w_var) * Variable(torch.Tensor([-1])))\n",
    "\n",
    "            if (score != 0):\n",
    "                scores.append(torch.log(score))\n",
    "            else:\n",
    "                scores.append(torch.log(score + torch.Tensor([0.0000001])))           \n",
    "\n",
    "#         with torch.autograd.detect_anomaly():\n",
    "#             print(scores)\n",
    "\n",
    "    \n",
    "        loss = loss_fn(scores)\n",
    "#         loss = torch.mean(torch.stack(it_losses))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (i % 2000 == 0):\n",
    "            if i > 0:\n",
    "                total_loss /= 2000\n",
    "                \n",
    "            if (i % 100000 == 0):\n",
    "                dump(model, pref + str(i))\n",
    "                test(model)\n",
    "            \n",
    "            log.debug('Average loss at step %d: %.4f', i, total_loss)\n",
    "            total_loss = 0\n",
    "            losses.append(total_loss)\n",
    "      \n",
    "    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_model, tr_losses = train_skipgram()\n",
    "\n",
    "# dump(model, pref + \"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_embed(model):\n",
    "    return model.in_embeddings(\n",
    "            Variable(\n",
    "                torch.LongTensor(\n",
    "                    list(range(len(groups)))\n",
    "                )\n",
    "            )\n",
    "        ).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "num_clusters = 32\n",
    "\n",
    "all_embed = get_all_embed(model)\n",
    "\n",
    "km = sklearn.cluster.KMeans(n_clusters = num_clusters)\n",
    "\n",
    "km.fit(all_embed)\n",
    "\n",
    "predicitons = km.predict(all_embed)\n",
    "\n",
    "target = predicitons[w2i[29534144]]\n",
    "\n",
    "res = []\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    if predicitons[i] == target:\n",
    "        res.append(i2w[i])\n",
    "\n",
    "if (len(res) < 180):\n",
    "    print(get_info(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = predicitons[w2i[12648877]]\n",
    "\n",
    "print (km.cluster_centers_[target])\n",
    "\n",
    "t = []\n",
    "        \n",
    "for j in range(vocab_size):\n",
    "    cent = Variable(torch.FloatTensor([km.cluster_centers_[target]]))\n",
    "    embed_ctx = model.in_embeddings(Variable(torch.LongTensor([j]))).view((1, -1))\n",
    "\n",
    "    score = F.cosine_similarity(cent, embed_ctx)\n",
    "    \n",
    "    t.append([score, i2w[j]])\n",
    "\n",
    "t.sort(key = lambda x: -x[0])\n",
    "\n",
    "ids = []\n",
    "res = t[:10]\n",
    "\n",
    "for k in res:\n",
    "    ids.append(k[1])\n",
    "\n",
    "info = get_info(ids)\n",
    "\n",
    "print(i)\n",
    "for i in range(10):\n",
    "    print(res[i], ' ', info[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(verbose=1)\n",
    "\n",
    "embed_2d = tsne.fit_transform(all_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = [-1.0 for i in range(embedding_size)]\n",
    "\n",
    "cls_index = [i for i in range(num_clusters)]\n",
    "\n",
    "cls_index.sort(key = lambda x: F.cosine_similarity(\n",
    "                    torch.FloatTensor(km.cluster_centers_[x]).view((1, -1)), \n",
    "                    torch.FloatTensor(ones).view((1, -1))\n",
    "              ))\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (40, 20))\n",
    "\n",
    "cmap = plt.get_cmap(\"jet\", num_clusters)\n",
    "\n",
    "sct = ax.scatter(\n",
    "    x = lmap(lambda x: x[0], embed_2d), \n",
    "    y = lmap(lambda x: x[1], embed_2d), \n",
    "    c = lmap(lambda x: cls_index.index(x), predicitons), \n",
    "    s = 70,\n",
    "    cmap = cmap,\n",
    "    alpha = 0.4\n",
    ")\n",
    "\n",
    "ax.set_title(\"Clusters are colored with gradation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_in(x_center, y_center, limit, with_data = False):\n",
    "    sc_data = zip(\n",
    "        lmap(lambda x: x[0], embed_2d), \n",
    "        lmap(lambda y: y[1], embed_2d), \n",
    "        lmap(lambda c: cls_index.index(c), predicitons),\n",
    "        list(range(len(embed_2d)))\n",
    "    )\n",
    "      \n",
    "    sc_data = lfilter(lambda elem: abs(elem[0] - x_center) < limit \n",
    "                      and abs(elem[1] - y_center) < limit,\n",
    "                      sc_data)\n",
    "\n",
    "    if (len(sc_data) == 0):\n",
    "        print(\"No data\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (10, 5))\n",
    "\n",
    "    classes = lmap(lambda x: x[2], sc_data)\n",
    "    \n",
    "    enum = list(enumerate(set(classes)))\n",
    "    \n",
    "    class_remap = dict(lmap(lambda x:(x[1], x[0]), enum))\n",
    "    classes_map = dict(enum) \n",
    "    \n",
    "    classes = lmap(lambda x: class_remap[x], classes)\n",
    "    \n",
    "    cmap = plt.get_cmap(\"jet\", len(set(classes)))\n",
    "    \n",
    "    sct = ax.scatter(\n",
    "        x = lmap(lambda x: x[0], sc_data), \n",
    "        y = lmap(lambda x: x[1], sc_data), \n",
    "        c = classes, \n",
    "        s = 70,\n",
    "        cmap = cmap,\n",
    "        alpha = 1\n",
    "    )\n",
    "    \n",
    "    cb = plt.colorbar(sct, spacing = \"proportional\", ticks = np.linspace(0, len(classes), len(classes) + 1))\n",
    "\n",
    "    cb.set_alpha(1)\n",
    "    cb.draw_all()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if (with_data):\n",
    "        for c in class_remap.keys():\n",
    "            print(class_remap[c])\n",
    "            \n",
    "            sleep(0.5)\n",
    "            \n",
    "            info_print(\n",
    "                lmap(lambda x: i2w[x[3]], lfilter(lambda x: x[2] == c, sc_data))\n",
    "            )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"data/categories_predict_dataset_v2.csv\", index_col = False)\n",
    "df = raw_df[raw_df.id.isin(w2i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Total groups in main dataset: \", len(groups), \" common groups in data sets:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(set(df.general)))\n",
    "print (len(set(df.detailed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cat_count = 150\n",
    "\n",
    "cat_dict = dict()\n",
    "\n",
    "cat_lst = list(df.general)\n",
    "\n",
    "for c in cat_lst:\n",
    "    if c in cat_dict:\n",
    "        cat_dict[c] += 1\n",
    "    else:\n",
    "        cat_dict[c] = 1\n",
    "        \n",
    "cat_set = set()\n",
    "\n",
    "for item in cat_dict.items():\n",
    "    if (item[1] >= min_cat_count):\n",
    "        cat_set.add(item[0])\n",
    "        \n",
    "print(lfilter(lambda x: x[0] in cat_set, cat_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "\n",
    "def get_classifiers():\n",
    "    params = {'verbose': 0, 'n_estimators': 100}\n",
    "\n",
    "    gbc = GradientBoostingClassifier(**params)\n",
    "    \n",
    "    abc = AdaBoostClassifier(n_estimators = 100)\n",
    "    \n",
    "    s_clf = svm.LinearSVC()\n",
    "    \n",
    "    sgd = linear_model.SGDClassifier(max_iter = 1000, tol = 0.001)\n",
    "    \n",
    "    return [gbc, abc, s_clf, sgd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(all_embed, classifier):\n",
    "    cl_data = list(enumerate(all_embed))\n",
    "\n",
    "    df_ids = list(df[\"id\"])\n",
    "\n",
    "    cl_data = lfilter(lambda x: i2w[x[0]] in df_ids, cl_data)\n",
    "\n",
    "    cl_data = lmap(\n",
    "        lambda x: [x[0], x[1], list(df[df.id == i2w[x[0]]].general)[0]], \n",
    "        cl_data\n",
    "    )\n",
    "    \n",
    "    cl_data = lfilter(lambda x: x[2] in cat_set, cl_data)\n",
    "    \n",
    "    cl_train, cl_test = train_test_split(cl_data)\n",
    "    \n",
    "    trained = classifier.fit(lmap(lambda x: x[1], cl_train), lmap(lambda x: x[2], cl_train))\n",
    "    \n",
    "    return [trained, f1_score(\n",
    "        lmap(lambda x: x[2], cl_test),\n",
    "        trained.predict(lmap(lambda x: x[1], cl_test)),\n",
    "        average = \"micro\"\n",
    "    )]\n",
    "\n",
    "def classify_model(model_name, classifier):\n",
    "    return classify(get_all_embed(load(model_name)), classifier)\n",
    "\n",
    "def classify_all(embeds):\n",
    "    classifiers = get_classifiers()\n",
    "    \n",
    "    for c in classifiers:\n",
    "        __, f1 = classify(embeds, c)\n",
    "        \n",
    "        print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_embed_file = glob.glob(\"scala/**/out/ALS_embeddings/*.json\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embeds(embed_dict, i2w_file):\n",
    "    _i2w = load(i2w_file)\n",
    "    \n",
    "    res = list(embed_dict.items())\n",
    "    \n",
    "    res = lmap(lambda x: (_i2w[x[0]], x[1]), res)\n",
    "    \n",
    "    res = lmap(lambda x: (w2i[x[0]], x[1]), res)\n",
    "    \n",
    "    sorted(res, key = lambda x: x[0])\n",
    "    \n",
    "    return lmap(lambda x: x[1], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_embeds = []\n",
    "\n",
    "with open(als_embed_file) as json_file: \n",
    "    raw_als_embeds = dict()\n",
    "    \n",
    "    embed_list = []\n",
    "    \n",
    "    for single in json_file.readlines():\n",
    "        embed_list.append(json.loads(single))\n",
    "        \n",
    "    for emb in embed_list:\n",
    "        raw_als_embeds[emb['id']] = emb['features']\n",
    "        \n",
    "    als_embeds = map_embeds(raw_als_embeds, \"als_i2w\")\n",
    "    \n",
    "    assert len(als_embeds) == len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_embed_file = glob.glob(\"scala/**/out/lda_embeddings\")[0]\n",
    "\n",
    "lda_embeds = []\n",
    "\n",
    "with open(lda_embed_file) as f: \n",
    "    raw_lda_embeds = dict()\n",
    "    \n",
    "    embed_list = []\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for single in f.readlines():\n",
    "        raw_lda_embeds[i] = lmap(lambda x: float(x),\n",
    "            lfilter(\n",
    "                lambda x: len(x), \n",
    "                re.split(\" \", single.rstrip())\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        i += 1\n",
    "            \n",
    "    lda_embeds = map_embeds(raw_lda_embeds, \"lda_i2w\")\n",
    "    \n",
    "    assert len(lda_embeds) == len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_all(get_all_embed(load(\"final_model\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_all(als_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_all(lda_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
