{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from six.moves import xrange \n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "log = logging.getLogger('log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "lhnd = logging.StreamHandler()\n",
    "lhnd.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "lhnd.setFormatter(formatter)\n",
    "\n",
    "log.addHandler(lhnd)\n",
    "\n",
    "%autonotify -a 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_dumps = False\n",
    "\n",
    "def lmap(f, arr):\n",
    "    return list(map(f, arr))\n",
    "\n",
    "def lfilter(f, arr):\n",
    "    return list(filter(f, arr))\n",
    "\n",
    "def foreach(it, f):\n",
    "    for e in it:\n",
    "        f(e)\n",
    "        \n",
    "def dump(data, name):\n",
    "    with open('data/' + name, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load(name):\n",
    "    with open('data/' + name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def load_or_dump(path, func):\n",
    "    if not Path('data/' + path).exists() or ignore_dumps:\n",
    "        res = func()\n",
    "    \n",
    "        dump(res, path)\n",
    "    else:\n",
    "        res = load(path)\n",
    "        \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "with open('auth/token') as f:\n",
    "    token = f.readline().strip()\n",
    "\n",
    "def get_info(ids):\n",
    "    sleep(0.2)\n",
    "    mc = 'members_count'\n",
    "    payload = {'v': '5.92', 'access_token': token, 'fields':mc}\n",
    "    \n",
    "    str_ids = functools.reduce(\n",
    "        lambda x, y: x + y,\n",
    "        lmap(lambda x: str(x) + ',', ids)\n",
    "    )\n",
    "    \n",
    "    print(str_ids)\n",
    "    \n",
    "    payload['group_ids'] = str_ids[0:- 1]\n",
    "    \n",
    "    r = requests.get('https://api.vk.com/method/groups.getById', \n",
    "                     params=payload)\n",
    "    \n",
    "    if (not 'response' in r.json()):\n",
    "        print(r.json())\n",
    "        \n",
    "    res = lmap(lambda x: (x['name'], x['screen_name'], \"{:,}\".format(x[mc]) if mc in x else -1),r.json()['response'])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 1015925\n",
    "\n",
    "def raw_data_filter(file):\n",
    "    # Mapping to events\n",
    "    res = list()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for line in file:\n",
    "        cur = line.rstrip().split(',')\n",
    "        cur = lmap(lambda p: (re.sub(';.*', '', p), re.sub('.*;', '', p)), cur)\n",
    "\n",
    "        session = list()\n",
    "        \n",
    "        for j in range(0, len(cur)):\n",
    "            try:\n",
    "                session.append(int(cur[j][1]))\n",
    "            except ValueError:\n",
    "                None\n",
    "                \n",
    "        res.append(session)\n",
    "\n",
    "        i = i + 1\n",
    "                \n",
    "        if (i % 100000 == 0):\n",
    "            gc.collect()\n",
    "\n",
    "            log.debug(\"%d %% of mapping is done.\", i / total * 100)\n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode == 0):\n",
    "    raw_data = load_or_dump('raw', lambda: raw_data_filter(open(\"data/public_sessions_2.txt\",\"r\")))\n",
    "\n",
    "    log.info(\"Data loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_count(data):\n",
    "    total = dict()\n",
    "\n",
    "    for i in data:\n",
    "        for j in i[0]:\n",
    "            if (j in total.keys()):\n",
    "                total[j] = total[j] + 1\n",
    "            else:\n",
    "                total[j] = 1\n",
    "                \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words_data(file):\n",
    "    words = []\n",
    "    \n",
    "    for line in file:\n",
    "        for word in line.split():\n",
    "            words.append(word)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode == 1):\n",
    "    words_size = 50000\n",
    "    \n",
    "    ignore_dumps = True\n",
    "    data = load_or_dump('raw_txt', lambda: load_words_data(open(\"data/text8.txt\",\"r\")))\n",
    "    groups = group_count([[data]])\n",
    "    \n",
    "    dictlist = list(groups.items())\n",
    "    dictlist.sort(key = lambda x: x[1])\n",
    "    allowed = set(lmap(lambda x: x[0], dictlist[-words_size:]))\n",
    "\n",
    "    for i in xrange(len(data)):\n",
    "        if not data[i] in allowed:\n",
    "            data[i] = '-1'\n",
    "            \n",
    "    groups = group_count([[data]])\n",
    "    \n",
    "    data = [[data, []]]\n",
    "    \n",
    "    print(len(groups))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_session_size = 2\n",
    "max_session_size = 20\n",
    "\n",
    "def initiail_mapping(lst, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set()\n",
    "        malformed = set()\n",
    "        \n",
    "        for event in session:\n",
    "            if (event < 0):\n",
    "                sub_event = -event\n",
    "                \n",
    "                if (sub_event in sub or sub_event in malformed):\n",
    "                    sub.discard(sub_event)\n",
    "                    unsub.discard(sub_event)\n",
    "                    malformed.add(sub_event)\n",
    "                else:\n",
    "                    unsub.add(sub_event)\n",
    "            else:\n",
    "                if (event in unsub or event in malformed):\n",
    "                    unsub.discard(event)\n",
    "                    sub.discard(event)\n",
    "                    malformed.add(event)\n",
    "                else:\n",
    "                    sub.add(event)\n",
    "        \n",
    "        if (len(sub) >= min_session_size and len(sub) <= max_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "    \n",
    "\n",
    "def set_map(lst, cnt, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set() \n",
    "        \n",
    "        for event in session[0]:\n",
    "            if (cnt[event] > min_allowed):\n",
    "                sub.add(event)\n",
    "                \n",
    "        for event in session[1]:\n",
    "            if (cnt.get(event, -1) > min_allowed):\n",
    "                unsub.add(event)    \n",
    "        \n",
    "        if (len(sub) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def drop_uncommon(raw_data, min_allowed = 10):\n",
    "    cnt = None\n",
    "    sorted_cnt = None\n",
    "    \n",
    "    data, groups = initiail_mapping(raw_data, min_allowed)\n",
    "    cnt = group_count(data) \n",
    "    sorted_cnt = sorted(list(cnt.values()))\n",
    "    \n",
    "    while (cnt == None or sorted_cnt[0] < min_allowed):\n",
    "        data, groups = set_map(data, cnt, min_allowed)\n",
    "                \n",
    "        cnt = group_count(data) \n",
    "        sorted_cnt = sorted(list(cnt.values()))\n",
    "        \n",
    "        log.info(\"Length of data:   %d\", len(data))\n",
    "        log.info(\"Total length:     %d\", \n",
    "                functools.reduce((lambda x, y: x + y), lmap(lambda a: len(a), data))\n",
    "                )\n",
    "        log.info(\"Number of groups: %d\", len(groups))\n",
    "        log.info(\"Minimum count:    %d\\n\", sorted_cnt[0])\n",
    "        \n",
    "    return data, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (mode == 0):\n",
    "    ignore_dumps = True\n",
    "    data, groups = load_or_dump('final_data', lambda: drop_uncommon(raw_data, 50))\n",
    "\n",
    "    most_common = sorted(group_count(data).items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = {w: i for i, w in enumerate(groups)}\n",
    "i2w = {i: w for i, w in enumerate(groups)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i2w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = None\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dex = 0\n",
    "event_dex = 0\n",
    "\n",
    "def generate_batch(batch_size, negative_size, window_size = 1):\n",
    "    assert min_session_size > 1\n",
    "    \n",
    "    global session_dex\n",
    "    global event_dex\n",
    "    \n",
    "    labels = np.ndarray(shape=(batch_size, window_size), dtype=np.int32)\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    negative = np.ndarray(shape=(batch_size, negative_size), dtype=np.int32)\n",
    "     \n",
    "    current = 0\n",
    "    session = list(data[session_dex][0])\n",
    "    \n",
    "    for i in range(0, batch_size):     \n",
    "        batch[i] = w2i[session[event_dex]]\n",
    "        \n",
    "        for j in range(1, window_size + 1):\n",
    "            labels[i][j - 1] = w2i[session[(event_dex + j) % len(session)]]\n",
    "            \n",
    "            if (labels[i][j - 1] == batch[i]):\n",
    "                labels[i][j - 1] = labels[i][j - 2]\n",
    "            \n",
    "        neg = 0\n",
    "        \n",
    "        for j in data[session_dex][1]:\n",
    "            negative[i][neg] = w2i[j]\n",
    "            neg += 1\n",
    "            if (neg == negative_size):\n",
    "                break\n",
    "                \n",
    "        rand_neg = np.random.randint(len(groups), size=negative_size - neg)\n",
    "        \n",
    "        for j in range(0, negative_size - neg):\n",
    "            negative[i][neg + j] = rand_neg[j]\n",
    "            \n",
    "        event_dex += 1\n",
    "\n",
    "        if (event_dex == len(session)):\n",
    "            event_dex = 0\n",
    "            session_dex = session_dex + 1\n",
    "            if (session_dex >= len(data)):\n",
    "                session_dex = 0\n",
    "            session = list(data[session_dex][0])        \n",
    "     \n",
    "#     return batch, labels, []\n",
    "    return batch, labels, negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session_dex = 0\n",
    "\n",
    "print(data[session_dex])\n",
    "print(data[session_dex + 1])\n",
    "\n",
    "batch, labels, negative = generate_batch(16, 2, 2)\n",
    "\n",
    "for i in range(10):\n",
    "#     print(i2w[batch[i]], '->', lmap(lambda x: i2w[x], labels[i]), )\n",
    "    print(i2w[batch[i]], '->', lmap(lambda x: i2w[x], labels[i]), '-> (negative)', lmap(lambda x: i2w[x], negative[i]))\n",
    "    \n",
    "print(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data = None\n",
    "test_ids=lfilter(lambda x: x in w2i, [129440544, 28261334, 92876084, 51016572, 91933860])\n",
    "\n",
    "if (mode == 1):\n",
    "    test_ids = ['term', 'first', 'used', 'early', 'against', 'working']\n",
    "\n",
    "learning_rate = 0.1\n",
    "vocab_size = len(groups)\n",
    "\n",
    "window_size = 4\n",
    "embedding_size = 32\n",
    "negative_size = 10\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(emb, index, f = None):\n",
    "    p = emb[index]\n",
    "    cnst = tf.constant(p, shape=[1, embedding_size])\n",
    "    d = tf.matmul(cnst, emb, transpose_b=True).eval()[0]\n",
    "\n",
    "    dxs = np.argsort(np.array(d))\n",
    "    \n",
    "    ids = []\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(dxs) - 10, len(dxs)):\n",
    "        ids.append(i2w[dxs[i]])\n",
    "        res.append(d[dxs[i]])\n",
    "    \n",
    "    if (mode == 0):\n",
    "        info = get_info(ids)\n",
    "    else:\n",
    "        info = ids\n",
    "    \n",
    "    for i in xrange(len(res)):\n",
    "        print(ids[i], ' ', res[i], ' ', info[i])\n",
    "        \n",
    "        if (f != None):\n",
    "            f.write(str(ids[i]) + ' ' + str(res[i]) + ' ' + str(info[i]) + '\\n')\n",
    "            \n",
    "def test(model):\n",
    "    for i in test_ids:\n",
    "        t = []\n",
    "        \n",
    "        for j in range(vocab_size):\n",
    "            fst = Variable(torch.LongTensor([w2i[i]]))\n",
    "            snd = Variable(torch.LongTensor([j]))\n",
    "            t.append([model.score(fst, snd), i2w[j]])\n",
    "        \n",
    "        t.sort(key = lambda x: -x[0])\n",
    "        \n",
    "        ids = []\n",
    "        res = t[:10]\n",
    "       \n",
    "        for k in res:\n",
    "            ids.append(k[1])\n",
    "            \n",
    "        info = get_info(ids)\n",
    "        \n",
    "        print(i)\n",
    "        for i in range(10):\n",
    "            print(res[i], ' ', info[i])\n",
    "#         print(t[0][1])\n",
    "#         print(get_info(t[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iterations = 1350000\n",
    "\n",
    "def loss_sampled(scores):\n",
    "    res = scores[0]\n",
    "    \n",
    "    for i in range(1, len(scores)):\n",
    "        res = res + scores[i]\n",
    "        \n",
    "    return res * Variable(torch.Tensor([-1]))\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.in_embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.out_embeddings(context).view((1, -1))\n",
    "\n",
    "        score = torch.mm(embed_focus, torch.t(embed_ctx))\n",
    "      \n",
    "        return score\n",
    "    \n",
    "    def score(self, focus, context):\n",
    "        embed_focus = self.in_embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.in_embeddings(context).view((1, -1))\n",
    "\n",
    "        score = F.cosine_similarity(embed_focus, embed_ctx)\n",
    "    \n",
    "        return score\n",
    "    \n",
    "model = SkipGram(vocab_size, embedding_size)    \n",
    "pref = \"/m1/\"\n",
    "    \n",
    "def train_skipgram():\n",
    "    losses = []\n",
    "    loss_fn = loss_sampled\n",
    "\n",
    "    print(model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    total_loss = .0\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        target, contexts, negative = generate_batch(batch_size, negative_size, window_size)\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        it_losses = []\n",
    "        \n",
    "        for j in range(len(target)):  \n",
    "            scores = []\n",
    "            \n",
    "            in_w_var = Variable(torch.LongTensor([target[j]]))\n",
    "\n",
    "            for ctx in contexts[j]:\n",
    "                out_w_var = Variable(torch.LongTensor([ctx]))\n",
    "\n",
    "                score = torch.sigmoid(model(in_w_var, out_w_var))\n",
    "\n",
    "                if (score != 0):\n",
    "                    scores.append(torch.log(score))\n",
    "                else:\n",
    "                    scores.append(torch.log(score + torch.Tensor([0.0000001])))\n",
    "\n",
    "            for neg in negative[j]:\n",
    "                out_w_var = Variable(torch.LongTensor([neg]))\n",
    "\n",
    "                score = torch.sigmoid(model(in_w_var, out_w_var) * Variable(torch.Tensor([-1])))\n",
    "\n",
    "                if (score != 0):\n",
    "                    scores.append(torch.log(score))\n",
    "                else:\n",
    "                    scores.append(torch.log(score + torch.Tensor([0.0000001])))           \n",
    "            \n",
    "#         with torch.autograd.detect_anomaly():\n",
    "#             print(scores)\n",
    "            it_losses.append(loss_fn(scores))\n",
    "    \n",
    "        loss = it_losses[0]\n",
    "#         loss = torch.mean(torch.stack(it_losses))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (i % 2000 == 0):\n",
    "            if i > 0:\n",
    "                total_loss /= 2000\n",
    "                \n",
    "            if (i % 10000 == 0):\n",
    "                dump(model, pref + str(i))\n",
    "                test(model)\n",
    "            \n",
    "            log.debug('Average loss at step %d: %.4f', i, total_loss)\n",
    "            total_loss = 0\n",
    "            losses.append(total_loss)\n",
    "      \n",
    "    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_model, sg_losses = train_skipgram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "test_ids=[129440544, 28261334, 92876084, 51016572, 91933860]\n",
    "\n",
    "for i in data:\n",
    "    for j in test_ids:\n",
    "        if j in i[0]:\n",
    "            f = list(i[0])\n",
    "            f.remove(j)\n",
    "            f.append(j)\n",
    "            lst.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lst))\n",
    "print(len(lst[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in xrange(100):\n",
    "#     print(get_info(lst[i]))\n",
    "#     print()\n",
    "#     print(\"==================================================\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram2(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram2, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.embeddings(context).view((1, -1))\n",
    "\n",
    "        score = torch.mm(embed_focus, torch.t(embed_ctx))\n",
    "      \n",
    "        return score\n",
    "    \n",
    "    def score(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.embeddings(context).view((1, -1))\n",
    "\n",
    "        score = F.cosine_similarity(embed_focus, embed_ctx)\n",
    "    \n",
    "        return score\n",
    "    \n",
    "model = SkipGram2(vocab_size, embedding_size)    \n",
    "pref = \"/m2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_model, sg_losses = train_skipgram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
