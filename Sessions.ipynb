{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import requests\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import sklearn\n",
    "\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "log = logging.getLogger('log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "lhnd = logging.StreamHandler()\n",
    "lhnd.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "lhnd.setFormatter(formatter)\n",
    "\n",
    "log.addHandler(lhnd)\n",
    "\n",
    "%autonotify -a 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_dumps = False\n",
    "\n",
    "def lmap(f, arr):\n",
    "    return list(map(f, arr))\n",
    "\n",
    "def lfilter(f, arr):\n",
    "    return list(filter(f, arr))\n",
    "\n",
    "def foreach(it, f):\n",
    "    for e in it:\n",
    "        f(e)\n",
    "        \n",
    "def dump(data, name):\n",
    "    with open('data/' + name, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load(name):\n",
    "    with open('data/' + name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def load_or_dump(path, func):\n",
    "    if not Path('data/' + path).exists() or ignore_dumps:\n",
    "        res = func()\n",
    "    \n",
    "        dump(res, path)\n",
    "    else:\n",
    "        res = load(path)\n",
    "        \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "with open('auth/token') as f:\n",
    "    token = f.readline().strip()\n",
    "\n",
    "def get_info(ids):\n",
    "    sleep(0.2)\n",
    "    mc = 'members_count'\n",
    "    payload = {'v': '5.92', 'access_token': token, 'fields':mc}\n",
    "    \n",
    "    str_ids = functools.reduce(\n",
    "        lambda x, y: x + y,\n",
    "        lmap(lambda x: str(x) + ',', ids)\n",
    "    )\n",
    "    \n",
    "    print(str_ids)\n",
    "    \n",
    "    payload['group_ids'] = str_ids[0:- 1]\n",
    "    \n",
    "    r = requests.get('https://api.vk.com/method/groups.getById', \n",
    "                     params=payload)\n",
    "    \n",
    "    if (not 'response' in r.json()):\n",
    "        print(r.json())\n",
    "        \n",
    "    res = lmap(lambda x: (x['name'], x['screen_name'], \"{:,}\".format(x[mc]) if mc in x else -1),r.json()['response'])\n",
    "    \n",
    "    return(res)\n",
    "\n",
    "def info_print(lst):\n",
    "    info = get_info(lst)\n",
    "    \n",
    "    print(lmap(lambda x: x[0], info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 1015925\n",
    "\n",
    "def raw_data_filter(file):\n",
    "    # Mapping to events\n",
    "    res = list()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for line in file:\n",
    "        cur = line.rstrip().split(',')\n",
    "        cur = lmap(lambda p: (re.sub(';.*', '', p), re.sub('.*;', '', p)), cur)\n",
    "\n",
    "        session = list()\n",
    "        \n",
    "        for j in range(0, len(cur)):\n",
    "            try:\n",
    "                session.append(int(cur[j][1]))\n",
    "            except ValueError:\n",
    "                None\n",
    "                \n",
    "        res.append(session)\n",
    "\n",
    "        i = i + 1\n",
    "                \n",
    "        if (i % 100000 == 0):\n",
    "            gc.collect()\n",
    "\n",
    "            log.debug(\"%d %% of mapping is done.\", i / total * 100)\n",
    "\n",
    "    \n",
    "    return res\n",
    "\n",
    "def raw_data_filter_likes(file):\n",
    "    # Mapping to events\n",
    "    res = list()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    groups = set()\n",
    "    \n",
    "    for line in file:\n",
    "        cur = line.rstrip().split(',')\n",
    "        cur = lmap(lambda p: (re.sub(';.*', '', p), re.sub('.*;', '', p)), cur)\n",
    "\n",
    "        session = set()\n",
    "        \n",
    "        for j in range(0, len(cur)):\n",
    "            try:\n",
    "                action = int(cur[j][1])\n",
    "                \n",
    "                session.add(action)\n",
    "            \n",
    "                groups.add(action)\n",
    "            except ValueError:\n",
    "                None\n",
    "                \n",
    "        res.append((session, None))\n",
    "\n",
    "        i = i + 1\n",
    "                \n",
    "        if (i % 100000 == 0):\n",
    "            gc.collect()\n",
    "\n",
    "            log.debug(\"%d %% of mapping is done.\", i / total * 100)\n",
    "\n",
    "    \n",
    "    return (res, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw(raw, file):\n",
    "    raw_data = load_or_dump(raw, lambda: raw_data_filter(open(file, \"r\")))\n",
    "\n",
    "    log.info(\"Data loaded\")\n",
    "    \n",
    "    return raw_data   \n",
    "\n",
    "def load_comdined(raw_subs, file_subs, raw_likes, file_likes):\n",
    "    raw_data = load_raw(raw_subs, file_subs)\n",
    "    \n",
    "    raw_data_likes = load_raw(raw_likes, file_likes)\n",
    "    \n",
    "    log.info(\"All data loaded\")\n",
    "\n",
    "    raw_data = lmap(lambda x: (x, 0), raw_data)\n",
    "    \n",
    "    raw_data_likes = lmap(lambda x: (x, 3), raw_data_likes)\n",
    "    \n",
    "    raw_data.extend(raw_data_likes)\n",
    "    \n",
    "    raw_data_likes = None\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    random.shuffle(raw_data)\n",
    "    \n",
    "    return raw_data[:int(total)]\n",
    "\n",
    "if (mode == 0):\n",
    "    raw_data = load_raw(\"raw\", \"data/public_sessions_2.txt\")\n",
    "    \n",
    "if (mode == 2):\n",
    "    total = 10927682\n",
    "    \n",
    "    if (ignore_dumps):\n",
    "        raw_data = load_or_dump('raw_likes', lambda: raw_data_filter_likes(open(\"data/all_sessions_likes.txt\",\"r\")))\n",
    "\n",
    "        log.info(\"Data loaded\")\n",
    "        \n",
    "if (mode == 3):\n",
    "    raw_data = load_raw('trim_likes', \"data/trim_likes.txt\")\n",
    "    \n",
    "if (mode == 4):\n",
    "    raw_data = load_comdined(\"raw\", \"data/public_sessions_2.txt\", \"trim_likes\", \"data/trim_likes.txt\")\n",
    "    \n",
    "if (mode == 9):\n",
    "    raw_data = load_raw('raw_small', \"data/public_sessions_2_small.txt\")\n",
    "    \n",
    "if (mode == 10):   \n",
    "    raw_data = load_raw('raw_likes_small', \"data/likes_small.txt\")\n",
    "    \n",
    "if (mode == 11):\n",
    "    ignore_dumps = False\n",
    "    \n",
    "    total = 185000\n",
    "    \n",
    "    raw_data = load_comdined(\"raw_small\", \"data/public_sessions_2_small.txt\", \n",
    "                             \"raw_likes_small\", \"data/likes_small.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_count(data):\n",
    "    total = dict()\n",
    "\n",
    "    for i in data:\n",
    "        for j in i[0]:\n",
    "            if (j in total.keys()):\n",
    "                total[j] = total[j] + 1\n",
    "            else:\n",
    "                total[j] = 1\n",
    "                \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words_data(file):\n",
    "    words = []\n",
    "    \n",
    "    for line in file:\n",
    "        for word in line.split():\n",
    "            words.append(word)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode == 1):\n",
    "    words_size = 50000\n",
    "    \n",
    "    ignore_dumps = True\n",
    "    data = load_or_dump('raw_txt', lambda: load_words_data(open(\"data/text8.txt\",\"r\")))\n",
    "    groups = group_count([[data]])\n",
    "    \n",
    "    dictlist = list(groups.items())\n",
    "    dictlist.sort(key = lambda x: x[1])\n",
    "    allowed = set(lmap(lambda x: x[0], dictlist[-words_size:]))\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if not data[i] in allowed:\n",
    "            data[i] = '-1'\n",
    "            \n",
    "    groups = group_count([[data]])\n",
    "    \n",
    "    data = [[data, []]]\n",
    "    \n",
    "    print(len(groups))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_session_size = 2\n",
    "max_session_size = 20\n",
    "\n",
    "def handle_session(session, groups):\n",
    "    unsub = set()\n",
    "    sub = set()\n",
    "    malformed = set()\n",
    "\n",
    "    for event in session:\n",
    "        if (event < 0):\n",
    "            sub_event = -event\n",
    "\n",
    "            if (sub_event in sub or sub_event in malformed):\n",
    "                sub.discard(sub_event)\n",
    "                unsub.discard(sub_event)\n",
    "                malformed.add(sub_event)\n",
    "            else:\n",
    "                unsub.add(sub_event)\n",
    "        else:\n",
    "            if (event in unsub or event in malformed):\n",
    "                unsub.discard(event)\n",
    "                sub.discard(event)\n",
    "                malformed.add(event)\n",
    "            else:\n",
    "                sub.add(event)\n",
    "\n",
    "    if (len(sub) >= min_session_size and len(sub) <= max_session_size):\n",
    "        for event in sub:\n",
    "            groups.add(event)\n",
    "        for event in unsub:\n",
    "            groups.add(event)\n",
    "\n",
    "        return (sub, unsub)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def initiail_mapping(lst):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        s = handle_session(session, groups)\n",
    "    \n",
    "        if (s != None):\n",
    "            result.append(s)\n",
    "        \n",
    "    return result, groups\n",
    "    \n",
    "def initiail_mapping_likes2(lst):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for id, session in enumerate(lst):\n",
    "        if (id % 100000 == 0):\n",
    "            log.info(\"Processing %i\", id)\n",
    "        \n",
    "        if (len(session) >= min_session_size and \n",
    "            len(session) <= max_session_size and \n",
    "            len(set(session)) >= min_session_size):\n",
    "            for event in session:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((session, set()))\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def initiail_mapping_combined(lst):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for id, single in enumerate(lst):\n",
    "        if (id % 100000 == 0):\n",
    "            log.info(\"Processing %i\", id)\n",
    "        \n",
    "        session = single[0]\n",
    "        \n",
    "        if (single[1] == 0):\n",
    "            s = handle_session(session, groups)\n",
    "            \n",
    "            if (s != None):\n",
    "                result.append((s[0], s[1], 0))\n",
    "        else:\n",
    "            sub = set(session)\n",
    "\n",
    "            if (len(sub) >= min_session_size and len(sub) <= max_session_size):\n",
    "                for event in sub:\n",
    "                    groups.add(event)\n",
    "\n",
    "                result.append((sub, set(), single[1]))\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def set_map_handle(cnt, min_allowed, session, groups):\n",
    "    sub = set() \n",
    "        \n",
    "    unsub = None\n",
    "\n",
    "    for event in session[0]:\n",
    "        if (cnt[event] > min_allowed):\n",
    "            sub.add(event)\n",
    "\n",
    "    if (session[1] != None):\n",
    "        unsub = set()\n",
    "\n",
    "        for event in session[1]:\n",
    "            if (cnt.get(event, -1) > min_allowed):\n",
    "                unsub.add(event)    \n",
    "\n",
    "    if (len(sub) >= min_session_size):\n",
    "        for event in sub:\n",
    "            groups.add(event)\n",
    "\n",
    "        if (unsub != None):\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "\n",
    "        return (sub, unsub)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def set_map(lst, cnt, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for id, session in enumerate(lst):\n",
    "        if (id % 100000 == 0):\n",
    "            log.info(\"Processing %i\", id)\n",
    "\n",
    "        s = set_map_handle(cnt, min_allowed, session, groups)\n",
    "        \n",
    "        if (s != None):\n",
    "            result.append(s)        \n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def set_map_likes(lst, cnt, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for id, session in enumerate(lst):\n",
    "        if (id % 100000 == 0):\n",
    "            log.info(\"Processing %i\", id)\n",
    "\n",
    "        sub = list() \n",
    "        \n",
    "        unsub = None\n",
    "        \n",
    "        for event in session[0]:\n",
    "            if (cnt[event] > min_allowed):\n",
    "                sub.append(event)\n",
    "          \n",
    "        if (len(sub) >= min_session_size and len(set(sub)) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "\n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def set_map_combined(lst, cnt, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "        \n",
    "    for id, session in enumerate(lst):\n",
    "        if (id % 100000 == 0):\n",
    "            log.info(\"Processing %i\", id)\n",
    "            \n",
    "        if (session[2] == 0):\n",
    "            s = set_map_handle(cnt, min_allowed, session, groups)\n",
    "\n",
    "            if (s != None):\n",
    "                result.append((s[0], s[1], session[2]))   \n",
    "        else:\n",
    "            sub = list() \n",
    "        \n",
    "            unsub = None\n",
    "\n",
    "            for event in session[0]:\n",
    "                if (cnt[event] > min_allowed):\n",
    "                    sub.append(event)\n",
    "\n",
    "            if (len(sub) >= min_session_size and len(set(sub)) >= min_session_size):\n",
    "                for event in sub:\n",
    "                    groups.add(event)\n",
    "\n",
    "                result.append((sub, unsub, session[2]))\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def drop_uncommon(raw_data, init_mapping_fn, set_map_fn, min_allowed = 10):\n",
    "    cnt = None\n",
    "    sorted_cnt = None\n",
    "    \n",
    "    data, groups = init_mapping_fn(raw_data)\n",
    "    cnt = group_count(data) \n",
    "    sorted_cnt = sorted(list(cnt.values()))\n",
    "    \n",
    "    while (cnt == None or sorted_cnt[0] < min_allowed):\n",
    "        gc.collect()\n",
    "        \n",
    "        data, groups = set_map_fn(data, cnt, min_allowed)\n",
    "                \n",
    "        cnt = group_count(data) \n",
    "        sorted_cnt = sorted(list(cnt.values()))\n",
    "        \n",
    "        log.info(\"Length of data:   %d\", len(data))\n",
    "        log.info(\"Total length:     %d\", \n",
    "                functools.reduce((lambda x, y: x + y), lmap(lambda a: len(a), data))\n",
    "                )\n",
    "        log.info(\"Number of groups: %d\", len(groups))\n",
    "        log.info(\"Minimum count:    %d\\n\", sorted_cnt[0])\n",
    "        \n",
    "    return data, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode != 2):\n",
    "    print(raw_data[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ignore_dumps = False\n",
    "\n",
    "def map_subs(final_data):\n",
    "    data, groups = load_or_dump(final_data, lambda: drop_uncommon(raw_data, initiail_mapping, set_map, 50))\n",
    "\n",
    "    most_common = sorted(group_count(data).items(), key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    return data, groups, most_common \n",
    "\n",
    "def map_likes(final_data):\n",
    "    data, groups = load_or_dump(final_data, lambda: drop_uncommon(\n",
    "        raw_data, \n",
    "        initiail_mapping_likes2, \n",
    "        set_map_likes, \n",
    "        50\n",
    "    ))\n",
    "\n",
    "    most_common = sorted(group_count(data).items(), key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    return data, groups, most_common \n",
    "\n",
    "def map_combined(final_data):\n",
    "    data, groups = load_or_dump(final_data, lambda: drop_uncommon(\n",
    "        raw_data, \n",
    "        initiail_mapping_combined, \n",
    "        set_map_combined, \n",
    "        50\n",
    "    ))\n",
    "\n",
    "    most_common = sorted(group_count(data).items(), key=lambda x: x[1], reverse=True) \n",
    "    \n",
    "    return data, groups, most_common  \n",
    "\n",
    "\n",
    "if (mode == 0):\n",
    "    ignore_dumps = False\n",
    "    \n",
    "    data, groups, most_common = map_subs('final_data')\n",
    "    \n",
    "if (mode == 2):\n",
    "    ignore_dumps = False\n",
    "    \n",
    "    data, groups = load_or_dump('final_data_likes', lambda: drop_uncommon(raw_data, lambda x: x, set_map, 50))\n",
    "\n",
    "    most_common = sorted(group_count(data).items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "if (mode == 3):\n",
    "    ignore_dumps = False\n",
    "    \n",
    "    data, groups, most_common = map_likes('final_data_trim_likes')\n",
    "    \n",
    "if (mode == 4):\n",
    "    ignore_dumps = False\n",
    "    \n",
    "    data, groups, most_common = map_combined('final_data_combined')\n",
    "    \n",
    "if (mode == 9):\n",
    "    ignore_dumps = False\n",
    "    \n",
    "    data, groups, most_common = map_subs('final_data_small')\n",
    "    \n",
    "if (mode == 10):\n",
    "    ignore_dumps = False\n",
    "    \n",
    "    data, groups, most_common = map_likes('final_data_small_likes')\n",
    "    \n",
    "if (mode == 11):\n",
    "    ignore_dumps = True\n",
    "    \n",
    "    data, groups, most_common = map_combined('final_data_small_combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(groups))\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = {w: i for i, w in enumerate(groups)}\n",
    "i2w = {i: w for i, w in enumerate(groups)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_path = \"data/filtered\"\n",
    "\n",
    "def ds_sub(id, session):\n",
    "    for sub in session[0]:\n",
    "        out.write(str(id) + \" \" + str(w2i[sub]) + \"\\n\")\n",
    "\n",
    "    for unsub in session[1]:\n",
    "        out.write(str(id) + \" \" + str(-w2i[unsub]) + \"\\n\")\n",
    "        \n",
    "def ds_sub2(id, session, nl = \"\\n\"):\n",
    "    for sub in session[0]:\n",
    "        out.write(str(w2i[sub]) + \" \")\n",
    "\n",
    "    out.write(nl)\n",
    "    \n",
    "def ds_like(id, session):\n",
    "    _s = set(session[0])\n",
    "                \n",
    "    for sub in _s:\n",
    "        out.write(str(id) + \" \" + str(w2i[sub]) + \" \" + str(session[0].count(sub)) + \"\\n\")\n",
    "    \n",
    "def ds_like2(id, session):\n",
    "    for sub in session[0]:\n",
    "        out.write(str(w2i[sub]) + \" \")\n",
    "\n",
    "    out.write(\"\\n\")\n",
    "\n",
    "if not Path(fil_path).exists():\n",
    "    if (mode == 9):\n",
    "        with open(fil_path, \"w\") as out:  \n",
    "            for id, session in enumerate(data):\n",
    "                ds_sub(id, session)\n",
    "\n",
    "        with open(\"data/filtered_all\", \"w\") as out:  \n",
    "            for id, session in enumerate(data):\n",
    "                ds_sub2(id, session)\n",
    "                \n",
    "    if (mode == 10):\n",
    "        with open(fil_path, \"w\") as out:  \n",
    "            for id, session in enumerate(data):\n",
    "                ds_like(id, session)\n",
    "\n",
    "        with open(\"data/filtered_all\", \"w\") as out:  \n",
    "            for id, session in enumerate(data):\n",
    "                ds_like2(id, session)\n",
    "                \n",
    "    if (mode == 11):\n",
    "        with open(fil_path, \"w\") as out:  \n",
    "            for id, session in enumerate(data):\n",
    "                if (session[2] == 0):\n",
    "                    ds_sub(id, session)\n",
    "                else:\n",
    "                    ds_like(id, session)\n",
    "\n",
    "        with open(\"data/filtered_all\", \"w\") as out:  \n",
    "            for id, session in enumerate(data):\n",
    "                if (session[2] == 0):\n",
    "                    ds_sub2(id, session, \"*\\n\")\n",
    "                else:\n",
    "                    ds_like2(id, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i2w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = None\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dex = 0\n",
    "event_dex = 0\n",
    "\n",
    "def generate_window_batch(negative_size, window_size = 1):\n",
    "    assert min_session_size > 1\n",
    "    \n",
    "    global session_dex\n",
    "    global event_dex\n",
    "    \n",
    "    labels = []\n",
    "    batch = None\n",
    "    negative = np.ndarray(shape=(negative_size), dtype=np.int32)\n",
    "     \n",
    "    current = 0\n",
    "    session = list(data[session_dex][0])\n",
    "    \n",
    "    batch = w2i[session[event_dex]]\n",
    "    \n",
    "    for j in range(max(0, event_dex - window_size), event_dex):\n",
    "        labels.append(w2i[session[j]])\n",
    "\n",
    "    for j in range(event_dex + 1, min(len(session), event_dex + 1 + window_size)):\n",
    "        labels.append(w2i[session[j]])\n",
    "\n",
    "    neg = 0\n",
    "\n",
    "    if (data[session_dex][1] != None):\n",
    "        for j in data[session_dex]:\n",
    "            negative[neg] = w2i[j]\n",
    "            neg += 1\n",
    "            if (neg == negative_size):\n",
    "                break\n",
    "\n",
    "    rand_neg = np.random.randint(len(groups), size=negative_size - neg)\n",
    "\n",
    "    for j in range(0, negative_size - neg):\n",
    "        negative[neg + j] = rand_neg[j]\n",
    "\n",
    "    event_dex += 1\n",
    "\n",
    "    if (event_dex == len(session)):\n",
    "        event_dex = 0\n",
    "        session_dex = session_dex + 1\n",
    "        if (session_dex >= len(data)):\n",
    "            session_dex = 0\n",
    "        session = list(data[session_dex][0])        \n",
    "     \n",
    "    return batch, labels, negative\n",
    "\n",
    "\n",
    "def generate_batch(negative_size):\n",
    "    assert min_session_size > 1\n",
    "    \n",
    "    global session_dex\n",
    "    global event_dex\n",
    "    \n",
    "    session = list(data[session_dex][0])\n",
    "    \n",
    "    labels = lmap(lambda x: w2i[x], session)\n",
    "    batch = w2i[session[event_dex]]\n",
    "    negative = np.ndarray(shape=(negative_size), dtype=np.int32)\n",
    "                \n",
    "    neg = 0\n",
    "\n",
    "    if (data[session_dex][1] != None):\n",
    "        for j in data[session_dex][1]:\n",
    "            negative[neg] = w2i[j]\n",
    "            neg += 1\n",
    "            if (neg == negative_size):\n",
    "                break\n",
    "\n",
    "    rand_neg = np.random.randint(len(groups), size=negative_size - neg)\n",
    "\n",
    "    for j in range(0, negative_size - neg):\n",
    "        negative[neg + j] = rand_neg[j]\n",
    "\n",
    "    event_dex += 1\n",
    "\n",
    "    if (event_dex == len(session)):\n",
    "        event_dex = 0\n",
    "        session_dex = session_dex + 1\n",
    "        if (session_dex >= len(data)):\n",
    "            session_dex = 0 \n",
    "\n",
    "    return batch, labels, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session_dex = 2\n",
    "\n",
    "print(data[session_dex])\n",
    "print(data[session_dex + 1])\n",
    "\n",
    "if (mode == 3):\n",
    "    batch, labels, negative = generate_window_batch(10, 2)\n",
    "else:\n",
    "    batch, labels, negative = generate_batch(10)\n",
    "    \n",
    "print (batch)\n",
    "print (labels)\n",
    "print (negative)\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "print(i2w[batch], '->', lmap(lambda x: i2w[x], labels), '-> (negative)', lmap(lambda x: i2w[x], negative))\n",
    "    \n",
    "print(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = lfilter(lambda x: x in w2i, [129440544, 28261334, 92876084, 51016572, 91933860, 22751485])\n",
    "\n",
    "if (mode == 1):\n",
    "    test_ids = ['term', 'first', 'used', 'early', 'against', 'working']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for i in data:\n",
    "    for j in test_ids:\n",
    "        if j in i[0]:\n",
    "            f = list(i[0])\n",
    "            f.remove(j)\n",
    "            f.append(j)\n",
    "            lst.append(f)\n",
    "            \n",
    "random.shuffle(lst)\n",
    "\n",
    "if (len(lst) > 0):\n",
    "    for i in range(10):\n",
    "        sleep(1)\n",
    "        print(get_info(lst[i]))\n",
    "        print()\n",
    "        print(\"==================================================\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_device():\n",
    "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = available_device()\n",
    "\n",
    "# device = \"cpu\"\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    for i in test_ids:\n",
    "        t = []\n",
    "        \n",
    "        for j in range(vocab_size):\n",
    "            fst = Variable(torch.LongTensor([w2i[i]]).to(device))\n",
    "            snd = Variable(torch.LongTensor([j]).to(device))\n",
    "            t.append([model.score(fst, snd), i2w[j]])\n",
    "        \n",
    "        t.sort(key = lambda x: -x[0])\n",
    "        \n",
    "        ids = []\n",
    "        res = t[:10]\n",
    "       \n",
    "        for k in res:\n",
    "            ids.append(k[1])\n",
    "            \n",
    "        info = get_info(ids)\n",
    "        \n",
    "        print(i)\n",
    "        for i in range(10):\n",
    "            print(res[i], ' ', info[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "vocab_size = len(groups)\n",
    "\n",
    "window_size = 4\n",
    "embedding_size = 64\n",
    "negative_size = 5\n",
    "batch_size = 1\n",
    "\n",
    "pref = \"/None/\"\n",
    "\n",
    "session_dex = 0\n",
    "event_dex = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data[0])\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iterations = 501000\n",
    "\n",
    "def loss_sampled(scores):\n",
    "    res = scores[0]\n",
    "    \n",
    "    for i in range(1, len(scores)):\n",
    "        res = res + scores[i]\n",
    "        \n",
    "    return res * Variable(torch.Tensor([-1]).to(device))\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embd_size).to(device)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embd_size).to(device)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.in_embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.out_embeddings(context).view(-1, embedding_size)\n",
    "\n",
    "        score = torch.mm(embed_focus, torch.t(embed_ctx))\n",
    "      \n",
    "        return score\n",
    "    \n",
    "    def score(self, focus, context):\n",
    "        embed_focus = self.in_embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.in_embeddings(context).view((1, -1))\n",
    "\n",
    "        score = F.cosine_similarity(embed_focus, embed_ctx)\n",
    "    \n",
    "        return score\n",
    "    \n",
    "model = SkipGram(vocab_size, embedding_size).to(device)\n",
    "    \n",
    "def train_skipgram():\n",
    "    losses = []\n",
    "    loss_fn = loss_sampled\n",
    "    \n",
    "    global mode\n",
    "\n",
    "    print(model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    total_loss = .0\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        lm = mode\n",
    "        \n",
    "        c = 1\n",
    "        \n",
    "        if (len(data[session_dex]) > 2):\n",
    "            lm = data[session_dex][2]\n",
    "            \n",
    "            if (mode == 0):\n",
    "                c = 3\n",
    "        \n",
    "        if (lm == 3 or lm == 10):\n",
    "            target, contexts, negative = generate_window_batch(negative_size, window_size)\n",
    "        else:\n",
    "            target, contexts, negative = generate_batch(negative_size)\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        it_losses = []\n",
    "        \n",
    "        scores = []\n",
    "\n",
    "        in_w_var = Variable(torch.LongTensor([target])).to(device)\n",
    "\n",
    "        scrs = Variable(torch.LongTensor(contexts)).to(device)\n",
    "        \n",
    "        scrs = torch.sigmoid(model(in_w_var, scrs))\n",
    "        \n",
    "        scrs = torch.log(scrs)\n",
    "              \n",
    "        n_scrs = Variable(torch.LongTensor(negative)).to(device)\n",
    "        \n",
    "        n_scrs = torch.sigmoid(model(in_w_var, n_scrs) * Variable(torch.Tensor([-1])).to(device)  )\n",
    "        \n",
    "        n_scrs = torch.log(n_scrs)\n",
    "    \n",
    "        loss = - (torch.sum(scrs) + torch.sum(n_scrs)) / ((len(contexts) + len(negative)) / c)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (i % 2000 == 0 and i > 0):\n",
    "            if i > 0:\n",
    "                total_loss /= 2000\n",
    "                \n",
    "            if (i % 100000 == 0):\n",
    "                dump(model, pref + '{0:08d}'.format(i))\n",
    "#                 test(model)\n",
    "            \n",
    "            log.debug('Average loss at step %d: %.4f', i, total_loss)\n",
    "            total_loss = 0\n",
    "            losses.append(total_loss)\n",
    "      \n",
    "    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_model, tr_losses = train_skipgram()\n",
    "\n",
    "# dump(model, pref + \"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(\"24-combined/07000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(\"23-small-likes/00500000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_embed(model):\n",
    "    return model.in_embeddings(\n",
    "            Variable(\n",
    "                torch.LongTensor(\n",
    "                    list(range(len(groups)))\n",
    "                )\n",
    "            ).to(device)\n",
    "        ).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"data/categories_predict_dataset_v2.csv\", index_col = False)\n",
    "df = raw_df[raw_df.id.isin(w2i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Total groups in main dataset: \", len(groups), \" common groups in data sets:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(set(df.general)))\n",
    "print (len(set(df.detailed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cat_count = 200\n",
    "\n",
    "black_list = []\n",
    "\n",
    "top = 10\n",
    "\n",
    "cats = None\n",
    "\n",
    "if mode < 9:\n",
    "    cats = {\"g\":\n",
    "            [\"988a7c0d\", \"d0157bde\", \"46988013\", \"9d802769\", \"de91548c\", \n",
    "            \"7394d6a2\", \"1a4668e5\", \"80d061f1\", \"8eb49651\", \"1bd3ff12\"],\n",
    "            \"d\":\n",
    "            ['3f496854', '14c24f5a', '36e5c6fd', 'c9b867bc', '75671547', \n",
    "             '4262731e', 'ef14d17c', '58d06151', 'dee4c7aa', 'a4e3f544']\n",
    "           }\n",
    "\n",
    "if (mode == 9):\n",
    "    min_cat_count = 40\n",
    "#     Noisy\n",
    "    black_list = [\"39245b73\", \"1a4668e5\"]\n",
    "    top = 5\n",
    "\n",
    "if (mode == 10):\n",
    "    min_cat_count = 80\n",
    "    black_list = [\"39245b73\", \"1a4668e5\", \"1bd3ff12\", \"dee4c7aa\"]\n",
    "    top = 5\n",
    "    \n",
    "if (mode == 11):\n",
    "    min_cat_count = 0\n",
    "    black_list = [\"39245b73\", \"1a4668e5\", \"1bd3ff12\", \"dee4c7aa\"]\n",
    "    top = 5\n",
    "\n",
    "def fill_cat_set(t):\n",
    "    cat_set = set()\n",
    "    \n",
    "    cat_dict = dict()\n",
    "\n",
    "    if (t == \"g\"):\n",
    "        cat_lst = list(df.general)\n",
    "    else:\n",
    "        cat_lst = list(df.detailed)\n",
    "        \n",
    "    for c in cat_lst:\n",
    "        if c in cat_dict:\n",
    "            cat_dict[c] += 1\n",
    "        else:\n",
    "            cat_dict[c] = 1\n",
    "\n",
    "    for item in cat_dict.items():\n",
    "        if (item[1] >= min_cat_count and not item[0] in black_list):\n",
    "            cat_set.add(item[0])\n",
    "            \n",
    "    cat_set = list(cat_set)\n",
    "    \n",
    "    if (cats != None):\n",
    "        cat_set = cats[t]\n",
    "    \n",
    "    cat_set = sorted(cat_set, key = lambda x: -cat_dict[x])\n",
    "\n",
    "    cat_set = set(cat_set[:top])\n",
    "    \n",
    "    print(lfilter(lambda x: x[0] in cat_set, cat_dict.items()))\n",
    "    \n",
    "    return cat_set, cat_dict\n",
    "\n",
    "cat_set_g, cat_dict_g = fill_cat_set(\"g\")\n",
    "cat_set_d, cat_dict_d = fill_cat_set(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "\n",
    "def get_classifiers():\n",
    "    params = {'verbose': 0, 'n_estimators': 100}\n",
    "\n",
    "    gbc = GradientBoostingClassifier(**params)\n",
    "    \n",
    "    abc = AdaBoostClassifier(n_estimators = 100)\n",
    "    \n",
    "    s_clf = svm.LinearSVC()\n",
    "    \n",
    "    sgd = linear_model.SGDClassifier(max_iter = 1000, tol = 0.001)\n",
    "    \n",
    "    return [gbc, abc, s_clf, sgd]\n",
    "#     return [gbc]\n",
    "\n",
    "def f1_class(expected, predicted, classifier):\n",
    "    f1_micro = f1_score(expected, predicted, average = \"weighted\")\n",
    "    \n",
    "    f1_ = f1_score(expected, predicted, average = None)\n",
    "    \n",
    "    f1_ = list(zip(f1_, classifier.classes_))\n",
    "    \n",
    "    return {\"micro\": f1_micro, \"classes\": f1_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = 5\n",
    "\n",
    "def classify(all_embed, classifier, t = \"g\"):\n",
    "    cat_set = cat_set_g if t == \"g\" else cat_set_d\n",
    "    \n",
    "    cl_data = list(enumerate(all_embed))\n",
    "    \n",
    "    df_ids = list(df[\"id\"])\n",
    "\n",
    "    cl_data = lfilter(lambda x: i2w[x[0]] in df_ids, cl_data)\n",
    "\n",
    "    if (t == \"g\"):\n",
    "        cl_data = lmap(\n",
    "            lambda x: [x[0], x[1], list(df[df.id == i2w[x[0]]].general)[0]], \n",
    "            cl_data\n",
    "        )\n",
    "    else:\n",
    "        cl_data = lmap(\n",
    "            lambda x: [x[0], x[1], list(df[df.id == i2w[x[0]]].detailed)[0]], \n",
    "            cl_data\n",
    "        )\n",
    "    \n",
    "    cl_data = lfilter(lambda x: x[2] in cat_set, cl_data)\n",
    "    \n",
    "    cl_train, cl_test = train_test_split(cl_data)\n",
    "    \n",
    "    trained = classifier.fit(lmap(lambda x: x[1], cl_train), lmap(lambda x: x[2], cl_train))\n",
    "    \n",
    "    f1_res = f1_class(\n",
    "        lmap(lambda x: x[2], cl_test),\n",
    "        trained.predict(lmap(lambda x: x[1], cl_test)),\n",
    "        classifier\n",
    "    )\n",
    "    \n",
    "    return [trained, f1_res]\n",
    "\n",
    "def classify_model(model_name, classifier):\n",
    "    return classify(get_all_embed(load(model_name)), classifier)\n",
    "\n",
    "def to_df(score, t):\n",
    "    cat_dict = cat_dict_g if t == \"g\" else cat_dict_d\n",
    "       \n",
    "    df = pd.DataFrame({\n",
    "        \"f1_result\": lmap(lambda x: format(x[0], \".3f\"), score),\n",
    "        \"category\": lmap(lambda x: x[1], score),\n",
    "        \"frequency\": lmap(lambda x: cat_dict[x[1]], score)\n",
    "    })\n",
    "    \n",
    "    df = df.sort_values(by=[\"frequency\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def classify_all(embeds, t):\n",
    "    classifiers = get_classifiers()\n",
    "    \n",
    "    f1_scores = []\n",
    "    \n",
    "    for id, c in enumerate(classifiers):\n",
    "        micro = []\n",
    "        classes = []\n",
    "        \n",
    "        for i in range(times):\n",
    "            log.info(\"%d classifier, %d times\", id, i)\n",
    "            \n",
    "            __, f1 = classify(embeds, c, t)\n",
    "            \n",
    "            micro.append(f1[\"micro\"])\n",
    "                      \n",
    "            if (len(classes) == 0):\n",
    "                classes = f1[\"classes\"]\n",
    "            else:\n",
    "                for j in range(len(classes)):\n",
    "                    classes[j] = (classes[j][0] + f1[\"classes\"][j][0], classes[j][1])\n",
    "        \n",
    "        log.info(\"%d classifier is finished\", id)\n",
    "                \n",
    "        micro = sum(micro) / len(micro)\n",
    "        \n",
    "        classes = lmap(lambda x: (x[0] / times, x[1]), classes)\n",
    "                               \n",
    "        f1_scores.append((to_df(classes, t), micro))\n",
    "        \n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = classify_all(get_all_embed(model), \"g\")\n",
    "\n",
    "print(scores)\n",
    "\n",
    "scores = classify_all(get_all_embed(model), \"d\")\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = classify_all(get_all_embed(load(\"24-combined/02000000\")), \"g\")\n",
    "\n",
    "print(scores)\n",
    "\n",
    "scores = classify_all(get_all_embed(load(\"24-combined/02000000\")), \"d\")\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embeds(embed_dict):   \n",
    "    res = list(embed_dict.items())\n",
    "    \n",
    "    res = lmap(lambda x: (i2w[x[0]], x[1]), res)\n",
    "    \n",
    "    res = lmap(lambda x: (w2i[x[0]], x[1]), res)\n",
    "    \n",
    "    sorted(res, key = lambda x: x[0])\n",
    "    \n",
    "    res = lmap(lambda x: x[1], res)\n",
    "    \n",
    "    #return res\n",
    "    return lmap(lambda x: x / np.linalg.norm(x), res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_embed_file = glob.glob(\"scala/**/out/\" + str(mode) + \"/ALS_embeddings/*.json\")[0]\n",
    "\n",
    "als_embeds = []\n",
    "\n",
    "with open(als_embed_file) as json_file: \n",
    "    raw_als_embeds = dict()\n",
    "\n",
    "    embed_list = []\n",
    "\n",
    "    for single in json_file.readlines():\n",
    "        embed_list.append(json.loads(single))\n",
    "\n",
    "    for emb in embed_list:\n",
    "        raw_als_embeds[emb['id']] = emb['features']\n",
    "\n",
    "    als_embeds = map_embeds(raw_als_embeds)\n",
    "\n",
    "    assert len(als_embeds) == len(groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = classify_all(als_embeds, \"g\")\n",
    "\n",
    "print(scores)\n",
    "\n",
    "scores = classify_all(als_embeds, \"d\")\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_embed_file = glob.glob(\"scala/**/out/\" + str(mode) + \"/lda_embeddings/part-*\")[0]\n",
    "\n",
    "lda_embeds = []\n",
    "\n",
    "with open(lda_embed_file) as f: \n",
    "    raw_lda_embeds = dict()\n",
    "    \n",
    "    embed_list = []\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for single in f.readlines():\n",
    "        raw_lda_embeds[i] = lmap(lambda x: float(x),\n",
    "            lfilter(\n",
    "                lambda x: len(x), \n",
    "                re.split(\" \", single.rstrip())\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        i += 1\n",
    "            \n",
    "    lda_embeds = map_embeds(raw_lda_embeds)\n",
    "    \n",
    "    assert len(lda_embeds) == len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = classify_all(lda_embeds, \"g\")\n",
    "\n",
    "print(scores)\n",
    "\n",
    "scores = classify_all(lda_embeds, \"d\")\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(df):\n",
    "    df.to_csv(\"data/classification_result.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_embed_classify(t):\n",
    "    if (t == \"g\"):\n",
    "        generals = lmap(lambda x: list(df[df.id == i2w[x]].general), list(range(len(groups))))\n",
    "    else:\n",
    "        generals = lmap(lambda x: list(df[df.id == i2w[x]].detailed), list(range(len(groups))))\n",
    "    \n",
    "    cat_set = cat_set_g if t == \"g\" else cat_set_d\n",
    "    \n",
    "    missing = []\n",
    "    dexes = []\n",
    "\n",
    "    for i in range(len(generals)):\n",
    "        if (len(generals[i]) == 0 or not (generals[i][0] in cat_set)):\n",
    "            missing.append(i)\n",
    "        else:\n",
    "            dexes.append(i)\n",
    "\n",
    "    generals = lfilter(lambda x: len(x) != 0 and (x[0] in cat_set), generals)\n",
    "\n",
    "    generals = lmap(lambda x: x[0], generals)\n",
    "    \n",
    "    mdata = np.ndarray(shape=(len(generals), len(data)), dtype=np.int8)\n",
    "\n",
    "    for i in dexes:\n",
    "        for ev in data[i][0]:\n",
    "            g = w2i[ev]\n",
    "\n",
    "            mdata[g - len(lfilter(lambda x: x <= g, missing))][i] += 1\n",
    "\n",
    "    assert (len(generals) == len(mdata))\n",
    "        \n",
    "    f1_scores = []\n",
    "        \n",
    "    for id, c in enumerate(get_classifiers()):\n",
    "        micro = []\n",
    "        classes = []\n",
    "        \n",
    "        for i in range(times):\n",
    "            log.info(\"%d classifier, %d times\", id, i)\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(mdata, generals)\n",
    "\n",
    "            X_train = sparse.csr_matrix(X_train)\n",
    "\n",
    "            X_test = sparse.csr_matrix(X_test)\n",
    "\n",
    "            trained = c.fit(X_train, y_train)\n",
    "\n",
    "            pred = trained.predict(sparse.csr_matrix(X_test)) \n",
    "\n",
    "            f1 = f1_class(y_test, pred, trained)\n",
    "            \n",
    "            micro.append(f1[\"micro\"])\n",
    "                      \n",
    "            if (len(classes) == 0):\n",
    "                classes = f1[\"classes\"]\n",
    "            else:\n",
    "                for j in range(len(classes)):\n",
    "                    classes[j] = (classes[j][0] + f1[\"classes\"][j][0], classes[j][1])\n",
    "        \n",
    "        log.info(\"%d classifier is finished\", id)\n",
    "                \n",
    "        micro = sum(micro) / len(micro)\n",
    "        \n",
    "        classes = lmap(lambda x: (x[0] / times, x[1]), classes)\n",
    "                               \n",
    "        f1_scores.append((to_df(classes, t), micro))\n",
    "        \n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_embed_classify(\"g\"))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(no_embed_classify(\"d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_pred_data = lfilter(\n",
    "    lambda x: len(x) > 3 and x[len(x) - 1] not in set(x[:(len(x) - 2)]), \n",
    "    lmap(lambda x: list(x[0]), data)\n",
    ")\n",
    "\n",
    "random.shuffle(sess_pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_next_predict(embeds, test, top = 50):\n",
    "    emb_t = lmap(lambda x: Variable(torch.FloatTensor(x)), embeds)\n",
    "    \n",
    "    ans = []\n",
    "    \n",
    "    in_top = 0\n",
    "    \n",
    "    for id, single in enumerate(test):\n",
    "        if (id % 10 == 0):\n",
    "            log.info(\"Step %d\", id)\n",
    "    \n",
    "        actual = w2i[single[len(single) - 1]]\n",
    "        \n",
    "        sum_ = emb_t[w2i[single[0]]]\n",
    "\n",
    "        for i in range(1, len(single) - 1):\n",
    "            sum_ = sum_ + emb_t[w2i[single[i]]]\n",
    "            \n",
    "        similar = []\n",
    "                \n",
    "        for i in range(len(emb_t)):\n",
    "            similar.append((F.cosine_similarity(emb_t[i].view((1, -1)), sum_.view((1, -1))), i))\n",
    "            \n",
    "        similar = sorted(similar, key = lambda x: -x[0])\n",
    "       \n",
    "        for i in range(len(similar)):\n",
    "            if similar[i][1] == actual:\n",
    "                ans.append(i)\n",
    "                \n",
    "                if (i <= top):\n",
    "                    in_top += 1\n",
    "\n",
    "        assert (len(ans) == id + 1)\n",
    "   \n",
    "    return [sum(ans) / len(ans), in_top / len(ans)] \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_next_predict(als_embeds, sess_pred_data[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_next_predict(lda_embeds, sess_pred_data[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_next_predict(get_all_embed(model), sess_pred_data[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_t = lmap(lambda x: Variable(torch.FloatTensor(x)), lda_embeds)\n",
    "\n",
    "sum_ = emb_t[0]\n",
    "\n",
    "similar = []\n",
    "\n",
    "for i in range(len(emb_t)):\n",
    "    similar.append((F.cosine_similarity(emb_t[i].view((1, -1)), sum_.view((1, -1))), i))\n",
    "    \n",
    "similar = sorted(similar, key = lambda x: -x[0])\n",
    "\n",
    "print(get_info(lmap(lambda x: i2w[x[1]], similar[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "num_clusters = 32\n",
    "\n",
    "all_embed = get_all_embed(model)\n",
    "\n",
    "km = sklearn.cluster.KMeans(n_clusters = num_clusters)\n",
    "\n",
    "km.fit(all_embed)\n",
    "\n",
    "clusters = km.predict(all_embed)\n",
    "\n",
    "target = clusters[w2i[29534144]]\n",
    "\n",
    "res = []\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    if clusters[i] == target:\n",
    "        res.append(i2w[i])\n",
    "\n",
    "if (len(res) < 180):\n",
    "    print(get_info(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = clusters[w2i[12648877]]\n",
    "\n",
    "print (km.cluster_centers_[target])\n",
    "\n",
    "t = []\n",
    "        \n",
    "for j in range(vocab_size):\n",
    "    cent = Variable(torch.FloatTensor([km.cluster_centers_[target]])).to(device)\n",
    "    embed_ctx = model.in_embeddings(Variable(torch.LongTensor([j])).to(device)).view((1, -1))\n",
    "\n",
    "    score = F.cosine_similarity(cent, embed_ctx)\n",
    "    \n",
    "    t.append([score, i2w[j]])\n",
    "\n",
    "t.sort(key = lambda x: -x[0])\n",
    "\n",
    "ids = []\n",
    "res = t[:10]\n",
    "\n",
    "for k in res:\n",
    "    ids.append(k[1])\n",
    "\n",
    "info = get_info(ids)\n",
    "\n",
    "print(i)\n",
    "for i in range(10):\n",
    "    print(res[i], ' ', info[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(verbose=1)\n",
    "\n",
    "embed_2d = tsne.fit_transform(all_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = [-1.0 for i in range(embedding_size)]\n",
    "\n",
    "cls_index = [i for i in range(num_clusters)]\n",
    "\n",
    "cls_index.sort(key = functools.cmp_to_key(lambda x, y: F.cosine_similarity(\n",
    "                    torch.FloatTensor(km.cluster_centers_[x]).view((1, -1)), \n",
    "                    torch.FloatTensor(km.cluster_centers_[y]).view((1, -1))\n",
    "              )))\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (40, 20))\n",
    "\n",
    "cmap = plt.get_cmap(\"hsv\", num_clusters)\n",
    "\n",
    "sct = ax.scatter(\n",
    "    x = lmap(lambda x: x[0], embed_2d), \n",
    "    y = lmap(lambda x: x[1], embed_2d), \n",
    "    c = lmap(lambda x: cls_index.index(x), predicitons), \n",
    "    s = 70,\n",
    "    cmap = cmap,\n",
    "    alpha = 0.4\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show_ids = [138248199, 66322477, 157744447, 166036059, 146079944, 178824336, 130360681, 41523650]\n",
    "to_show_lables = [\"Казахстан\",  \"Школьные\", \"Подростковые (Д)\", \"Подростковые(M)\", \"Подростковые\", \"Игры/шутеры\", \"Ремонт\", \"K-pop\"]\n",
    "\n",
    "to_show = lmap(lambda x: clusters[w2i[x]], to_show_ids)\n",
    "to_show.insert(0, None)\n",
    "\n",
    "cmap = plt.get_cmap(\"jet\", len(to_show) - 1)\n",
    "\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "cmaplist = lmap(lambda x: (x[0], x[1], x[2], 0.7), cmaplist)\n",
    "cmaplist.insert(0, (.5, .5, .5, 0.1))\n",
    "\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list('Custom cmap1', cmaplist, len(cmaplist))\n",
    "\n",
    "clrs = lmap(lambda x: to_show.index(x) if x in to_show else 0, clusters)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (40, 20))\n",
    "\n",
    "sct = ax.scatter(\n",
    "    x = lmap(lambda x: x[0], embed_2d), \n",
    "    y = lmap(lambda x: x[1], embed_2d), \n",
    "    c = clrs, \n",
    "    s = 70,\n",
    "    cmap = cmap\n",
    ")\n",
    "\n",
    "hand_list = list(zip(to_show_lables, cmaplist[1:]))\n",
    "\n",
    "hand_list = lmap(lambda x: mpatches.Patch(color = x[1], label = x[0]), hand_list)\n",
    "\n",
    "plt.legend(handles = hand_list, prop={'size': 36})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_in(x_center, y_center, limit, with_data = False):\n",
    "    sc_data = zip(\n",
    "        lmap(lambda x: x[0], embed_2d), \n",
    "        lmap(lambda y: y[1], embed_2d), \n",
    "        lmap(lambda c: cls_index.index(c), clusters),\n",
    "        list(range(len(embed_2d)))\n",
    "    )\n",
    "      \n",
    "    sc_data = lfilter(lambda elem: abs(elem[0] - x_center) < limit \n",
    "                      and abs(elem[1] - y_center) < limit,\n",
    "                      sc_data)\n",
    "\n",
    "    if (len(sc_data) == 0):\n",
    "        print(\"No data\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (10, 5))\n",
    "\n",
    "    classes = lmap(lambda x: x[2], sc_data)\n",
    "    \n",
    "    enum = list(enumerate(set(classes)))\n",
    "    \n",
    "    class_remap = dict(lmap(lambda x:(x[1], x[0]), enum))\n",
    "    classes_map = dict(enum) \n",
    "    \n",
    "    classes = lmap(lambda x: class_remap[x], classes)\n",
    "    \n",
    "    cmap = plt.get_cmap(\"jet\", len(set(classes)))\n",
    "    \n",
    "    sct = ax.scatter(\n",
    "        x = lmap(lambda x: x[0], sc_data), \n",
    "        y = lmap(lambda x: x[1], sc_data), \n",
    "        c = classes, \n",
    "        s = 70,\n",
    "        cmap = cmap\n",
    "    )\n",
    "    \n",
    "    cb = plt.colorbar(sct, spacing = \"proportional\", ticks = np.linspace(0, len(classes), len(classes) + 1))\n",
    "\n",
    "    cb.set_alpha(1)\n",
    "    cb.draw_all()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if (with_data):\n",
    "        for c in class_remap.keys():\n",
    "            print(class_remap[c])\n",
    "            \n",
    "            sleep(0.5)\n",
    "            \n",
    "            info_print(\n",
    "                lmap(lambda x: i2w[x[3]], lfilter(lambda x: x[2] == c, sc_data))\n",
    "            )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = [-1.0 for i in range(embedding_size)]\n",
    "\n",
    "#cls_types = [None, \"9d802769\", \"e48d7610\"]\n",
    "#cls_lables = [None, \"Спорт\", \"Авто и мото\"]\n",
    "cls_types = [None, \"4262731e\", \"75265a2c\"]\n",
    "cls_lables = [None, \"Рецепты и еда\", \"Стиль, одежда, обувь\"]\n",
    "\n",
    "cls = list(range(len(embed_2d)))\n",
    "\n",
    "cls = lmap(lambda x: i2w[x], cls)\n",
    "cls = lmap(lambda x: list(df[df.id == x].detailed), cls)\n",
    "cls = lmap(lambda x: None if len(x) == 0 else x[0], cls)\n",
    "cls = lmap(lambda x: 0 if not x in cls_types else cls_types.index(x), cls)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (40, 20))\n",
    "\n",
    "cmap = plt.get_cmap(\"jet\", len(cls_types) - 1)\n",
    "\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "cmaplist = lmap(lambda x: (x[0], x[1], x[2], 0.7), cmaplist)\n",
    "cmaplist.insert(0, (.5, .5, .5, 0.1))\n",
    "\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, len(cmaplist))\n",
    "\n",
    "sct = ax.scatter(\n",
    "    x = lmap(lambda x: x[0], embed_2d), \n",
    "    y = lmap(lambda x: x[1], embed_2d), \n",
    "    c = cls, \n",
    "    s = 70,\n",
    "    cmap = cmap\n",
    ")\n",
    "\n",
    "hand_list = list(zip(cls_lables, cmaplist))\n",
    "\n",
    "hand_list = lmap(lambda x: mpatches.Patch(color = x[1], label = x[0]), hand_list)\n",
    "\n",
    "plt.legend(handles = hand_list[1:], prop={'size': 36})\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
