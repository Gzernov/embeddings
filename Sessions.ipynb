{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from six.moves import xrange \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "log = logging.getLogger('log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "lhnd = logging.StreamHandler()\n",
    "lhnd.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "lhnd.setFormatter(formatter)\n",
    "\n",
    "log.addHandler(lhnd)\n",
    "\n",
    "%autonotify -a 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_dumps = False\n",
    "\n",
    "def lmap(f, arr):\n",
    "    return list(map(f, arr))\n",
    "\n",
    "def lfilter(f, arr):\n",
    "    return list(filter(f, arr))\n",
    "\n",
    "def foreach(it, f):\n",
    "    for e in it:\n",
    "        f(e)\n",
    "        \n",
    "def dump(data, name):\n",
    "    with open('data/' + name, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load(name):\n",
    "    with open('data/' + name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def load_or_dump(path, func):\n",
    "    if not Path('data/' + path).exists() or ignore_dumps:\n",
    "        res = func()\n",
    "    \n",
    "        dump(res, path)\n",
    "    else:\n",
    "        res = load(path)\n",
    "        \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "with open('auth/token') as f:\n",
    "    token = f.readline().strip()\n",
    "\n",
    "def get_info(ids):\n",
    "    sleep(0.2)\n",
    "    mc = 'members_count'\n",
    "    payload = {'v': '5.92', 'access_token': token, 'fields':mc}\n",
    "    \n",
    "    str_ids = functools.reduce(\n",
    "        lambda x, y: x + y,\n",
    "        lmap(lambda x: str(x) + ',', ids)\n",
    "    )\n",
    "    \n",
    "    payload['group_ids'] = str_ids[0:- 1]\n",
    "    \n",
    "    r = requests.get('https://api.vk.com/method/groups.getById', \n",
    "                     params=payload)\n",
    "    \n",
    "    if (not 'response' in r.json()):\n",
    "        print(r.json())\n",
    "        \n",
    "    res = lmap(lambda x: (x['name'], x['screen_name'], \"{:,}\".format(x[mc]) if mc in x else -1),r.json()['response'])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 947528\n",
    "\n",
    "def raw_data_filter(file):\n",
    "    # Mapping to events\n",
    "    res = list()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for line in file:\n",
    "        cur = line.rstrip().split(',')\n",
    "        cur = lmap(lambda p: (re.sub(';.*', '', p), re.sub('.*;', '', p)), cur)\n",
    "\n",
    "        session = list()\n",
    "        \n",
    "        for j in range(0, len(cur)):\n",
    "            try:\n",
    "                session.append(int(cur[j][1]))\n",
    "            except ValueError:\n",
    "                None\n",
    "                \n",
    "        res.append(session)\n",
    "\n",
    "        i = i + 1\n",
    "                \n",
    "        if (i % 100000 == 0):\n",
    "            gc.collect()\n",
    "\n",
    "            log.debug(\"%d %% of mapping is done.\", i / total * 100)\n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-18 10:38:48,502 INFO Data loaded\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_or_dump('raw', lambda: raw_data_filter(open(\"data/sessions_public.txt\",\"r\")))\n",
    "\n",
    "log.info(\"Data loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_count(data):\n",
    "    total = dict()\n",
    "\n",
    "    for i in data:\n",
    "        for j in i[0]:\n",
    "            if (j in total.keys()):\n",
    "                total[j] = total[j] + 1\n",
    "            else:\n",
    "                total[j] = 1\n",
    "                \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_session_size = 5\n",
    "\n",
    "def initiail_mapping(lst, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set()\n",
    "        malformed = set()\n",
    "        \n",
    "        for event in session:\n",
    "            if (event < 0):\n",
    "                sub_event = -event\n",
    "                \n",
    "                if (sub_event in sub or sub_event in malformed):\n",
    "                    sub.discard(sub_event)\n",
    "                    unsub.discard(sub_event)\n",
    "                    malformed.add(sub_event)\n",
    "                else:\n",
    "                    unsub.add(sub_event)\n",
    "            else:\n",
    "                if (event in unsub or event in malformed):\n",
    "                    unsub.discard(event)\n",
    "                    sub.discard(event)\n",
    "                    malformed.add(event)\n",
    "                else:\n",
    "                    sub.add(event)\n",
    "        \n",
    "        if (len(sub) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "    \n",
    "\n",
    "def set_map(lst, cnt, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set() \n",
    "        \n",
    "        for event in session[0]:\n",
    "            if (cnt[event] > min_allowed):\n",
    "                sub.add(event)\n",
    "                \n",
    "        for event in session[1]:\n",
    "            if (cnt.get(event, -1) > min_allowed):\n",
    "                unsub.add(event)    \n",
    "        \n",
    "        if (len(sub) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def drop_uncommon(raw_data, min_allowed = 10):\n",
    "    cnt = None\n",
    "    sorted_cnt = None\n",
    "    \n",
    "    data, groups = initiail_mapping(raw_data, min_allowed)\n",
    "    cnt = group_count(data) \n",
    "    sorted_cnt = sorted(list(cnt.values()))\n",
    "    \n",
    "    while (cnt == None or sorted_cnt[0] < min_allowed):\n",
    "        data, groups = set_map(data, cnt, min_allowed)\n",
    "                \n",
    "        cnt = group_count(data) \n",
    "        sorted_cnt = sorted(list(cnt.values()))\n",
    "        \n",
    "        log.info(\"Length of data:   %d\", len(data))\n",
    "        log.info(\"Total length:     %d\", \n",
    "                functools.reduce((lambda x, y: x + y), lmap(lambda a: len(a), data))\n",
    "                )\n",
    "        log.info(\"Number of groups: %d\", len(groups))\n",
    "        log.info(\"Minimum count:    %d\\n\", sorted_cnt[0])\n",
    "        \n",
    "    return data, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ignore_dumps = False\n",
    "data, groups = load_or_dump('final_data', lambda: drop_uncommon(raw_data, 50))\n",
    "\n",
    "most_common = sorted(group_count(data).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "w2i = {w: i for i, w in enumerate(groups)}\n",
    "i2w = {i: w for i, w in enumerate(groups)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = None\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dex = 0\n",
    "event_dex = 0\n",
    "\n",
    "def generate_batch(batch_size, negative_size, window_size = 1):\n",
    "    assert min_session_size >= window_size * 2 + 1 \n",
    "    assert batch_size % (window_size * 2) == 0\n",
    "    \n",
    "    global session_dex\n",
    "    global event_dex\n",
    "    \n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    negative = np.ndarray(shape=(negative_size), dtype=np.int32)\n",
    "    \n",
    "    if (event_dex == 0):\n",
    "        event_dex = window_size\n",
    "    \n",
    "    current = 0\n",
    "    session = list(data[session_dex][0])\n",
    "    \n",
    "    while (current < batch_size):\n",
    "        i = 0\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if (j != 0):\n",
    "                labels[current + i][0] = w2i[session[event_dex + j]]\n",
    "                batch[current + i] = w2i[session[event_dex]]\n",
    "                i += 1\n",
    "\n",
    "        event_dex += 1\n",
    "        current += window_size * 2\n",
    "\n",
    "        if (event_dex + window_size >= len(session)):\n",
    "            event_dex = window_size\n",
    "            session_dex = session_dex + 1\n",
    "            if (session_dex >= len(data)):\n",
    "                session_dex = 0\n",
    "            session = list(data[session_dex][0])\n",
    "            \n",
    "    neg = 0\n",
    "        \n",
    "    for i in data[session_dex][1]:\n",
    "        negative[neg] = w2i[i]\n",
    "        neg += 1\n",
    "        if (neg == negative_size):\n",
    "            break\n",
    "            \n",
    "    rand_neg = np.random.randint(len(groups), size=negative_size - neg)\n",
    "        \n",
    "    for i in range(0, negative_size - neg):\n",
    "        negative[neg + i] = rand_neg[i]\n",
    "        \n",
    "     \n",
    "    return batch, labels, negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({23372133, 1959, 20650061, 70034991, 22741624, 35540891, 75909948}, set())\n",
      "({25794755, 91683885, 34812270, 39325103, 49128190, 34523318, 42533142, 46755517, 104237982}, set())\n",
      "26468 20650061 -> 15329 23372133\n",
      "26468 20650061 -> 745 1959\n",
      "26468 20650061 -> 15791 70034991\n",
      "26468 20650061 -> 24456 22741624\n",
      "15791 70034991 -> 745 1959\n",
      "15791 70034991 -> 26468 20650061\n",
      "15791 70034991 -> 24456 22741624\n",
      "15791 70034991 -> 7531 35540891\n",
      "24456 22741624 -> 26468 20650061\n",
      "24456 22741624 -> 15791 70034991\n",
      "[19650 26897 16391  8306 36492]\n"
     ]
    }
   ],
   "source": [
    "batch, labels, negative = generate_batch(16, 5, 2)\n",
    "\n",
    "print(data[0])\n",
    "print(data[1])\n",
    "\n",
    "for i in range(10):\n",
    "    print(batch[i], i2w[batch[i]], '->', labels[i, 0],\n",
    "          i2w[labels[i, 0]])\n",
    "    \n",
    "print(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data = None\n",
    "\n",
    "learning_rate = 0.5\n",
    "vocabulary_size = len(groups)\n",
    "\n",
    "window_size = 1\n",
    "embedding_size = 48\n",
    "num_sampled = 5\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(emb, index, f = None):\n",
    "    p = emb[index]\n",
    "    cnst = tf.constant(p, shape=[1, embedding_size])\n",
    "    d = tf.matmul(cnst, emb, transpose_b=True).eval()[0]\n",
    "\n",
    "    dxs = np.argsort(np.array(d))\n",
    "    \n",
    "    ids = []\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(dxs) - 10, len(dxs)):\n",
    "        ids.append(i2w[dxs[i]])\n",
    "        res.append(d[dxs[i]])\n",
    "        \n",
    "    info = get_info(ids)\n",
    "    \n",
    "    for i in xrange(len(res)):\n",
    "        print(ids[i], ' ', res[i], ' ', info[i])\n",
    "        \n",
    "        if (f != None):\n",
    "            f.write(str(ids[i]) + ' ' + str(res[i]) + ' ' + str(info[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 200000\n",
    "\n",
    "def mk_negative_samples(train_labels, num_sampled, negative_labels):\n",
    "    tmp = tf.random.log_uniform_candidate_sampler(\n",
    "                            true_classes=train_labels,\n",
    "                            num_true=1,\n",
    "                            num_sampled=num_sampled,\n",
    "                            unique=True,\n",
    "                            range_max=num_sampled,\n",
    "                            seed=None,\n",
    "                            name=None\n",
    "                        )\n",
    "\n",
    "    return (tf.map_fn(lambda x: negative_labels[x], tmp.sampled_candidates), \n",
    "                             tmp.true_expected_count, \n",
    "                             tmp.sampled_expected_count)\n",
    "\n",
    "def loss_function(nce_weights, nce_biases, train_labels, negative_labels, embed, num_sampled, vocabulary_size):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights=nce_weights,\n",
    "                        biases=nce_biases,\n",
    "                        labels=train_labels,\n",
    "                        inputs=embed,\n",
    "                        num_sampled=num_sampled,\n",
    "                        num_classes=vocabulary_size,\n",
    "                        num_true=1,\n",
    "                        sampled_values=mk_negative_samples(train_labels, num_sampled, negative_labels),\n",
    "                        remove_accidental_hits=True,\n",
    "                        partition_strategy='mod',\n",
    "                        name='sampled_softmax_loss',\n",
    "                        seed=None)\n",
    "\n",
    "\n",
    "def tf_train(window_size, embedding_size, num_sampled, batch_size):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        # Input data.\n",
    "        with tf.name_scope('inputs'):\n",
    "            train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "            negative_samples = tf.placeholder(tf.int64, shape=[num_sampled])\n",
    "            train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])\n",
    "\n",
    "        # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "        with tf.device('/cpu:0'):\n",
    "            # Look up embeddings for inputs.\n",
    "            with tf.name_scope('embeddings'):\n",
    "                embeddings = tf.Variable(\n",
    "                    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "                )\n",
    "                embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "          # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.reduce_mean(loss_function(nce_weights, \n",
    "                                 nce_biases, \n",
    "                                 train_labels, \n",
    "                                 negative_samples, \n",
    "                                 embed, \n",
    "                                 num_sampled, \n",
    "                                 vocabulary_size\n",
    "                                ))\n",
    "\n",
    "            # Add the loss value as a scalar to summary.\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        with tf.name_scope('optimizer'):\n",
    "              optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Compute the cosine similarity between minibatch examples and all\n",
    "        # embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        #valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "        #                                          valid_dataset)\n",
    "        #similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        # Merge all summaries.\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Create a saver.\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    with tf.Session(graph=graph) as session:     \n",
    "        \n",
    "        # Open a writer to write summaries.\n",
    "        writer = tf.summary.FileWriter(\"tmp\", session.graph)\n",
    "\n",
    "        # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        log.info('Initialized. Embedding size: %s; Num sampled: %s; Window size: %s; Batch size: %s', embedding_size, num_sampled, window_size, batch_size)\n",
    "        average_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "            batch_inputs, batch_labels, batch_negative = generate_batch(batch_size, num_sampled, window_size)\n",
    "            feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels, negative_samples:batch_negative}\n",
    "            \n",
    "            # Define metadata variable.\n",
    "            run_metadata = tf.RunMetadata()\n",
    "\n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "            # Also, evaluate the merged op to get all summaries from the returned\n",
    "            # \"summary\" variable. Feed metadata variable to session for visualizing\n",
    "            # the graph in TensorBoard.\n",
    "            _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                             feed_dict=feed_dict,\n",
    "                                             run_metadata=run_metadata)     \n",
    "            average_loss += loss_val\n",
    "\n",
    "            # Add returned summaries to writer in each step.\n",
    "            writer.add_summary(summary, step)\n",
    "            # Add metadata to visualize the graph for the last run.\n",
    "            if step == (num_steps - 1):\n",
    "                writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                      average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000\n",
    "                # batches.\n",
    "                log.debug('Average loss at step %d: %.4f', step, average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            #if step % 20000 == 0 and step != 0:\n",
    "                #print('Most closest to ', most_common[0][0])\n",
    "                #get_closest(normalized_embeddings.eval(), w2i[most_common[0][0]])\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    return graph, final_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-18 18:12:56,152 INFO Initialized. Embedding size: 48; Num sampled: 5; Window size: 1; Batch size: 4\n",
      "2019-03-18 18:12:56,208 DEBUG Average loss at step 0: 0.0039\n",
      "2019-03-18 18:12:57,685 DEBUG Average loss at step 2000: 0.0087\n",
      "2019-03-18 18:12:59,207 DEBUG Average loss at step 4000: 0.0080\n",
      "2019-03-18 18:13:00,761 DEBUG Average loss at step 6000: 0.0081\n",
      "2019-03-18 18:13:02,229 DEBUG Average loss at step 8000: 0.0084\n",
      "2019-03-18 18:13:03,753 DEBUG Average loss at step 10000: 0.0075\n",
      "2019-03-18 18:13:05,256 DEBUG Average loss at step 12000: 0.0081\n",
      "2019-03-18 18:13:06,718 DEBUG Average loss at step 14000: 0.0075\n",
      "2019-03-18 18:13:08,279 DEBUG Average loss at step 16000: 0.0072\n",
      "2019-03-18 18:13:09,741 DEBUG Average loss at step 18000: 0.0088\n",
      "2019-03-18 18:13:11,191 DEBUG Average loss at step 20000: 0.0080\n",
      "2019-03-18 18:13:12,648 DEBUG Average loss at step 22000: 0.0079\n",
      "2019-03-18 18:13:14,091 DEBUG Average loss at step 24000: 0.0081\n",
      "2019-03-18 18:13:15,548 DEBUG Average loss at step 26000: 0.0072\n",
      "2019-03-18 18:13:17,128 DEBUG Average loss at step 28000: 0.0077\n",
      "2019-03-18 18:13:18,767 DEBUG Average loss at step 30000: 0.0076\n",
      "2019-03-18 18:13:20,322 DEBUG Average loss at step 32000: 0.0074\n",
      "2019-03-18 18:13:21,785 DEBUG Average loss at step 34000: 0.0078\n",
      "2019-03-18 18:13:23,229 DEBUG Average loss at step 36000: 0.0075\n",
      "2019-03-18 18:13:24,677 DEBUG Average loss at step 38000: 0.0071\n",
      "2019-03-18 18:13:26,123 DEBUG Average loss at step 40000: 0.0066\n",
      "2019-03-18 18:13:27,573 DEBUG Average loss at step 42000: 0.0073\n",
      "2019-03-18 18:13:29,030 DEBUG Average loss at step 44000: 0.0073\n",
      "2019-03-18 18:13:30,477 DEBUG Average loss at step 46000: 0.0072\n",
      "2019-03-18 18:13:32,061 DEBUG Average loss at step 48000: 0.0072\n",
      "2019-03-18 18:13:33,696 DEBUG Average loss at step 50000: 0.0069\n",
      "2019-03-18 18:13:35,152 DEBUG Average loss at step 52000: 0.0068\n",
      "2019-03-18 18:13:36,698 DEBUG Average loss at step 54000: 0.0070\n",
      "2019-03-18 18:13:38,244 DEBUG Average loss at step 56000: 0.0072\n",
      "2019-03-18 18:13:39,864 DEBUG Average loss at step 58000: 0.0068\n",
      "2019-03-18 18:13:41,615 DEBUG Average loss at step 60000: 0.0069\n",
      "2019-03-18 18:13:43,215 DEBUG Average loss at step 62000: 0.0071\n",
      "2019-03-18 18:13:44,726 DEBUG Average loss at step 64000: 0.0076\n",
      "2019-03-18 18:13:46,175 DEBUG Average loss at step 66000: 0.0070\n",
      "2019-03-18 18:13:47,623 DEBUG Average loss at step 68000: 0.0072\n",
      "2019-03-18 18:13:49,063 DEBUG Average loss at step 70000: 0.0075\n",
      "2019-03-18 18:13:50,515 DEBUG Average loss at step 72000: 0.0078\n",
      "2019-03-18 18:13:51,978 DEBUG Average loss at step 74000: 0.0067\n",
      "2019-03-18 18:13:53,432 DEBUG Average loss at step 76000: 0.0071\n",
      "2019-03-18 18:13:54,886 DEBUG Average loss at step 78000: 0.0072\n",
      "2019-03-18 18:13:56,356 DEBUG Average loss at step 80000: 0.0066\n",
      "2019-03-18 18:13:57,803 DEBUG Average loss at step 82000: 0.0067\n",
      "2019-03-18 18:13:59,286 DEBUG Average loss at step 84000: 0.0066\n",
      "2019-03-18 18:14:00,751 DEBUG Average loss at step 86000: 0.0070\n",
      "2019-03-18 18:14:02,222 DEBUG Average loss at step 88000: 0.0073\n",
      "2019-03-18 18:14:03,675 DEBUG Average loss at step 90000: 0.0063\n",
      "2019-03-18 18:14:05,133 DEBUG Average loss at step 92000: 0.0064\n",
      "2019-03-18 18:14:06,604 DEBUG Average loss at step 94000: 0.0066\n",
      "2019-03-18 18:14:08,062 DEBUG Average loss at step 96000: 0.0062\n",
      "2019-03-18 18:14:09,522 DEBUG Average loss at step 98000: 0.0062\n",
      "2019-03-18 18:14:10,982 DEBUG Average loss at step 100000: 0.0064\n",
      "2019-03-18 18:14:12,450 DEBUG Average loss at step 102000: 0.0071\n",
      "2019-03-18 18:14:13,909 DEBUG Average loss at step 104000: 0.0067\n",
      "2019-03-18 18:14:15,380 DEBUG Average loss at step 106000: 0.0069\n",
      "2019-03-18 18:14:16,838 DEBUG Average loss at step 108000: 0.0071\n",
      "2019-03-18 18:14:18,305 DEBUG Average loss at step 110000: 0.0063\n",
      "2019-03-18 18:14:19,846 DEBUG Average loss at step 112000: 0.0066\n",
      "2019-03-18 18:14:21,372 DEBUG Average loss at step 114000: 0.0061\n",
      "2019-03-18 18:14:22,851 DEBUG Average loss at step 116000: 0.0071\n",
      "2019-03-18 18:14:24,307 DEBUG Average loss at step 118000: 0.0058\n",
      "2019-03-18 18:14:25,774 DEBUG Average loss at step 120000: 0.0064\n",
      "2019-03-18 18:14:27,231 DEBUG Average loss at step 122000: 0.0068\n",
      "2019-03-18 18:14:28,727 DEBUG Average loss at step 124000: 0.0068\n",
      "2019-03-18 18:14:30,182 DEBUG Average loss at step 126000: 0.0065\n",
      "2019-03-18 18:14:31,728 DEBUG Average loss at step 128000: 0.0066\n",
      "2019-03-18 18:14:33,341 DEBUG Average loss at step 130000: 0.0070\n",
      "2019-03-18 18:14:34,992 DEBUG Average loss at step 132000: 0.0068\n",
      "2019-03-18 18:14:36,762 DEBUG Average loss at step 134000: 0.0059\n",
      "2019-03-18 18:14:38,313 DEBUG Average loss at step 136000: 0.0067\n",
      "2019-03-18 18:14:39,923 DEBUG Average loss at step 138000: 0.0063\n",
      "2019-03-18 18:14:41,463 DEBUG Average loss at step 140000: 0.0060\n",
      "2019-03-18 18:14:43,059 DEBUG Average loss at step 142000: 0.0079\n",
      "2019-03-18 18:14:44,576 DEBUG Average loss at step 144000: 0.0063\n",
      "2019-03-18 18:14:46,288 DEBUG Average loss at step 146000: 0.0063\n",
      "2019-03-18 18:14:48,092 DEBUG Average loss at step 148000: 0.0071\n",
      "2019-03-18 18:14:50,345 DEBUG Average loss at step 150000: 0.0074\n",
      "2019-03-18 18:14:53,471 DEBUG Average loss at step 152000: 0.0061\n",
      "2019-03-18 18:14:56,943 DEBUG Average loss at step 154000: 0.0063\n",
      "2019-03-18 18:14:59,872 DEBUG Average loss at step 156000: 0.0059\n",
      "2019-03-18 18:15:04,644 DEBUG Average loss at step 158000: 0.0062\n",
      "2019-03-18 18:15:08,776 DEBUG Average loss at step 160000: 0.0063\n",
      "2019-03-18 18:15:13,064 DEBUG Average loss at step 162000: 0.0058\n",
      "2019-03-18 18:15:17,841 DEBUG Average loss at step 164000: 0.0065\n",
      "2019-03-18 18:15:23,734 DEBUG Average loss at step 166000: 0.0060\n",
      "2019-03-18 18:15:28,081 DEBUG Average loss at step 168000: 0.0060\n",
      "2019-03-18 18:15:32,734 DEBUG Average loss at step 170000: 0.0056\n",
      "2019-03-18 18:15:38,391 DEBUG Average loss at step 172000: 0.0063\n",
      "2019-03-18 18:15:44,989 DEBUG Average loss at step 174000: 0.0062\n",
      "2019-03-18 18:15:49,778 DEBUG Average loss at step 176000: 0.0062\n",
      "2019-03-18 18:15:52,925 DEBUG Average loss at step 178000: 0.0064\n",
      "2019-03-18 18:15:55,149 DEBUG Average loss at step 180000: 0.0060\n",
      "2019-03-18 18:15:56,706 DEBUG Average loss at step 182000: 0.0059\n",
      "2019-03-18 18:15:58,234 DEBUG Average loss at step 184000: 0.0058\n",
      "2019-03-18 18:15:59,777 DEBUG Average loss at step 186000: 0.0061\n",
      "2019-03-18 18:16:01,307 DEBUG Average loss at step 188000: 0.0063\n",
      "2019-03-18 18:16:02,831 DEBUG Average loss at step 190000: 0.0057\n",
      "2019-03-18 18:16:04,343 DEBUG Average loss at step 192000: 0.0066\n",
      "2019-03-18 18:16:05,879 DEBUG Average loss at step 194000: 0.0060\n",
      "2019-03-18 18:16:07,392 DEBUG Average loss at step 196000: 0.0060\n",
      "2019-03-18 18:16:08,891 DEBUG Average loss at step 198000: 0.0058\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"5c0574d8-04b8-49b5-aa46-c2facb3f1b7d\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"5c0574d8-04b8-49b5-aa46-c2facb3f1b7d\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": false};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph, final_embeddings = tf_train(window_size, embedding_size, num_sampled, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def print_random(graph, final_embeddings, window_size, embedding_size, num_sampled):\n",
    "    try:\n",
    "        p = 'data/reports/' + 'es_' + str(embedding_size) + '_ns_' + str(num_sampled) + '_ws_' + str(window_size)\n",
    "        with open(p, 'w') as f:\n",
    "            for i in range(0, 10):\n",
    "                with tf.Session(graph=graph) as session:\n",
    "                    index = len(groups) * i // 10 + randint(0, 100)\n",
    "\n",
    "                    f.write(str(index) + '\\n')\n",
    "                    print(index)\n",
    "\n",
    "                    get_closest(final_embeddings, w2i[most_common[index][0]], f)\n",
    "\n",
    "                f.write('\\n\\n')\n",
    "                print('\\n')\n",
    "    except err:\n",
    "        print(err)\n",
    "        None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "105503338   0.49311572   ('–°–ø–æ—Ä—Ç–ù—è—à–µ—á–∫–∞', 'club105503338', '72,334')\n",
      "132782438   0.49472433   ('–ë–æ–µ–≤—ã–µ –ñ–∏–≥—É–ª–∏', 'boevaja_classica', '517,595')\n",
      "86526388   0.49519143   ('–†–∞–±–æ—Ç–∞ –ê—Å—Ç–∞–Ω–∞', 'job_astana', '33,609')\n",
      "145233167   0.4958241   ('BTS', 'lovvekorean', '134,595')\n",
      "94404133   0.5004687   ('–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ß–µ–ª—è–±–∏–Ω—Å–∫', 'ch_free', '31,827')\n",
      "17154489   0.50466454   ('–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π | –°—Ç–∞—Ç—É—Å—ã ‚ôÇ‚ôÄ ‚ô•  by Love ‚ô•', 'psiholog_otnosheniy', '77,035')\n",
      "115954385   0.50707656   ('–û–¥–µ—Ç–∞—è –≤ –°—á–∞—Å—Ç—å–µ. –ü—Å–∏—Ö–æ–ª–æ–≥–∏—è/–≠–∑–æ—Ç–µ—Ä–∏–∫–∞', 'club115954385', '18,484')\n",
      "34084756   0.5082171   ('PRO FOTO', 'profotopro', '137,378')\n",
      "70181160   0.5413219   ('Hearthstone —Å –•–∞–ø–ø–æ–π', 'clubhappa', '37,261')\n",
      "40567146   0.9999999   ('–õ–∞–π—Ñ—Ö–∞–∫', 'lhack', '7,944,169')\n",
      "\n",
      "\n",
      "4946\n",
      "31955706   0.4758619   ('ÿßŸÑÿ≥ŸÉŸàÿ™ ŸÖŸÜ ÿ∞Ÿáÿ®', 'alishaisakova', '139,285')\n",
      "139355889   0.47945952   ('–ë–µ—Å–ø–ª–∞—Ç–Ω–æ –∑–∞ —Ä–µ–ø–æ—Å—Ç | –ö–æ–Ω–∫—É—Ä—Å—ã', 'freefor_repost', -1)\n",
      "145491320   0.48079926   ('Full HD', 'full_hd2', '23,488')\n",
      "166991867   0.50103796   ('you disappoint me', 'club166991867', '40,896')\n",
      "31162282   0.5043559   ('Ancord - –ø–∞–±–ª–∏–∫ —Ñ–∞–Ω–∞—Ç–æ–≤ –æ–∑–≤—É—á–∫–∏', 'ancordpage', '107,852')\n",
      "68530208   0.5089486   ('–†—É—Å—Å–∫–æ–µ , –ß–∞—Å—Ç–Ω–æ–µ –∏ –î–æ–º–∞—à–Ω–µ–µ –ü–û–†–ù–û,–ú–ò–ù–ï–¢,–ê–ù–ê–õ!', 'club68530208', '16,844')\n",
      "118842368   0.5244212   ('–°—Ç–∞—Ä–∏–∫ –ü–æ—Ö–∞–±—ã—á', 'club118842368', '33,041')\n",
      "69222461   0.53148985   ('–°–õ–ê–í–Ø–ù–°–ö–ò–ô –ú–ò–†', 'slavyanemir', '76,237')\n",
      "37207068   0.5996443   ('–î–Ω–µ–≤–Ω–∏–∫ –°–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –î–µ–≤—É—à–∫–∏ . –°—Ç–∞—Ç—É—Å—ã', 'women.manner', '901,329')\n",
      "149878785   1.0   ('—á–µ—Ä–Ω–æ–µ –∏ –±–µ–ª–æ–µ', 'cocky_dove', '534,658')\n",
      "\n",
      "\n",
      "9765\n",
      "32657308   0.50565886   ('–¢–ò–ü–ò–ß–ù–ê–Ø –î–ï–í–£–®–ö–ê', 'iokean', '670,522')\n",
      "100867898   0.5109795   ('K.O.Y', 'k_o_y_yki', '40,348')\n",
      "158003111   0.5145847   ('–ñ–∏–≤–æ—Ç–Ω—ã–µ - –≤ —é–º–æ—Ä–µ !', 'jivotnie_v_umore', '22,203')\n",
      "134488897   0.5228721   ('–°—Ç–∞–Ω—å –£—Å–ø–µ—à–Ω—ã–º | –ü—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ —Å–ø–æ—Ä—Ç', 'rezerv_dm', '57,306')\n",
      "140728995   0.5242412   ('–ù–∏–∫—Ç–æ —Ç–µ–±—è –Ω–µ —Å–ª—ã—à–∏—Ç ¬©', 'no_one_will_hear_you', '50,027')\n",
      "91055713   0.53019655   ('–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏ –Ω–µ–æ–±—ã—á–Ω—ã–µ –ø–æ–¥–∞—Ä–∫–∏', 'originalitems', '36,004')\n",
      "125449777   0.5327132   ('–û–¥–µ–∂–¥–∞ –∏ –æ–±—É–≤—å –î–æ–Ω–µ—Ü–∫-–î–ù–†', 'red_wall_dnr', '25,355')\n",
      "129806371   0.53573334   ('–†–∞–±–æ—Ç–∞ –≤ –ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫–µ', 'club129806371', '33,316')\n",
      "73948343   0.5582477   ('–°–∏–ª–∞ –º—ã—Å–ª–∏', 'publicfeelthepower', '386,016')\n",
      "86530505   0.9999999   ('–ü–∏—Ñ –ø–∞—Ñ.', 'pifpaf004', '147,571')\n",
      "\n",
      "\n",
      "14568\n",
      "127960208   0.49379316   ('GTA –†–û–°–°–ò–Ø | RPBOX (–†–ü–ë–û–ö–°)', 'rpbox', '90,176')\n",
      "22421319   0.49752715   ('–ü–æ–º–æ—â—å –≤ –∫–æ–Ω–∫—É—Ä—Å–µ|–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–∏', '1helpme', -1)\n",
      "148589939   0.5010873   ('W O R L D', 'btu_world', '106,820')\n",
      "104518035   0.501324   ('–ë–ª–æ–≥ –ê—Ä—Ç–µ–º–∞ –î–æ—Ä—É—á–µ–Ω–∫–æ', 'artemdoruchenko', '56,840')\n",
      "36422332   0.5035672   ('–§–∏–ª—å–º—ã –∏ –°–µ—Ä–∏–∞–ª—ã 2018 [–§–∏–ª—å–º –í—Å–µ–º]', 'filmvsem', '175,668')\n",
      "62536814   0.5057448   ('Franceüá´üá∑', 'france_777', '53,293')\n",
      "107110988   0.50738597   ('–ë–û–õ–¨–®–ê–Ø –ì–†–£–î–¨ –¢–û–õ–°–¢–£–®–ö–ò', 'big_fatty', '35,863')\n",
      "22192347   0.5144558   ('The Elder Scrolls', 'the_elder_council', '129,409')\n",
      "46729094   0.5194784   ('–£—á–µ–±–Ω–∏–∫ —Ä–µ–º–æ–Ω—Ç–∞ | –°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', 'uchebnik_remonta', '259,108')\n",
      "40766972   0.99999994   ('–°–∫–æ—Ä–æ –≤ –ü–∏—Ç–µ—Ä–µ', 'piter_tomorrow', '186,533')\n",
      "\n",
      "\n",
      "19391\n",
      "23141788   0.50376415   ('–ò–¥–µ–∞–ª—å–Ω–∞—è. –î–∏–µ—Ç—ã –∏ —Å–ø–æ—Ä—Ç', 'befitness', '1,703')\n",
      "106402320   0.51689214   ('–í–Ω–µ–∑–∞–ø–Ω–æ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', 'puk_punk', '34,841')\n",
      "130865560   0.5170442   (\"‚òÖMen's Car & Girl look‚òÖ\", 'cars_look', -1)\n",
      "107395916   0.5270181   ('–ö–∏–µ–≤ | –ó–Ω–∞–∫–æ–º—Å—Ç–≤–∞ –£–∫—Ä–∞–∏–Ω–∞ | –ó–Ω–∞–π–æ–º—Å—Ç–≤–∞ –£–∫—Ä–∞—ó–Ω–∞', 'love_ukrenian', '26,266')\n",
      "11062633   0.54143   ('–ü–µ—Å–Ω–∏ | –õ–∏—Ä–∏–∫–∞', 'lirikslov', '103,411')\n",
      "66523083   0.55978686   ('–®–ö–û–õ–ê –°–ú–ï–•–ê „ÉÑ', 'shk_smex', '843,391')\n",
      "78600075   0.564777   ('–ò–ó–ù–ê–ù–ö–ê –§–ò–¢–ù–ï–°–ê', 'fitnessnaiznanky', '60,609')\n",
      "6992713   0.5767462   (\"–ú—É–∂—Å–∫–æ–π –≤–∑–≥–ª—è–¥ l MAN'S LOOK\", 'manss_look', '35,465')\n",
      "134748687   0.5947615   ('NeProstoGamer', 'neprostogamer', '22,482')\n",
      "124421250   0.9999999   ('–ú–Ω–µ –ø–æ–∫–∞–∑–∞–ª–æ—Å—å | –ó–∞ —Å–µ–∫—É–Ω–¥—É –¥–æ..', 'zasekundu_pokazalos', '52,542')\n",
      "\n",
      "\n",
      "24245\n",
      "60861374   0.49422085   ('–ö—Ä–æ–≤–∞—Ç–∏-–ú–ê–®–ò–ù–´ (–º–µ–±–µ–ª—å –∫—Ä–æ–≤–∞—Ç—å –º–∞—à–∏–Ω–∞ ) –ú–ê–¢–†–ê–°–´', 'crovatmashina35', '37,929')\n",
      "119720095   0.49477464   ('–í–ï–ô–ü/VAPE', 'vapecook', '119,207')\n",
      "99119665   0.4976841   ('–†–∞–±–æ—Ç–∞ –≤ –ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥–µ', 'rg.kaliningrad', '23,990')\n",
      "105074179   0.5046749   ('–ù–∏–¢—ç–ü—ç', 'nitepe', '221,805')\n",
      "136501982   0.50912243   ('‚Ä∫ typical noora s√¶tre | skam', 'typicalnoora', '15,422')\n",
      "54128517   0.5189973   ('–û–±—ä—è–≤–ª–µ–Ω–∏—è –°–µ–≤–∞—Å—Ç–æ–ø–æ–ª—è', 'objavlenijasevastopol', '33,217')\n",
      "19211513   0.5194759   ('–°–º–æ—Ç—Ä–µ—Ç—å —Ñ–∏–ª—å–º—ã –∏ –∫–∞—á–∞—Ç—å —Ñ–∞–π–ª—ã –ª–µ–≥–∫–æ —Å MediaGet', 'mediaget', '214,077')\n",
      "23459755   0.53197795   ('Kiss', 'kiss_club', '56,773')\n",
      "95460709   0.53283453   ('–†—É–∫–æ–¥–µ–ª–∏–µ  | –Ø –∏ –ú–∞–Ω–∏—è –ú–æ—è', 'maniya_moya', '247,040')\n",
      "103059882   1.0   ('–°–æ—é–∑ –õ–µ—Å–æ–∑–∞–≥–æ—Ç–æ–≤–∏—Ç–µ–ª–µ–π. –•–∞—Ä–≤–µ—Å—Ç–µ—Ä—ã –∏ –§–æ—Ä–≤–∞—Ä–¥–µ—Ä—ã.', 'harvester_forvarder', '43,997')\n",
      "\n",
      "\n",
      "29160\n",
      "94666891   0.5112818   ('–ú—É–¥—Ä–æ—Å—Ç—å | –¶–∏—Ç–∞—Ç—ã', 'wisdom_vk', '102,340')\n",
      "67132775   0.5115619   ('–ì–æ—Ä–æ—Å–∫–æ–ø. –í–µ—Å—ã', 'vesyhoroscop', '73,767')\n",
      "104258857   0.5156521   ('–ß–µ–ª—è–±–∏–Ω—Å–∫ –°–∏—Ç–∏', 'chelyabinsk_interesting', '130,352')\n",
      "90839293   0.515982   ('–ü–æ–ª–æ–≤–æ–µ —Å–æ–∑—Ä–µ–≤–∞–Ω–∏–µ 18+', 'ps_man_18', '41,081')\n",
      "164921669   0.5163965   ('–º–æ–π –ª—é–±–∏–º—ã–π –∞–¥–±–ª–æ–∫', 'club164921669', '16,363')\n",
      "35778073   0.5201448   ('–ó–ª–∞ –ñ—ñ–Ω–∫–∞', 'evil_women_original', '437,998')\n",
      "152015719   0.5438863   ('Aliexpress –¥–ª—è –ê–≤—Ç–æ', 'aliexpforcars', '83,740')\n",
      "90629151   0.5705888   ('–ò–≥—Ä–∞ \" –°–º–∞–π–ª—ã \"', 'play_smile100', '30,360')\n",
      "140113391   0.5921392   ('–ù–∞ –ø–æ–∑–∏—Ç–∏–≤–µ. CS:GO –∏ DOTA2 POSITIVE!', 'csgopositive_com', '53,536')\n",
      "12121796   1.0000001   ('–ë–∞–∑–∞ –°–µ—Ä–∏–∞–ª–æ–≤', 'hserials', '101,696')\n",
      "\n",
      "\n",
      "33959\n",
      "51527856   0.49827376   ('Need for Speed', 'nfs', '148,108')\n",
      "16324215   0.49927554   ('Music & Girls | –ú—É–∑—ã–∫–∞ –∏ –î–µ–≤—É—à–∫–∏', 'music_girlz', '96,024')\n",
      "115781935   0.50100577   ('–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π! –¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è –ø–æ–¥–∞—Ä–∫–æ–≤!', 'podarokfreee', '46,528')\n",
      "154037633   0.51268744   ('–ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä –í—ã–µ–∑–¥–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞', 'pomoshauto1', '37,971')\n",
      "118291616   0.5132684   ('THIS IS BTS, BITCH!', 'thisisbtsbitch', '36,581')\n",
      "150023689   0.5247631   ('PERFECTKEYS - –°–∫–∏–Ω—ã –∏ –ò–≥—Ä—ã –Ω–∞ —Ö–∞–ª—è–≤—É!', 'perfectkeys', '145,800')\n",
      "33755237   0.5372599   ('–ú–æ–π –∑–Ω–∞–∫ –∑–æ–¥–∏–∞–∫–∞ –í–µ—Å—ã', 'moi_znak_vesi', '420,657')\n",
      "49757948   0.5704364   ('–í–∫—É—Å–Ω—ã–µ –†–µ—Ü–µ–ø—Ç—ã| –°–∞–ª–∞—Ç—ã. –í—ã–ø–µ—á–∫–∞. –ö—É–ª–∏–Ω–∞—Ä–∏—è', 'vkusnayaedazdes', '548,677')\n",
      "99394451   0.6095257   ('raz dva tri', 'rdtpub', '198,687')\n",
      "171311618   0.9999999   ('–§–æ—Ç–æ–∂–∞–±–∞', 'psjaba', '28,647')\n",
      "\n",
      "\n",
      "38872\n",
      "129277123   0.5160337   ('–ò—â—É –ú–æ–¥–µ–ª—å –ú–æ—Å–∫–≤–∞', 'msk_mm', '51,471')\n",
      "130625149   0.5213729   ('Feder', 'federuniverse', '44,792')\n",
      "36600638   0.52544516   ('–ú–∞–ª–æ–º–µ—Ç—Ä–∞–∂–∫–∞', 'interdecor', '620,578')\n",
      "59169940   0.52745676   ('–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –≤—ã–ø—É—Å–∫–Ω—ã—Ö, –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–æ–≤', 'goodtime_agency', '21,299')\n",
      "26990345   0.5289206   ('–°–ï–†–ì–ï–ô –õ–ê–ó–ê–†–ï–í', 'sergeylazarevgroup', '69,728')\n",
      "24390680   0.529653   ('–ì—Ä—É–±–æ? –ü—Ä–æ—Å—Ç–∏—Ç–µ.', 'grybo', '1,691,124')\n",
      "46365682   0.5388339   ('–ú–∏—Ä –∫—Ä–∞—Å–∏–≤—ã–π –∫–∞–∫ —Å—é–∑–∞–Ω–∏.–ö–µ—Ä–∞–º–∏–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω', 'syuzani', '73,975')\n",
      "82110701   0.578851   ('–¢–µ—Ö–Ω–æ–±–ª–æ–≥', 'game.space', '119,558')\n",
      "123633256   0.63267094   ('–û—Ç–≤–µ—Ç—ã –û–ì–≠', 'oge_shka_run', -1)\n",
      "154503679   1.0   ('‚úò–õ–µ—Å–Ω—ã–µ –°–∞–Ω–∏—Ç–∞—Ä—ã‚úò', 'lsanitar', '41,314')\n",
      "\n",
      "\n",
      "43686\n",
      "109082094   0.50091624   ('Motorola Russia', 'motorola_rus', '58,655')\n",
      "40044114   0.50287706   ('–û—Å—Ç—Ä–æ–≤ –∏–≥—Ä', 'club40044114', '388,538')\n",
      "56250666   0.50481373   ('–ù–ï–î–í–ò–ñ–ò–ú–û–°–¢–¨ | –û–ë–™–Ø–í–õ–ï–ù–ò–Ø ‚ñ∫ –ö–´–ó–´–õ –¢–´–í–ê', 'dom17rus', '41,031')\n",
      "111381362   0.51107395   ('Tom Ford', 'tom144', '35,096')\n",
      "6970317   0.5207712   ('–ö–∞–Ω–∞–ª 1+1', 'tv1plus1ua', '586,212')\n",
      "102003253   0.52342844   ('–ë–£–°–¢ –ò –ü–†–û–î–ê–ñ–ê –ê–ö–ö–ê–£–ù–¢–û–í', 'ali_do', -1)\n",
      "44698322   0.52399683   ('–¢–≤–æ—Ä—á–µ—Å—Ç–≤–æ –Ω–∞ –∫—É—Ö–Ω–µ | –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ä–µ—Ü–µ–ø—Ç—ã', 'super.kitchen', '110,454')\n",
      "93778781   0.529177   ('AuF', 'auf005', '44,298')\n",
      "41449839   0.5372726   ('–°–ï–ö–°  18+', 'cekc_sex', '1,204,749')\n",
      "11459414   0.9999999   ('G-DRAGON', 'gdragongroup', '57,549')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_random(graph, final_embeddings, window_size, embedding_size, num_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96482844   0.49202156   ('–ë–µ—Ç–∫–µ –∞–π—Ç–∞–º—ã–Ω‚óÑ‚óÑ', 'club96482844', '79,611')\n",
      "95128329   0.49394947   ('–ö–∞–†—Ç–ï–ª–¨', 'showroom121', '39,835')\n",
      "93965789   0.49399173   ('–≠–ª–∏—Ç–Ω—ã–π –∫–ª—É–±  \"AlexGrom\"', 'alexgrombet', '20,422')\n",
      "132685380   0.49459141   ('STARRY SKY', 'starry_skyy', '68,922')\n",
      "38594501   0.49731797   ('–ë–µ—Ä—ë–∑–∫–∞', 'berezka', '159,671')\n",
      "32194500   0.49982646   ('–ë—Ä–∞—Ç, —Ç–æ–ª—å–∫–æ –¥–µ—Ä–∂–∏—Å—å‚ôî', 'ceny_brat', '3,420,282')\n",
      "129149255   0.5003519   ('–ü–æ—Ä–Ω–æ –≥–∏—Ñ–∫–∏', 'gif_sex_porn', '39,981')\n",
      "44429130   0.501256   ('–î–µ–≤—É—à–∫–∞ –º–µ—á—Ç—ã - –ö—Ä–∞—Å–æ—Ç–∞ –ú–æ–¥–∞ –°—Ç–∏–ª—å', 'devushka_m', '534,636')\n",
      "76271043   0.5273528   ('–ù–æ–≤—ã–π –∫—Ä–∏–ø—Ç–æ–º–∏—Ä', 'ccgfund', '114,193')\n",
      "129440544   1.0   ('eternal classic', 'eternalclassic', '132,568')\n",
      "\n",
      "\n",
      "62611753   0.4772137   ('–û—á—É–º–µ–ª—ã–µ —Ä—É—á–∫–∏', 'crazy.hands', '474,380')\n",
      "33833860   0.47859693   ('–ë–µ–≥ | –ú—É–∑—ã–∫–∞ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫', 'vk_run', '163,110')\n",
      "103231362   0.47870976   ('our world shinobi', 'our_world_shinobi', '43,196')\n",
      "9581149   0.48787045   ('Cultural and Education Section, British Embassy', 'rubritish', '47,042')\n",
      "67120205   0.4953021   ('–ò–¥–µ–∏ –†–µ–º–æ–Ω—Ç–∞ / –õ–∞–π—Ñ—Ö–∞–∫–∏, –î–∏–∑–∞–π–Ω, –î–æ–º, –ö—É—Ö–Ω—è', 'russia_lifehack', '281,495')\n",
      "127254325   0.4980186   ('–ü—Ä–∏–∑–Ω–∞–≤–∞—à–∫–∏ EagleZ [–ü–ò]', 'priznavashki.eaglez', '64,712')\n",
      "143220855   0.50610423   ('Ã∂–∑Ã∂–∞Ã∂–≤Ã∂—ÇÃ∂—ÄÃ∂–∞Ã∂ Ã∂–±Ã∂—ÄÃ∂–æÃ∂—àÃ∂—ÉÃ∂', 'smotri_na_mir_inache', '34,623')\n",
      "40825951   0.5158861   ('–ù–ï.KURILI', 'nekurili', '72,979')\n",
      "101135802   0.52745634   ('–ë—Ä–∏—Ç–∞–Ω—Å–∫–∏–µ –∏ –®–æ—Ç–ª–∞–Ω–¥—Å–∫–∏–µ –∫–æ—à–∫–∏, –∫–æ—Ç—ã, –∫–æ—Ç—è—Ç–∞', 'bscat', '23,360')\n",
      "28261334   1.0000001   ('TJ', 'tj', '686,523')\n",
      "\n",
      "\n",
      "80816321   0.48784557   ('–°–ø–æ—Ä—Ç –∫–∞–∫ —Å–µ–∫—Å - –æ–Ω –Ω—É–∂–µ–Ω –≤—Å–µ–º', 'club80816321', '84,358')\n",
      "148401642   0.48897213   ('Brawl Stars', 'clashgcr', '50,757')\n",
      "97980111   0.49018672   ('–Ω–∞ –∑–∞—Ä–µ', 'dgavrikovphoto', '36,093')\n",
      "124813322   0.50589454   ('–û—Ç–¥–∞–º –î–∞—Ä–æ–º –£—Ñ–∞', 'odufa', '97,190')\n",
      "116160601   0.5153004   ('–≠—Ç–æ Aliexpress –¥–µ—Ç–∫–∞', 'ali_style_ru', '61,789')\n",
      "23992862   0.5203849   ('–•–ö –¢—Ä–∞–∫—Ç–æ—Ä', 'hctraktor', '56,410')\n",
      "40417895   0.5220344   ('The Best Music', 'listentothebestmusic', '302,288')\n",
      "160396718   0.52813506   ('–ì–∞—Ä–º–æ–Ω–∏—è –¥—É—à–∏ |  –°–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏–µ', 'garmonia_dyshi', '80,374')\n",
      "19397891   0.59229976   ('BASS WAVE', 'basswave', '168,509')\n",
      "92876084   1.0000001   ('–ú–æ–∏ –ª—é–±–∏–º—ã–µ —é–º–æ—Ä–µ—Å–∫–∏', 'jumoreski', '231,593')\n",
      "\n",
      "\n",
      "78757650   0.48746413   ('–õ–µ—Å–±–∏ - –ë–∏ - –õ–µ–∑–±–∏ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞', 'lesbiantime', '41,727')\n",
      "40105130   0.49373907   ('·¥ò·¥Ä—Å—Å–≤·¥á—Ç —Ç–≤o·¥úx –≤–Ω è—Ç·¥ò·¥á–Ω–Ω·¥úx ·¥ã·¥ò·¥Ä—Åo—Ç', '7sky_7', '42,287')\n",
      "47512861   0.4963845   ('AVTO BAZAR KMV –ê–≤—Ç–æ—Ä—ã–Ω–æ–∫ –ü—è—Ç–∏–≥–æ—Ä—Å–∫', 'avto_bazar_kmv', '24,391')\n",
      "71954374   0.4967957   ('Gaping & –¢–û–ü –î—ã—Ä–æ—á–∫–∏', 'club_topass', '34,035')\n",
      "79926109   0.504278   ('–∏—Å—Ç–æ—Ä–∏—è –ø–æ–∏—Å–∫–∞', 'cheguglyatrusskiye', '112,430')\n",
      "56968937   0.508646   ('–ê–Ω–æ–Ω–∏–º–Ω—ã–π –≥–µ–π–º–µ—Ä| Dota Auto Chess', 'anongamer', '210,892')\n",
      "78610805   0.5134219   ('Clash of Clans/Royale –ü—Ä–æ–¥–∞–∂–∞ –∞–∫–∫–∞—É–Ω—Ç–æ–≤ | –ì–µ–º—ã', 'prodaem_akkaynty', '33,371')\n",
      "154921501   0.54397774   ('–ñ–∏–∑–Ω–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏', 'mylife_story', '192,626')\n",
      "118921194   0.5526232   ('‚ñ≤ –ü–∞—Ü–∞–Ω —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–æ–º ¬©', 'pacanzahodi', '31,975')\n",
      "51016572   0.9999998   ('–î–µ—Å–∏–≥–Ω', 'designmdk', '280,094')\n",
      "\n",
      "\n",
      "111136711   0.49829146   ('| –ü–û–•–£–î–ï–¢–¨ –ü–†–ê–í–ò–õ–¨–ù–û |', 'club111136711', '27,484')\n",
      "85957535   0.49870098   ('|BetSize| ‚Ä¢ –ü—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ —Å–ø–æ—Ä—Ç', 'betsize1', -1)\n",
      "1000830   0.50170445   ('–ê–Ω–∏–º–∞—Ü–∏–Ø', 'animaciyainfo', '47,611')\n",
      "116501447   0.508427   ('Minecraft —Å–µ—Ä–≤–µ—Ä CegouCraft - [1.8-1.13+]', 'cegoucraft.mine', '37,997')\n",
      "106297932   0.5112469   ('–≠—Ç–æ –≤–∞–º –Ω–µ –≠—Ç–æ', 'etnet', '359,840')\n",
      "136964272   0.5121804   ('PASTEL', 'pastelblog', '96,972')\n",
      "79625879   0.5201299   ('–û–¢–î–ê–ú –î–ê–†–û–ú –°–¢–ê–†–´–ô –û–°–ö–û–õ', 'daromstoskol', '32,089')\n",
      "89301866   0.5213394   ('History Facts', 'thehistory_facts', '959,710')\n",
      "88305316   0.5524124   ('–í–û –°–ê–î–£ –õ–ò,  –í –û–ì–û–†–û–î–ï -  —Å –ª—é–±–æ–≤—å—é –∫ –∑–µ–º–ª–µ', 'vo_sadu_li', '354,870')\n",
      "91933860   0.9999999   ('N + 1', 'nplusone', '174,924')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_ids=[129440544, 28261334, 92876084, 51016572, 91933860]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    for id in test_ids:\n",
    "        get_closest(final_embeddings, w2i[id])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    embedding_sizes = [32, 48, 64]\n",
    "    num_sampled_arr = [32, 48, 64]\n",
    "    window_sizes = [1, 2, 3, 4, 5]\n",
    "    batch_sizes = [128, 128, 132, 128, 130]\n",
    "    for i in xrange(len(embedding_sizes)):\n",
    "        for j in xrange(len(num_sampled_arr)):\n",
    "            for k in xrange(len(window_sizes)):\n",
    "                embedding_size = embedding_sizes[i]\n",
    "                num_sampled = num_sampled_arr[j]\n",
    "                window_size = window_sizes[k]\n",
    "                batch_size = batch_sizes[k]\n",
    "                \n",
    "                graph, final_embeddings = tf_train(window_size, embedding_size, num_sampled, batch_size)\n",
    "                \n",
    "                print_random(window_size, embedding_size, num_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
