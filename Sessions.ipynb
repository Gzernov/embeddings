{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from six.moves import xrange \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "log = logging.getLogger('log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "lhnd = logging.StreamHandler()\n",
    "lhnd.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "lhnd.setFormatter(formatter)\n",
    "\n",
    "log.addHandler(lhnd)\n",
    "\n",
    "%autonotify -a 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_dumps = False\n",
    "\n",
    "def lmap(f, arr):\n",
    "    return list(map(f, arr))\n",
    "\n",
    "def lfilter(f, arr):\n",
    "    return list(filter(f, arr))\n",
    "\n",
    "def foreach(it, f):\n",
    "    for e in it:\n",
    "        f(e)\n",
    "        \n",
    "def dump(data, name):\n",
    "    with open('data/' + name, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load(name):\n",
    "    with open('data/' + name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def load_or_dump(path, func):\n",
    "    if not Path('data/' + path).exists() or ignore_dumps:\n",
    "        res = func()\n",
    "    \n",
    "        dump(res, path)\n",
    "    else:\n",
    "        res = load(path)\n",
    "        \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "with open('auth/token') as f:\n",
    "    token = f.readline().strip()\n",
    "\n",
    "def get_info(ids):\n",
    "    sleep(0.2)\n",
    "    mc = 'members_count'\n",
    "    payload = {'v': '5.92', 'access_token': token, 'fields':mc}\n",
    "    \n",
    "    str_ids = functools.reduce(\n",
    "        lambda x, y: x + y,\n",
    "        lmap(lambda x: str(x) + ',', ids)\n",
    "    )\n",
    "    \n",
    "    payload['group_ids'] = str_ids[0:- 1]\n",
    "    \n",
    "    r = requests.get('https://api.vk.com/method/groups.getById', \n",
    "                     params=payload)\n",
    "    \n",
    "    if (not 'response' in r.json()):\n",
    "        print(r.json())\n",
    "        \n",
    "    res = lmap(lambda x: (x['name'], x['screen_name'], \"{:,}\".format(x[mc]) if mc in x else -1),r.json()['response'])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 947528\n",
    "\n",
    "def raw_data_filter(file):\n",
    "    # Mapping to events\n",
    "    res = list()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for line in file:\n",
    "        cur = line.rstrip().split(',')\n",
    "        cur = lmap(lambda p: (re.sub(';.*', '', p), re.sub('.*;', '', p)), cur)\n",
    "\n",
    "        session = list()\n",
    "        \n",
    "        for j in range(0, len(cur)):\n",
    "            try:\n",
    "                session.append(int(cur[j][1]))\n",
    "            except ValueError:\n",
    "                None\n",
    "                \n",
    "        res.append(session)\n",
    "\n",
    "        i = i + 1\n",
    "                \n",
    "        if (i % 100000 == 0):\n",
    "            gc.collect()\n",
    "\n",
    "            log.debug(\"%d %% of mapping is done.\", i / total * 100)\n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-18 10:38:48,502 INFO Data loaded\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_or_dump('raw', lambda: raw_data_filter(open(\"data/sessions_public.txt\",\"r\")))\n",
    "\n",
    "log.info(\"Data loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_count(data):\n",
    "    total = dict()\n",
    "\n",
    "    for i in data:\n",
    "        for j in i[0]:\n",
    "            if (j in total.keys()):\n",
    "                total[j] = total[j] + 1\n",
    "            else:\n",
    "                total[j] = 1\n",
    "                \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_session_size = 5\n",
    "\n",
    "def initiail_mapping(lst, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set()\n",
    "        malformed = set()\n",
    "        \n",
    "        for event in session:\n",
    "            if (event < 0):\n",
    "                sub_event = -event\n",
    "                \n",
    "                if (sub_event in sub or sub_event in malformed):\n",
    "                    sub.discard(sub_event)\n",
    "                    unsub.discard(sub_event)\n",
    "                    malformed.add(sub_event)\n",
    "                else:\n",
    "                    unsub.add(sub_event)\n",
    "            else:\n",
    "                if (event in unsub or event in malformed):\n",
    "                    unsub.discard(event)\n",
    "                    sub.discard(event)\n",
    "                    malformed.add(event)\n",
    "                else:\n",
    "                    sub.add(event)\n",
    "        \n",
    "        if (len(sub) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "    \n",
    "\n",
    "def set_map(lst, cnt, min_allowed):\n",
    "    result = []\n",
    "    groups = set()\n",
    "    \n",
    "    for session in lst:\n",
    "        unsub = set()\n",
    "        sub = set() \n",
    "        \n",
    "        for event in session[0]:\n",
    "            if (cnt[event] > min_allowed):\n",
    "                sub.add(event)\n",
    "                \n",
    "        for event in session[1]:\n",
    "            if (cnt.get(event, -1) > min_allowed):\n",
    "                unsub.add(event)    \n",
    "        \n",
    "        if (len(sub) >= min_session_size):\n",
    "            for event in sub:\n",
    "                groups.add(event)\n",
    "            for event in unsub:\n",
    "                groups.add(event)\n",
    "            \n",
    "            result.append((sub, unsub))\n",
    "    \n",
    "    return result, groups\n",
    "\n",
    "def drop_uncommon(raw_data, min_allowed = 10):\n",
    "    cnt = None\n",
    "    sorted_cnt = None\n",
    "    \n",
    "    data, groups = initiail_mapping(raw_data, min_allowed)\n",
    "    cnt = group_count(data) \n",
    "    sorted_cnt = sorted(list(cnt.values()))\n",
    "    \n",
    "    while (cnt == None or sorted_cnt[0] < min_allowed):\n",
    "        data, groups = set_map(data, cnt, min_allowed)\n",
    "                \n",
    "        cnt = group_count(data) \n",
    "        sorted_cnt = sorted(list(cnt.values()))\n",
    "        \n",
    "        log.info(\"Length of data:   %d\", len(data))\n",
    "        log.info(\"Total length:     %d\", \n",
    "                functools.reduce((lambda x, y: x + y), lmap(lambda a: len(a), data))\n",
    "                )\n",
    "        log.info(\"Number of groups: %d\", len(groups))\n",
    "        log.info(\"Minimum count:    %d\\n\", sorted_cnt[0])\n",
    "        \n",
    "    return data, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ignore_dumps = False\n",
    "data, groups = load_or_dump('final_data', lambda: drop_uncommon(raw_data, 50))\n",
    "\n",
    "most_common = sorted(group_count(data).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "w2i = {w: i for i, w in enumerate(groups)}\n",
    "i2w = {i: w for i, w in enumerate(groups)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = None\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dex = 0\n",
    "event_dex = 0\n",
    "\n",
    "def generate_batch(batch_size, negative_size, window_size = 1):\n",
    "    assert min_session_size >= window_size * 2 + 1 \n",
    "    assert batch_size % (window_size * 2) == 0\n",
    "    \n",
    "    global session_dex\n",
    "    global event_dex\n",
    "    \n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    negative = np.ndarray(shape=(negative_size), dtype=np.int32)\n",
    "    \n",
    "    if (event_dex == 0):\n",
    "        event_dex = window_size\n",
    "    \n",
    "    current = 0\n",
    "    session = list(data[session_dex][0])\n",
    "    \n",
    "    while (current < batch_size):\n",
    "        i = 0\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if (j != 0):\n",
    "                labels[current + i][0] = w2i[session[event_dex + j]]\n",
    "                batch[current + i] = w2i[session[event_dex]]\n",
    "                i += 1\n",
    "\n",
    "        event_dex += 1\n",
    "        current += window_size * 2\n",
    "\n",
    "        if (event_dex + window_size >= len(session)):\n",
    "            event_dex = window_size\n",
    "            session_dex = session_dex + 1\n",
    "            if (session_dex >= len(data)):\n",
    "                session_dex = 0\n",
    "            session = list(data[session_dex][0])\n",
    "            \n",
    "    neg = 0\n",
    "        \n",
    "    for i in data[session_dex][1]:\n",
    "        negative[neg] = w2i[i]\n",
    "        neg += 1\n",
    "        if (neg == negative_size):\n",
    "            break\n",
    "            \n",
    "    rand_neg = np.random.randint(len(groups), size=negative_size - neg)\n",
    "        \n",
    "    for i in range(0, negative_size - neg):\n",
    "        negative[neg + i] = rand_neg[i]\n",
    "        \n",
    "     \n",
    "    return batch, labels, negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({23372133, 1959, 20650061, 70034991, 22741624, 35540891, 75909948}, set())\n",
      "({25794755, 91683885, 34812270, 39325103, 49128190, 34523318, 42533142, 46755517, 104237982}, set())\n",
      "26468 20650061 -> 15329 23372133\n",
      "26468 20650061 -> 745 1959\n",
      "26468 20650061 -> 15791 70034991\n",
      "26468 20650061 -> 24456 22741624\n",
      "15791 70034991 -> 745 1959\n",
      "15791 70034991 -> 26468 20650061\n",
      "15791 70034991 -> 24456 22741624\n",
      "15791 70034991 -> 7531 35540891\n",
      "24456 22741624 -> 26468 20650061\n",
      "24456 22741624 -> 15791 70034991\n",
      "[19650 26897 16391  8306 36492]\n"
     ]
    }
   ],
   "source": [
    "batch, labels, negative = generate_batch(16, 5, 2)\n",
    "\n",
    "print(data[0])\n",
    "print(data[1])\n",
    "\n",
    "for i in range(10):\n",
    "    print(batch[i], i2w[batch[i]], '->', labels[i, 0],\n",
    "          i2w[labels[i, 0]])\n",
    "    \n",
    "print(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data = None\n",
    "\n",
    "learning_rate = 0.5\n",
    "vocabulary_size = len(groups)\n",
    "\n",
    "window_size = 1\n",
    "embedding_size = 48\n",
    "num_sampled = 5\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(emb, index, f = None):\n",
    "    p = emb[index]\n",
    "    cnst = tf.constant(p, shape=[1, embedding_size])\n",
    "    d = tf.matmul(cnst, emb, transpose_b=True).eval()[0]\n",
    "\n",
    "    dxs = np.argsort(np.array(d))\n",
    "    \n",
    "    ids = []\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(dxs) - 10, len(dxs)):\n",
    "        ids.append(i2w[dxs[i]])\n",
    "        res.append(d[dxs[i]])\n",
    "        \n",
    "    info = get_info(ids)\n",
    "    \n",
    "    for i in xrange(len(res)):\n",
    "        print(ids[i], ' ', res[i], ' ', info[i])\n",
    "        \n",
    "        if (f != None):\n",
    "            f.write(str(ids[i]) + ' ' + str(res[i]) + ' ' + str(info[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 200000\n",
    "\n",
    "def mk_negative_samples(train_labels, num_sampled, negative_labels):\n",
    "    tmp = tf.random.log_uniform_candidate_sampler(\n",
    "                            true_classes=train_labels,\n",
    "                            num_true=1,\n",
    "                            num_sampled=num_sampled,\n",
    "                            unique=True,\n",
    "                            range_max=num_sampled,\n",
    "                            seed=None,\n",
    "                            name=None\n",
    "                        )\n",
    "\n",
    "    return (tf.map_fn(lambda x: negative_labels[x], tmp.sampled_candidates), \n",
    "                             tmp.true_expected_count, \n",
    "                             tmp.sampled_expected_count)\n",
    "\n",
    "def loss_function(nce_weights, nce_biases, train_labels, negative_labels, embed, num_sampled, vocabulary_size):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights=nce_weights,\n",
    "                        biases=nce_biases,\n",
    "                        labels=train_labels,\n",
    "                        inputs=embed,\n",
    "                        num_sampled=num_sampled,\n",
    "                        num_classes=vocabulary_size,\n",
    "                        num_true=1,\n",
    "                        sampled_values=mk_negative_samples(train_labels, num_sampled, negative_labels),\n",
    "                        remove_accidental_hits=True,\n",
    "                        partition_strategy='mod',\n",
    "                        name='sampled_softmax_loss',\n",
    "                        seed=None)\n",
    "\n",
    "\n",
    "def tf_train(window_size, embedding_size, num_sampled, batch_size):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        # Input data.\n",
    "        with tf.name_scope('inputs'):\n",
    "            train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "            negative_samples = tf.placeholder(tf.int64, shape=[num_sampled])\n",
    "            train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])\n",
    "\n",
    "        # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "        with tf.device('/cpu:0'):\n",
    "            # Look up embeddings for inputs.\n",
    "            with tf.name_scope('embeddings'):\n",
    "                embeddings = tf.Variable(\n",
    "                    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "                )\n",
    "                embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "          # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.reduce_mean(loss_function(nce_weights, \n",
    "                                 nce_biases, \n",
    "                                 train_labels, \n",
    "                                 negative_samples, \n",
    "                                 embed, \n",
    "                                 num_sampled, \n",
    "                                 vocabulary_size\n",
    "                                ))\n",
    "\n",
    "            # Add the loss value as a scalar to summary.\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        with tf.name_scope('optimizer'):\n",
    "              optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Compute the cosine similarity between minibatch examples and all\n",
    "        # embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        #valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "        #                                          valid_dataset)\n",
    "        #similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        # Merge all summaries.\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Create a saver.\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    with tf.Session(graph=graph) as session:     \n",
    "        \n",
    "        # Open a writer to write summaries.\n",
    "        writer = tf.summary.FileWriter(\"tmp\", session.graph)\n",
    "\n",
    "        # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        log.info('Initialized. Embedding size: %s; Num sampled: %s; Window size: %s; Batch size: %s', embedding_size, num_sampled, window_size, batch_size)\n",
    "        average_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "            batch_inputs, batch_labels, batch_negative = generate_batch(batch_size, num_sampled, window_size)\n",
    "            feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels, negative_samples:batch_negative}\n",
    "            \n",
    "            # Define metadata variable.\n",
    "            run_metadata = tf.RunMetadata()\n",
    "\n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "            # Also, evaluate the merged op to get all summaries from the returned\n",
    "            # \"summary\" variable. Feed metadata variable to session for visualizing\n",
    "            # the graph in TensorBoard.\n",
    "            _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                             feed_dict=feed_dict,\n",
    "                                             run_metadata=run_metadata)     \n",
    "            average_loss += loss_val\n",
    "\n",
    "            # Add returned summaries to writer in each step.\n",
    "            writer.add_summary(summary, step)\n",
    "            # Add metadata to visualize the graph for the last run.\n",
    "            if step == (num_steps - 1):\n",
    "                writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                      average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000\n",
    "                # batches.\n",
    "                log.debug('Average loss at step %d: %.4f', step, average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            #if step % 20000 == 0 and step != 0:\n",
    "                #print('Most closest to ', most_common[0][0])\n",
    "                #get_closest(normalized_embeddings.eval(), w2i[most_common[0][0]])\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    return graph, final_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-18 18:12:56,152 INFO Initialized. Embedding size: 48; Num sampled: 5; Window size: 1; Batch size: 4\n",
      "2019-03-18 18:12:56,208 DEBUG Average loss at step 0: 0.0039\n",
      "2019-03-18 18:12:57,685 DEBUG Average loss at step 2000: 0.0087\n",
      "2019-03-18 18:12:59,207 DEBUG Average loss at step 4000: 0.0080\n",
      "2019-03-18 18:13:00,761 DEBUG Average loss at step 6000: 0.0081\n",
      "2019-03-18 18:13:02,229 DEBUG Average loss at step 8000: 0.0084\n",
      "2019-03-18 18:13:03,753 DEBUG Average loss at step 10000: 0.0075\n",
      "2019-03-18 18:13:05,256 DEBUG Average loss at step 12000: 0.0081\n",
      "2019-03-18 18:13:06,718 DEBUG Average loss at step 14000: 0.0075\n",
      "2019-03-18 18:13:08,279 DEBUG Average loss at step 16000: 0.0072\n",
      "2019-03-18 18:13:09,741 DEBUG Average loss at step 18000: 0.0088\n",
      "2019-03-18 18:13:11,191 DEBUG Average loss at step 20000: 0.0080\n",
      "2019-03-18 18:13:12,648 DEBUG Average loss at step 22000: 0.0079\n",
      "2019-03-18 18:13:14,091 DEBUG Average loss at step 24000: 0.0081\n",
      "2019-03-18 18:13:15,548 DEBUG Average loss at step 26000: 0.0072\n",
      "2019-03-18 18:13:17,128 DEBUG Average loss at step 28000: 0.0077\n",
      "2019-03-18 18:13:18,767 DEBUG Average loss at step 30000: 0.0076\n",
      "2019-03-18 18:13:20,322 DEBUG Average loss at step 32000: 0.0074\n",
      "2019-03-18 18:13:21,785 DEBUG Average loss at step 34000: 0.0078\n",
      "2019-03-18 18:13:23,229 DEBUG Average loss at step 36000: 0.0075\n",
      "2019-03-18 18:13:24,677 DEBUG Average loss at step 38000: 0.0071\n",
      "2019-03-18 18:13:26,123 DEBUG Average loss at step 40000: 0.0066\n",
      "2019-03-18 18:13:27,573 DEBUG Average loss at step 42000: 0.0073\n",
      "2019-03-18 18:13:29,030 DEBUG Average loss at step 44000: 0.0073\n",
      "2019-03-18 18:13:30,477 DEBUG Average loss at step 46000: 0.0072\n",
      "2019-03-18 18:13:32,061 DEBUG Average loss at step 48000: 0.0072\n",
      "2019-03-18 18:13:33,696 DEBUG Average loss at step 50000: 0.0069\n",
      "2019-03-18 18:13:35,152 DEBUG Average loss at step 52000: 0.0068\n",
      "2019-03-18 18:13:36,698 DEBUG Average loss at step 54000: 0.0070\n",
      "2019-03-18 18:13:38,244 DEBUG Average loss at step 56000: 0.0072\n",
      "2019-03-18 18:13:39,864 DEBUG Average loss at step 58000: 0.0068\n",
      "2019-03-18 18:13:41,615 DEBUG Average loss at step 60000: 0.0069\n",
      "2019-03-18 18:13:43,215 DEBUG Average loss at step 62000: 0.0071\n",
      "2019-03-18 18:13:44,726 DEBUG Average loss at step 64000: 0.0076\n",
      "2019-03-18 18:13:46,175 DEBUG Average loss at step 66000: 0.0070\n",
      "2019-03-18 18:13:47,623 DEBUG Average loss at step 68000: 0.0072\n",
      "2019-03-18 18:13:49,063 DEBUG Average loss at step 70000: 0.0075\n",
      "2019-03-18 18:13:50,515 DEBUG Average loss at step 72000: 0.0078\n",
      "2019-03-18 18:13:51,978 DEBUG Average loss at step 74000: 0.0067\n",
      "2019-03-18 18:13:53,432 DEBUG Average loss at step 76000: 0.0071\n",
      "2019-03-18 18:13:54,886 DEBUG Average loss at step 78000: 0.0072\n",
      "2019-03-18 18:13:56,356 DEBUG Average loss at step 80000: 0.0066\n",
      "2019-03-18 18:13:57,803 DEBUG Average loss at step 82000: 0.0067\n",
      "2019-03-18 18:13:59,286 DEBUG Average loss at step 84000: 0.0066\n",
      "2019-03-18 18:14:00,751 DEBUG Average loss at step 86000: 0.0070\n",
      "2019-03-18 18:14:02,222 DEBUG Average loss at step 88000: 0.0073\n",
      "2019-03-18 18:14:03,675 DEBUG Average loss at step 90000: 0.0063\n",
      "2019-03-18 18:14:05,133 DEBUG Average loss at step 92000: 0.0064\n",
      "2019-03-18 18:14:06,604 DEBUG Average loss at step 94000: 0.0066\n",
      "2019-03-18 18:14:08,062 DEBUG Average loss at step 96000: 0.0062\n",
      "2019-03-18 18:14:09,522 DEBUG Average loss at step 98000: 0.0062\n",
      "2019-03-18 18:14:10,982 DEBUG Average loss at step 100000: 0.0064\n",
      "2019-03-18 18:14:12,450 DEBUG Average loss at step 102000: 0.0071\n",
      "2019-03-18 18:14:13,909 DEBUG Average loss at step 104000: 0.0067\n",
      "2019-03-18 18:14:15,380 DEBUG Average loss at step 106000: 0.0069\n",
      "2019-03-18 18:14:16,838 DEBUG Average loss at step 108000: 0.0071\n",
      "2019-03-18 18:14:18,305 DEBUG Average loss at step 110000: 0.0063\n",
      "2019-03-18 18:14:19,846 DEBUG Average loss at step 112000: 0.0066\n",
      "2019-03-18 18:14:21,372 DEBUG Average loss at step 114000: 0.0061\n",
      "2019-03-18 18:14:22,851 DEBUG Average loss at step 116000: 0.0071\n",
      "2019-03-18 18:14:24,307 DEBUG Average loss at step 118000: 0.0058\n",
      "2019-03-18 18:14:25,774 DEBUG Average loss at step 120000: 0.0064\n",
      "2019-03-18 18:14:27,231 DEBUG Average loss at step 122000: 0.0068\n",
      "2019-03-18 18:14:28,727 DEBUG Average loss at step 124000: 0.0068\n",
      "2019-03-18 18:14:30,182 DEBUG Average loss at step 126000: 0.0065\n",
      "2019-03-18 18:14:31,728 DEBUG Average loss at step 128000: 0.0066\n",
      "2019-03-18 18:14:33,341 DEBUG Average loss at step 130000: 0.0070\n",
      "2019-03-18 18:14:34,992 DEBUG Average loss at step 132000: 0.0068\n",
      "2019-03-18 18:14:36,762 DEBUG Average loss at step 134000: 0.0059\n",
      "2019-03-18 18:14:38,313 DEBUG Average loss at step 136000: 0.0067\n",
      "2019-03-18 18:14:39,923 DEBUG Average loss at step 138000: 0.0063\n",
      "2019-03-18 18:14:41,463 DEBUG Average loss at step 140000: 0.0060\n",
      "2019-03-18 18:14:43,059 DEBUG Average loss at step 142000: 0.0079\n",
      "2019-03-18 18:14:44,576 DEBUG Average loss at step 144000: 0.0063\n",
      "2019-03-18 18:14:46,288 DEBUG Average loss at step 146000: 0.0063\n",
      "2019-03-18 18:14:48,092 DEBUG Average loss at step 148000: 0.0071\n",
      "2019-03-18 18:14:50,345 DEBUG Average loss at step 150000: 0.0074\n",
      "2019-03-18 18:14:53,471 DEBUG Average loss at step 152000: 0.0061\n",
      "2019-03-18 18:14:56,943 DEBUG Average loss at step 154000: 0.0063\n",
      "2019-03-18 18:14:59,872 DEBUG Average loss at step 156000: 0.0059\n",
      "2019-03-18 18:15:04,644 DEBUG Average loss at step 158000: 0.0062\n",
      "2019-03-18 18:15:08,776 DEBUG Average loss at step 160000: 0.0063\n",
      "2019-03-18 18:15:13,064 DEBUG Average loss at step 162000: 0.0058\n",
      "2019-03-18 18:15:17,841 DEBUG Average loss at step 164000: 0.0065\n",
      "2019-03-18 18:15:23,734 DEBUG Average loss at step 166000: 0.0060\n",
      "2019-03-18 18:15:28,081 DEBUG Average loss at step 168000: 0.0060\n",
      "2019-03-18 18:15:32,734 DEBUG Average loss at step 170000: 0.0056\n",
      "2019-03-18 18:15:38,391 DEBUG Average loss at step 172000: 0.0063\n",
      "2019-03-18 18:15:44,989 DEBUG Average loss at step 174000: 0.0062\n",
      "2019-03-18 18:15:49,778 DEBUG Average loss at step 176000: 0.0062\n",
      "2019-03-18 18:15:52,925 DEBUG Average loss at step 178000: 0.0064\n",
      "2019-03-18 18:15:55,149 DEBUG Average loss at step 180000: 0.0060\n",
      "2019-03-18 18:15:56,706 DEBUG Average loss at step 182000: 0.0059\n",
      "2019-03-18 18:15:58,234 DEBUG Average loss at step 184000: 0.0058\n",
      "2019-03-18 18:15:59,777 DEBUG Average loss at step 186000: 0.0061\n",
      "2019-03-18 18:16:01,307 DEBUG Average loss at step 188000: 0.0063\n",
      "2019-03-18 18:16:02,831 DEBUG Average loss at step 190000: 0.0057\n",
      "2019-03-18 18:16:04,343 DEBUG Average loss at step 192000: 0.0066\n",
      "2019-03-18 18:16:05,879 DEBUG Average loss at step 194000: 0.0060\n",
      "2019-03-18 18:16:07,392 DEBUG Average loss at step 196000: 0.0060\n",
      "2019-03-18 18:16:08,891 DEBUG Average loss at step 198000: 0.0058\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"5c0574d8-04b8-49b5-aa46-c2facb3f1b7d\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"5c0574d8-04b8-49b5-aa46-c2facb3f1b7d\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": false};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph, final_embeddings = tf_train(window_size, embedding_size, num_sampled, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def print_random(graph, final_embeddings, window_size, embedding_size, num_sampled):\n",
    "    try:\n",
    "        p = 'data/reports/' + 'es_' + str(embedding_size) + '_ns_' + str(num_sampled) + '_ws_' + str(window_size)\n",
    "        with open(p, 'w') as f:\n",
    "            for i in range(0, 10):\n",
    "                with tf.Session(graph=graph) as session:\n",
    "                    index = len(groups) * i // 10 + randint(0, 100)\n",
    "\n",
    "                    f.write(str(index) + '\\n')\n",
    "                    print(index)\n",
    "\n",
    "                    get_closest(final_embeddings, w2i[most_common[index][0]], f)\n",
    "\n",
    "                f.write('\\n\\n')\n",
    "                print('\\n')\n",
    "    except err:\n",
    "        print(err)\n",
    "        None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "105503338   0.49311572   ('СпортНяшечка', 'club105503338', '72,334')\n",
      "132782438   0.49472433   ('Боевые Жигули', 'boevaja_classica', '517,595')\n",
      "86526388   0.49519143   ('Работа Астана', 'job_astana', '33,609')\n",
      "145233167   0.4958241   ('BTS', 'lovvekorean', '134,595')\n",
      "94404133   0.5004687   ('Бесплатный Челябинск', 'ch_free', '31,827')\n",
      "17154489   0.50466454   ('Психология отношений | Статусы ♂♀ ♥  by Love ♥', 'psiholog_otnosheniy', '77,035')\n",
      "115954385   0.50707656   ('Одетая в Счастье. Психология/Эзотерика', 'club115954385', '18,484')\n",
      "34084756   0.5082171   ('PRO FOTO', 'profotopro', '137,378')\n",
      "70181160   0.5413219   ('Hearthstone с Хаппой', 'clubhappa', '37,261')\n",
      "40567146   0.9999999   ('Лайфхак', 'lhack', '7,944,169')\n",
      "\n",
      "\n",
      "4946\n",
      "31955706   0.4758619   ('السكوت من ذهب', 'alishaisakova', '139,285')\n",
      "139355889   0.47945952   ('Бесплатно за репост | Конкурсы', 'freefor_repost', -1)\n",
      "145491320   0.48079926   ('Full HD', 'full_hd2', '23,488')\n",
      "166991867   0.50103796   ('you disappoint me', 'club166991867', '40,896')\n",
      "31162282   0.5043559   ('Ancord - паблик фанатов озвучки', 'ancordpage', '107,852')\n",
      "68530208   0.5089486   ('Русское , Частное и Домашнее ПОРНО,МИНЕТ,АНАЛ!', 'club68530208', '16,844')\n",
      "118842368   0.5244212   ('Старик Похабыч', 'club118842368', '33,041')\n",
      "69222461   0.53148985   ('СЛАВЯНСКИЙ МИР', 'slavyanemir', '76,237')\n",
      "37207068   0.5996443   ('Дневник Современной Девушки . Статусы', 'women.manner', '901,329')\n",
      "149878785   1.0   ('черное и белое', 'cocky_dove', '534,658')\n",
      "\n",
      "\n",
      "9765\n",
      "32657308   0.50565886   ('ТИПИЧНАЯ ДЕВУШКА', 'iokean', '670,522')\n",
      "100867898   0.5109795   ('K.O.Y', 'k_o_y_yki', '40,348')\n",
      "158003111   0.5145847   ('Животные - в юморе !', 'jivotnie_v_umore', '22,203')\n",
      "134488897   0.5228721   ('Стань Успешным | Прогнозы на спорт', 'rezerv_dm', '57,306')\n",
      "140728995   0.5242412   ('Никто тебя не слышит ©', 'no_one_will_hear_you', '50,027')\n",
      "91055713   0.53019655   ('Оригинальные и необычные подарки', 'originalitems', '36,004')\n",
      "125449777   0.5327132   ('Одежда и обувь Донецк-ДНР', 'red_wall_dnr', '25,355')\n",
      "129806371   0.53573334   ('Работа в Красноярске', 'club129806371', '33,316')\n",
      "73948343   0.5582477   ('Сила мысли', 'publicfeelthepower', '386,016')\n",
      "86530505   0.9999999   ('Пиф паф.', 'pifpaf004', '147,571')\n",
      "\n",
      "\n",
      "14568\n",
      "127960208   0.49379316   ('GTA РОССИЯ | RPBOX (РПБОКС)', 'rpbox', '90,176')\n",
      "22421319   0.49752715   ('Помощь в конкурсе|голосовании', '1helpme', -1)\n",
      "148589939   0.5010873   ('W O R L D', 'btu_world', '106,820')\n",
      "104518035   0.501324   ('Блог Артема Дорученко', 'artemdoruchenko', '56,840')\n",
      "36422332   0.5035672   ('Фильмы и Сериалы 2018 [Фильм Всем]', 'filmvsem', '175,668')\n",
      "62536814   0.5057448   ('France🇫🇷', 'france_777', '53,293')\n",
      "107110988   0.50738597   ('БОЛЬШАЯ ГРУДЬ ТОЛСТУШКИ', 'big_fatty', '35,863')\n",
      "22192347   0.5144558   ('The Elder Scrolls', 'the_elder_council', '129,409')\n",
      "46729094   0.5194784   ('Учебник ремонта | Строительство', 'uchebnik_remonta', '259,108')\n",
      "40766972   0.99999994   ('Скоро в Питере', 'piter_tomorrow', '186,533')\n",
      "\n",
      "\n",
      "19391\n",
      "23141788   0.50376415   ('Идеальная. Диеты и спорт', 'befitness', '1,703')\n",
      "106402320   0.51689214   ('Внезапно прекрасно', 'puk_punk', '34,841')\n",
      "130865560   0.5170442   (\"★Men's Car & Girl look★\", 'cars_look', -1)\n",
      "107395916   0.5270181   ('Киев | Знакомства Украина | Знайомства Україна', 'love_ukrenian', '26,266')\n",
      "11062633   0.54143   ('Песни | Лирика', 'lirikslov', '103,411')\n",
      "66523083   0.55978686   ('ШКОЛА СМЕХА ツ', 'shk_smex', '843,391')\n",
      "78600075   0.564777   ('ИЗНАНКА ФИТНЕСА', 'fitnessnaiznanky', '60,609')\n",
      "6992713   0.5767462   (\"Мужской взгляд l MAN'S LOOK\", 'manss_look', '35,465')\n",
      "134748687   0.5947615   ('NeProstoGamer', 'neprostogamer', '22,482')\n",
      "124421250   0.9999999   ('Мне показалось | За секунду до..', 'zasekundu_pokazalos', '52,542')\n",
      "\n",
      "\n",
      "24245\n",
      "60861374   0.49422085   ('Кровати-МАШИНЫ (мебель кровать машина ) МАТРАСЫ', 'crovatmashina35', '37,929')\n",
      "119720095   0.49477464   ('ВЕЙП/VAPE', 'vapecook', '119,207')\n",
      "99119665   0.4976841   ('Работа в Калининграде', 'rg.kaliningrad', '23,990')\n",
      "105074179   0.5046749   ('НиТэПэ', 'nitepe', '221,805')\n",
      "136501982   0.50912243   ('› typical noora sætre | skam', 'typicalnoora', '15,422')\n",
      "54128517   0.5189973   ('Объявления Севастополя', 'objavlenijasevastopol', '33,217')\n",
      "19211513   0.5194759   ('Смотреть фильмы и качать файлы легко с MediaGet', 'mediaget', '214,077')\n",
      "23459755   0.53197795   ('Kiss', 'kiss_club', '56,773')\n",
      "95460709   0.53283453   ('Рукоделие  | Я и Мания Моя', 'maniya_moya', '247,040')\n",
      "103059882   1.0   ('Союз Лесозаготовителей. Харвестеры и Форвардеры.', 'harvester_forvarder', '43,997')\n",
      "\n",
      "\n",
      "29160\n",
      "94666891   0.5112818   ('Мудрость | Цитаты', 'wisdom_vk', '102,340')\n",
      "67132775   0.5115619   ('Гороскоп. Весы', 'vesyhoroscop', '73,767')\n",
      "104258857   0.5156521   ('Челябинск Сити', 'chelyabinsk_interesting', '130,352')\n",
      "90839293   0.515982   ('Половое созревание 18+', 'ps_man_18', '41,081')\n",
      "164921669   0.5163965   ('мой любимый адблок', 'club164921669', '16,363')\n",
      "35778073   0.5201448   ('Зла Жінка', 'evil_women_original', '437,998')\n",
      "152015719   0.5438863   ('Aliexpress для Авто', 'aliexpforcars', '83,740')\n",
      "90629151   0.5705888   ('Игра \" Смайлы \"', 'play_smile100', '30,360')\n",
      "140113391   0.5921392   ('На позитиве. CS:GO и DOTA2 POSITIVE!', 'csgopositive_com', '53,536')\n",
      "12121796   1.0000001   ('База Сериалов', 'hserials', '101,696')\n",
      "\n",
      "\n",
      "33959\n",
      "51527856   0.49827376   ('Need for Speed', 'nfs', '148,108')\n",
      "16324215   0.49927554   ('Music & Girls | Музыка и Девушки', 'music_girlz', '96,024')\n",
      "115781935   0.50100577   ('Бесплатный! Территория подарков!', 'podarokfreee', '46,528')\n",
      "154037633   0.51268744   ('Автоподбор Выездная диагностика', 'pomoshauto1', '37,971')\n",
      "118291616   0.5132684   ('THIS IS BTS, BITCH!', 'thisisbtsbitch', '36,581')\n",
      "150023689   0.5247631   ('PERFECTKEYS - Скины и Игры на халяву!', 'perfectkeys', '145,800')\n",
      "33755237   0.5372599   ('Мой знак зодиака Весы', 'moi_znak_vesi', '420,657')\n",
      "49757948   0.5704364   ('Вкусные Рецепты| Салаты. Выпечка. Кулинария', 'vkusnayaedazdes', '548,677')\n",
      "99394451   0.6095257   ('raz dva tri', 'rdtpub', '198,687')\n",
      "171311618   0.9999999   ('Фотожаба', 'psjaba', '28,647')\n",
      "\n",
      "\n",
      "38872\n",
      "129277123   0.5160337   ('Ищу Модель Москва', 'msk_mm', '51,471')\n",
      "130625149   0.5213729   ('Feder', 'federuniverse', '44,792')\n",
      "36600638   0.52544516   ('Малометражка', 'interdecor', '620,578')\n",
      "59169940   0.52745676   ('Организация и проведение выпускных, корпоративов', 'goodtime_agency', '21,299')\n",
      "26990345   0.5289206   ('СЕРГЕЙ ЛАЗАРЕВ', 'sergeylazarevgroup', '69,728')\n",
      "24390680   0.529653   ('Грубо? Простите.', 'grybo', '1,691,124')\n",
      "46365682   0.5388339   ('Мир красивый как сюзани.Керамика разных стран', 'syuzani', '73,975')\n",
      "82110701   0.578851   ('Техноблог', 'game.space', '119,558')\n",
      "123633256   0.63267094   ('Ответы ОГЭ', 'oge_shka_run', -1)\n",
      "154503679   1.0   ('✘Лесные Санитары✘', 'lsanitar', '41,314')\n",
      "\n",
      "\n",
      "43686\n",
      "109082094   0.50091624   ('Motorola Russia', 'motorola_rus', '58,655')\n",
      "40044114   0.50287706   ('Остров игр', 'club40044114', '388,538')\n",
      "56250666   0.50481373   ('НЕДВИЖИМОСТЬ | ОБЪЯВЛЕНИЯ ► КЫЗЫЛ ТЫВА', 'dom17rus', '41,031')\n",
      "111381362   0.51107395   ('Tom Ford', 'tom144', '35,096')\n",
      "6970317   0.5207712   ('Канал 1+1', 'tv1plus1ua', '586,212')\n",
      "102003253   0.52342844   ('БУСТ И ПРОДАЖА АККАУНТОВ', 'ali_do', -1)\n",
      "44698322   0.52399683   ('Творчество на кухне | Интересные рецепты', 'super.kitchen', '110,454')\n",
      "93778781   0.529177   ('AuF', 'auf005', '44,298')\n",
      "41449839   0.5372726   ('СЕКС  18+', 'cekc_sex', '1,204,749')\n",
      "11459414   0.9999999   ('G-DRAGON', 'gdragongroup', '57,549')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_random(graph, final_embeddings, window_size, embedding_size, num_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96482844   0.49202156   ('Бетке айтамын◄◄', 'club96482844', '79,611')\n",
      "95128329   0.49394947   ('КаРтЕлЬ', 'showroom121', '39,835')\n",
      "93965789   0.49399173   ('Элитный клуб  \"AlexGrom\"', 'alexgrombet', '20,422')\n",
      "132685380   0.49459141   ('STARRY SKY', 'starry_skyy', '68,922')\n",
      "38594501   0.49731797   ('Берёзка', 'berezka', '159,671')\n",
      "32194500   0.49982646   ('Брат, только держись♔', 'ceny_brat', '3,420,282')\n",
      "129149255   0.5003519   ('Порно гифки', 'gif_sex_porn', '39,981')\n",
      "44429130   0.501256   ('Девушка мечты - Красота Мода Стиль', 'devushka_m', '534,636')\n",
      "76271043   0.5273528   ('Новый криптомир', 'ccgfund', '114,193')\n",
      "129440544   1.0   ('eternal classic', 'eternalclassic', '132,568')\n",
      "\n",
      "\n",
      "62611753   0.4772137   ('Очумелые ручки', 'crazy.hands', '474,380')\n",
      "33833860   0.47859693   ('Бег | Музыка для тренировок', 'vk_run', '163,110')\n",
      "103231362   0.47870976   ('our world shinobi', 'our_world_shinobi', '43,196')\n",
      "9581149   0.48787045   ('Cultural and Education Section, British Embassy', 'rubritish', '47,042')\n",
      "67120205   0.4953021   ('Идеи Ремонта / Лайфхаки, Дизайн, Дом, Кухня', 'russia_lifehack', '281,495')\n",
      "127254325   0.4980186   ('Признавашки EagleZ [ПИ]', 'priznavashki.eaglez', '64,712')\n",
      "143220855   0.50610423   ('̶з̶а̶в̶т̶р̶а̶ ̶б̶р̶о̶ш̶у̶', 'smotri_na_mir_inache', '34,623')\n",
      "40825951   0.5158861   ('НЕ.KURILI', 'nekurili', '72,979')\n",
      "101135802   0.52745634   ('Британские и Шотландские кошки, коты, котята', 'bscat', '23,360')\n",
      "28261334   1.0000001   ('TJ', 'tj', '686,523')\n",
      "\n",
      "\n",
      "80816321   0.48784557   ('Спорт как секс - он нужен всем', 'club80816321', '84,358')\n",
      "148401642   0.48897213   ('Brawl Stars', 'clashgcr', '50,757')\n",
      "97980111   0.49018672   ('на заре', 'dgavrikovphoto', '36,093')\n",
      "124813322   0.50589454   ('Отдам Даром Уфа', 'odufa', '97,190')\n",
      "116160601   0.5153004   ('Это Aliexpress детка', 'ali_style_ru', '61,789')\n",
      "23992862   0.5203849   ('ХК Трактор', 'hctraktor', '56,410')\n",
      "40417895   0.5220344   ('The Best Music', 'listentothebestmusic', '302,288')\n",
      "160396718   0.52813506   ('Гармония души |  Саморазвитие', 'garmonia_dyshi', '80,374')\n",
      "19397891   0.59229976   ('BASS WAVE', 'basswave', '168,509')\n",
      "92876084   1.0000001   ('Мои любимые юморески', 'jumoreski', '231,593')\n",
      "\n",
      "\n",
      "78757650   0.48746413   ('Лесби - Би - Лезби знакомства', 'lesbiantime', '41,727')\n",
      "40105130   0.49373907   ('ᴘᴀссвᴇт твoᴜx внʏтᴘᴇннᴜx ᴋᴘᴀсoт', '7sky_7', '42,287')\n",
      "47512861   0.4963845   ('AVTO BAZAR KMV Авторынок Пятигорск', 'avto_bazar_kmv', '24,391')\n",
      "71954374   0.4967957   ('Gaping & ТОП Дырочки', 'club_topass', '34,035')\n",
      "79926109   0.504278   ('история поиска', 'cheguglyatrusskiye', '112,430')\n",
      "56968937   0.508646   ('Анонимный геймер| Dota Auto Chess', 'anongamer', '210,892')\n",
      "78610805   0.5134219   ('Clash of Clans/Royale Продажа аккаунтов | Гемы', 'prodaem_akkaynty', '33,371')\n",
      "154921501   0.54397774   ('Жизненные истории', 'mylife_story', '192,626')\n",
      "118921194   0.5526232   ('▲ Пацан с характером ©', 'pacanzahodi', '31,975')\n",
      "51016572   0.9999998   ('Десигн', 'designmdk', '280,094')\n",
      "\n",
      "\n",
      "111136711   0.49829146   ('| ПОХУДЕТЬ ПРАВИЛЬНО |', 'club111136711', '27,484')\n",
      "85957535   0.49870098   ('|BetSize| • Прогнозы на спорт', 'betsize1', -1)\n",
      "1000830   0.50170445   ('АнимациЯ', 'animaciyainfo', '47,611')\n",
      "116501447   0.508427   ('Minecraft сервер CegouCraft - [1.8-1.13+]', 'cegoucraft.mine', '37,997')\n",
      "106297932   0.5112469   ('Это вам не Это', 'etnet', '359,840')\n",
      "136964272   0.5121804   ('PASTEL', 'pastelblog', '96,972')\n",
      "79625879   0.5201299   ('ОТДАМ ДАРОМ СТАРЫЙ ОСКОЛ', 'daromstoskol', '32,089')\n",
      "89301866   0.5213394   ('History Facts', 'thehistory_facts', '959,710')\n",
      "88305316   0.5524124   ('ВО САДУ ЛИ,  В ОГОРОДЕ -  с любовью к земле', 'vo_sadu_li', '354,870')\n",
      "91933860   0.9999999   ('N + 1', 'nplusone', '174,924')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_ids=[129440544, 28261334, 92876084, 51016572, 91933860]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    for id in test_ids:\n",
    "        get_closest(final_embeddings, w2i[id])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    embedding_sizes = [32, 48, 64]\n",
    "    num_sampled_arr = [32, 48, 64]\n",
    "    window_sizes = [1, 2, 3, 4, 5]\n",
    "    batch_sizes = [128, 128, 132, 128, 130]\n",
    "    for i in xrange(len(embedding_sizes)):\n",
    "        for j in xrange(len(num_sampled_arr)):\n",
    "            for k in xrange(len(window_sizes)):\n",
    "                embedding_size = embedding_sizes[i]\n",
    "                num_sampled = num_sampled_arr[j]\n",
    "                window_size = window_sizes[k]\n",
    "                batch_size = batch_sizes[k]\n",
    "                \n",
    "                graph, final_embeddings = tf_train(window_size, embedding_size, num_sampled, batch_size)\n",
    "                \n",
    "                print_random(window_size, embedding_size, num_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
